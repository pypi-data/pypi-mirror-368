{"id": "820b4e75-d274-4a09-a224-3146501f07f0", "display_name": "-01-ai/yi-6b-chat", "description": null, "short_desc": null, "endpoints": [{"id": "predict_predictions_post", "display_name": "predict_predictions_post", "description": "Run a single prediction on the model", "short_desc": "Predict", "path": "/predictions", "parameters": [{"name": "prefer", "type": "string", "required": false, "default": null, "location": "header", "param_schema": {"type": "string", "title": "Prefer"}, "description": null}, {"name": "top_k", "type": "integer", "required": false, "default": 50, "location": "body", "param_schema": {"type": "integer", "title": "Top K", "default": 50, "x-order": 4, "description": "The number of highest probability tokens to consider for generating the output. If > 0, only keep the top k tokens with highest probability (top-k filtering)."}, "description": "The number of highest probability tokens to consider for generating the output. If > 0, only keep the top k tokens with highest probability (top-k filtering)."}, {"name": "top_p", "type": "number", "required": false, "default": 0.8, "location": "body", "param_schema": {"type": "number", "title": "Top P", "default": 0.8, "x-order": 3, "description": "A probability threshold for generating the output. If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)."}, "description": "A probability threshold for generating the output. If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)."}, {"name": "prompt", "type": "string", "required": true, "default": null, "location": "body", "param_schema": {"type": "string", "title": "Prompt", "x-order": 0}, "description": "prompt"}, {"name": "temperature", "type": "number", "required": false, "default": 0.3, "location": "body", "param_schema": {"type": "number", "title": "Temperature", "default": 0.3, "x-order": 2, "description": "The value used to modulate the next token probabilities."}, "description": "The value used to modulate the next token probabilities."}, {"name": "max_new_tokens", "type": "integer", "required": false, "default": 1024, "location": "body", "param_schema": {"type": "integer", "title": "Max New Tokens", "default": 1024, "x-order": 1, "description": "The maximum number of tokens the model should generate as output."}, "description": "The maximum number of tokens the model should generate as output."}, {"name": "prompt_template", "type": "string", "required": false, "default": "<|im_start|>system\nYou are a helpful assistant<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n", "location": "body", "param_schema": {"type": "string", "title": "Prompt Template", "default": "<|im_start|>system\nYou are a helpful assistant<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n", "x-order": 6, "description": "The template used to format the prompt. The input prompt is inserted into the template using the `{prompt}` placeholder."}, "description": "The template used to format the prompt. The input prompt is inserted into the template using the `{prompt}` placeholder."}, {"name": "repetition_penalty", "type": "number", "required": false, "default": 1.2, "location": "body", "param_schema": {"type": "number", "title": "Repetition Penalty", "default": 1.2, "x-order": 5, "description": "Repetition penalty"}, "description": "Repetition penalty"}], "responses": {"200": {"content": {"application/json": {"schema": {"$ref": "#/components/schemas/PredictionResponse"}}}, "description": "Successful Response"}, "422": {"content": {"application/json": {"schema": {"$ref": "#/components/schemas/HTTPValidationError"}}}, "description": "Validation Error"}}, "timeout_s": null}], "specification": "socaity", "used_models": [], "category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "family_id": "17a35c11-31a6-463f-a953-4d6f9e307174", "service_address": {"url": "http://localhost:8001/v1/-01-ai/yi-6b-chat"}, "created_at": "2025-08-12T10:19:56.186731+00:00", "version": "98134fb1dbf883bdfd56d4c78f96525035e807d9", "schemas": {"Input": {"type": "object", "title": "Input", "required": ["prompt"], "properties": {"top_k": {"type": "integer", "title": "Top K", "default": 50, "x-order": 4, "description": "The number of highest probability tokens to consider for generating the output. If > 0, only keep the top k tokens with highest probability (top-k filtering)."}, "top_p": {"type": "number", "title": "Top P", "default": 0.8, "x-order": 3, "description": "A probability threshold for generating the output. If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)."}, "prompt": {"type": "string", "title": "Prompt", "x-order": 0}, "temperature": {"type": "number", "title": "Temperature", "default": 0.3, "x-order": 2, "description": "The value used to modulate the next token probabilities."}, "max_new_tokens": {"type": "integer", "title": "Max New Tokens", "default": 1024, "x-order": 1, "description": "The maximum number of tokens the model should generate as output."}, "prompt_template": {"type": "string", "title": "Prompt Template", "default": "<|im_start|>system\nYou are a helpful assistant<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n", "x-order": 6, "description": "The template used to format the prompt. The input prompt is inserted into the template using the `{prompt}` placeholder."}, "repetition_penalty": {"type": "number", "title": "Repetition Penalty", "default": 1.2, "x-order": 5, "description": "Repetition penalty"}}}, "Output": {"type": "array", "items": {"type": "string"}, "title": "Output", "x-cog-array-type": "iterator", "x-cog-array-display": "concatenate"}, "Status": {"enum": ["starting", "processing", "succeeded", "canceled", "failed"], "type": "string", "title": "Status", "description": "An enumeration."}, "WebhookEvent": {"enum": ["start", "output", "logs", "completed"], "type": "string", "title": "WebhookEvent", "description": "An enumeration."}, "ValidationError": {"type": "object", "title": "ValidationError", "required": ["loc", "msg", "type"], "properties": {"loc": {"type": "array", "items": {"anyOf": [{"type": "string"}, {"type": "integer"}]}, "title": "Location"}, "msg": {"type": "string", "title": "Message"}, "type": {"type": "string", "title": "Error Type"}}}, "PredictionRequest": {"type": "object", "title": "PredictionRequest", "properties": {"id": {"type": "string", "title": "Id"}, "input": {"$ref": "#/components/schemas/Input"}, "webhook": {"type": "string", "title": "Webhook", "format": "uri", "maxLength": 65536, "minLength": 1}, "created_at": {"type": "string", "title": "Created At", "format": "date-time"}, "output_file_prefix": {"type": "string", "title": "Output File Prefix"}, "webhook_events_filter": {"type": "array", "items": {"$ref": "#/components/schemas/WebhookEvent"}, "default": ["start", "output", "logs", "completed"]}}}, "PredictionResponse": {"type": "object", "title": "PredictionResponse", "properties": {"id": {"type": "string", "title": "Id"}, "logs": {"type": "string", "title": "Logs", "default": ""}, "error": {"type": "string", "title": "Error"}, "input": {"$ref": "#/components/schemas/Input"}, "output": {"$ref": "#/components/schemas/Output"}, "status": {"$ref": "#/components/schemas/Status"}, "metrics": {"type": "object", "title": "Metrics"}, "version": {"type": "string", "title": "Version"}, "created_at": {"type": "string", "title": "Created At", "format": "date-time"}, "started_at": {"type": "string", "title": "Started At", "format": "date-time"}, "completed_at": {"type": "string", "title": "Completed At", "format": "date-time"}}}, "HTTPValidationError": {"type": "object", "title": "HTTPValidationError", "properties": {"detail": {"type": "array", "items": {"$ref": "#/components/schemas/ValidationError"}, "title": "Detail"}}}}}