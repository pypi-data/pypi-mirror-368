"""
L3 FullPhoneticReranker - å®Œæ•´èªéŸ³ç‰¹å¾µç²¾æ’å™¨
çµåˆè²éŸ»èª¿ä¸‰ç¶­åº¦èªéŸ³ç‰¹å¾µçš„æœ€çµ‚ç²¾æ’ï¼Œå°‡500å€™é¸ç¸®æ¸›åˆ°æœ€ä½³50å€‹çµæœ
"""

import time
import logging
from typing import List, Dict, Set, Optional, Tuple, Union, Any
from pathlib import Path
from dataclasses import dataclass

from core.phonetic_classifier import PhoneticClassifier, PhoneticFeatures
from analyzers.finals_analyzer import FinalsAnalyzer, FinalsFeatures
from analyzers.tone_analyzer import ToneAnalyzer, ToneFeatures
from data.super_dictionary_manager import SuperDictionaryManager


@dataclass
class CompletePhoneticFeatures:
    """å®Œæ•´èªéŸ³ç‰¹å¾µçµæ§‹"""
    character: str = ""
    consonant_features: Optional[PhoneticFeatures] = None
    finals_features: Optional[FinalsFeatures] = None
    tone_features: Optional[ToneFeatures] = None
    
    # ç¶œåˆç›¸ä¼¼åº¦åˆ†æ•¸
    total_similarity: float = 0.0
    consonant_similarity: float = 0.0
    finals_similarity: float = 0.0
    tone_similarity: float = 0.0


@dataclass
class WordPhoneticProfile:
    """è©å½™èªéŸ³ç‰¹å¾µæª”æ¡ˆ"""
    word: str = ""
    character_features: List[CompletePhoneticFeatures] = None
    word_length: int = 0
    overall_similarity: float = 0.0
    complexity_score: float = 0.0      # èªéŸ³è¤‡é›œåº¦è©•ä¼°
    
    def __post_init__(self):
        if self.character_features is None:
            self.character_features = []
        self.word_length = len(self.word)


class FullPhoneticReranker:
    """
    L3å®Œæ•´èªéŸ³ç‰¹å¾µç²¾æ’å™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ•´åˆè²éŸ»èª¿ä¸‰ç¶­åº¦èªéŸ³ç‰¹å¾µ
    2. å¯¦ç¾å¤šå­—è©é€å­—å°æ¯”èˆ‡æ•´é«”è©•ä¼°  
    3. æ”¯æ´èªéŸ³è¤‡é›œåº¦è©•ä¼°èˆ‡æ¡ˆä¾‹åˆ†ç´š
    4. è¨˜éŒ„è¨“ç·´æ•¸æ“šä¾›ç¬¬äºŒæœŸæ©Ÿå™¨å­¸ç¿’æ¨¡å‹ä½¿ç”¨
    5. åœ¨100mså…§è™•ç†500å€™é¸è©ï¼Œè¼¸å‡º50å€‹æœ€ä½³çµæœ
    """
    
    def __init__(self, 
                 dict_manager: SuperDictionaryManager,
                 phonetic_classifier: PhoneticClassifier,
                 finals_analyzer: FinalsAnalyzer,
                 tone_analyzer: ToneAnalyzer,
                 enable_data_logging: bool = True):
        """
        åˆå§‹åŒ–L3ç²¾æ’å™¨
        
        Args:
            dict_manager: å­—å…¸ç®¡ç†å™¨
            phonetic_classifier: è²æ¯åˆ†é¡å™¨
            finals_analyzer: éŸ»æ¯åˆ†æå™¨
            tone_analyzer: è²èª¿åˆ†æå™¨
            enable_data_logging: æ˜¯å¦å•Ÿç”¨æ•¸æ“šè¨˜éŒ„
        """
        self.logger = logging.getLogger(__name__)
        self.dict_manager = dict_manager
        self.consonant_classifier = phonetic_classifier
        self.finals_analyzer = finals_analyzer
        self.tone_analyzer = tone_analyzer
        self.enable_data_logging = enable_data_logging
        
        # èªéŸ³ç‰¹å¾µæ¬Šé‡é…ç½® (å¯èª¿åƒæ•¸)
        self.feature_weights = {
            "consonant": 0.35,      # è²æ¯æ¬Šé‡35%
            "finals": 0.45,         # éŸ»æ¯æ¬Šé‡45% (æ›´é‡è¦)
            "tone": 0.20           # è²èª¿æ¬Šé‡20%
        }
        
        # è¤‡é›œåº¦è©•ä¼°é…ç½®
        self.complexity_thresholds = {
            "simple": 0.8,          # ç°¡å–®æ¡ˆä¾‹ï¼šç›¸ä¼¼åº¦>0.8
            "medium": 0.5,          # ä¸­ç­‰æ¡ˆä¾‹ï¼š0.5<ç›¸ä¼¼åº¦â‰¤0.8
            "complex": 0.0          # è¤‡é›œæ¡ˆä¾‹ï¼šç›¸ä¼¼åº¦â‰¤0.5
        }
        
        # æ€§èƒ½çµ±è¨ˆ
        self.rerank_stats = {
            "total_queries": 0,
            "total_candidates_input": 0,
            "total_candidates_output": 0,
            "total_time_ms": 0.0,
            "complexity_distribution": {"simple": 0, "medium": 0, "complex": 0},
            "cache_hits": 0
        }
        
        # çµæœç·©å­˜
        self.result_cache: Dict[str, List[WordPhoneticProfile]] = {}
        self.cache_enabled = True
        
        # è¨“ç·´æ•¸æ“šè¨˜éŒ„
        self.training_data = [] if enable_data_logging else None
        
        self.logger.info("FullPhoneticReranker initialized")
    
    def rerank(self, query: str, 
               candidates: List[str], 
               top_k: int = 50) -> List[str]:
        """
        åŸ·è¡ŒL3å®Œæ•´èªéŸ³ç‰¹å¾µç²¾æ’
        
        Args:
            query: æŸ¥è©¢è©å½™
            candidates: L2é‡æ’å¾Œçš„å€™é¸è©åˆ—è¡¨ (~500å€‹)
            top_k: è¿”å›çš„top-kçµæœæ•¸é‡
            
        Returns:
            ç²¾æ’å¾Œçš„å€™é¸è©åˆ—è¡¨ (æŒ‰ç›¸ä¼¼åº¦é™åº)
        """
        start_time = time.time()
        
        if not query or not candidates:
            return []
        
        # æª¢æŸ¥ç·©å­˜
        cache_key = f"{query}_{len(candidates)}_{top_k}"
        if self.cache_enabled and cache_key in self.result_cache:
            self.rerank_stats["cache_hits"] += 1
            cached_profiles = self.result_cache[cache_key]
            return [profile.word for profile in cached_profiles]
        
        # æ§‹å»ºæŸ¥è©¢è©èªéŸ³ç‰¹å¾µæª”æ¡ˆ
        query_profile = self._build_phonetic_profile(query)
        
        # æ‰¹é‡æ§‹å»ºå€™é¸è©èªéŸ³ç‰¹å¾µæª”æ¡ˆ
        candidate_profiles = []
        for candidate in candidates:
            if candidate == query:
                # å®Œå…¨åŒ¹é…çµ¦äºˆæœ€é«˜åˆ†æ•¸
                profile = WordPhoneticProfile(word=candidate, overall_similarity=1.0)
                candidate_profiles.append(profile)
            else:
                profile = self._build_phonetic_profile(candidate)
                # è¨ˆç®—èˆ‡æŸ¥è©¢è©çš„ç¶œåˆç›¸ä¼¼åº¦
                profile.overall_similarity = self._calculate_word_similarity(query_profile, profile)
                # è©•ä¼°èªéŸ³è¤‡é›œåº¦
                profile.complexity_score = self._assess_complexity(query_profile, profile)
                candidate_profiles.append(profile)
        
        # æŒ‰ç¶œåˆç›¸ä¼¼åº¦æ’åº
        candidate_profiles.sort(key=lambda x: x.overall_similarity, reverse=True)
        top_profiles = candidate_profiles[:top_k]
        
        # è¨˜éŒ„è¨“ç·´æ•¸æ“š
        if self.enable_data_logging:
            self._log_training_data(query_profile, candidate_profiles, top_profiles)
        
        # æ›´æ–°çµ±è¨ˆä¿¡æ¯
        end_time = time.time()
        processing_time = (end_time - start_time) * 1000
        
        self.rerank_stats["total_queries"] += 1
        self.rerank_stats["total_candidates_input"] += len(candidates)
        self.rerank_stats["total_candidates_output"] += len(top_profiles)
        self.rerank_stats["total_time_ms"] += processing_time
        
        # æ›´æ–°è¤‡é›œåº¦åˆ†ä½ˆçµ±è¨ˆ
        for profile in top_profiles:
            if profile.overall_similarity > self.complexity_thresholds["simple"]:
                self.rerank_stats["complexity_distribution"]["simple"] += 1
            elif profile.overall_similarity > self.complexity_thresholds["medium"]:
                self.rerank_stats["complexity_distribution"]["medium"] += 1
            else:
                self.rerank_stats["complexity_distribution"]["complex"] += 1
        
        # ç·©å­˜çµæœ
        if self.cache_enabled:
            self.result_cache[cache_key] = top_profiles
        
        result_words = [profile.word for profile in top_profiles]
        
        self.logger.debug(f"L3 Rerank: {query} â†’ {len(candidates)} to {len(result_words)} "
                         f"({processing_time:.1f}ms)")
        
        return result_words
    
    def _build_phonetic_profile(self, word: str) -> WordPhoneticProfile:
        """æ§‹å»ºè©å½™çš„å®Œæ•´èªéŸ³ç‰¹å¾µæª”æ¡ˆ"""
        profile = WordPhoneticProfile(word=word)
        
        for char in word:
            char_features = CompletePhoneticFeatures(character=char)
            
            # æå–å„ç¶­åº¦ç‰¹å¾µ
            char_features.consonant_features = self.consonant_classifier.extract_phonetic_features(char)
            char_features.finals_features = self.finals_analyzer.extract_finals_features(char)
            char_features.tone_features = self.tone_analyzer.extract_tone_features(char)
            
            profile.character_features.append(char_features)
        
        return profile
    
    def _calculate_word_similarity(self, query_profile: WordPhoneticProfile, 
                                  candidate_profile: WordPhoneticProfile) -> float:
        """è¨ˆç®—è©å½™é–“çš„ç¶œåˆèªéŸ³ç›¸ä¼¼åº¦"""
        if not query_profile.character_features or not candidate_profile.character_features:
            return 0.0
        
        query_len = len(query_profile.character_features)
        candidate_len = len(candidate_profile.character_features)
        
        # ä½¿ç”¨å‹•æ…‹è¦åŠƒè™•ç†ç•°é•·åº¦è©å½™å°æ¯”
        similarity_matrix = self._build_similarity_matrix(
            query_profile.character_features,
            candidate_profile.character_features
        )
        
        # è¨ˆç®—æœ€ä½³å°é½Šç›¸ä¼¼åº¦
        optimal_similarity = self._find_optimal_alignment(similarity_matrix, query_len, candidate_len)
        
        # é•·åº¦æ‡²ç½°
        length_penalty = self._calculate_length_penalty(query_len, candidate_len)
        
        return optimal_similarity * length_penalty
    
    def _build_similarity_matrix(self, query_chars: List[CompletePhoneticFeatures],
                               candidate_chars: List[CompletePhoneticFeatures]) -> List[List[float]]:
        """å»ºç«‹å­—ç¬¦é–“ç›¸ä¼¼åº¦çŸ©é™£"""
        query_len = len(query_chars)
        candidate_len = len(candidate_chars)
        
        matrix = [[0.0] * candidate_len for _ in range(query_len)]
        
        for i in range(query_len):
            for j in range(candidate_len):
                matrix[i][j] = self._calculate_character_similarity(query_chars[i], candidate_chars[j])
        
        return matrix
    
    def _calculate_character_similarity(self, query_char: CompletePhoneticFeatures,
                                      candidate_char: CompletePhoneticFeatures) -> float:
        """è¨ˆç®—å–®å­—é–“çš„å®Œæ•´èªéŸ³ç›¸ä¼¼åº¦"""
        if query_char.character == candidate_char.character:
            return 1.0
        
        # è²æ¯ç›¸ä¼¼åº¦
        consonant_sim = self.consonant_classifier.calculate_consonant_similarity(
            query_char.consonant_features.consonant or "",
            candidate_char.consonant_features.consonant or ""
        )
        
        # éŸ»æ¯ç›¸ä¼¼åº¦
        finals_sim = self.finals_analyzer.calculate_finals_similarity(
            query_char.finals_features.finals,
            candidate_char.finals_features.finals
        )
        
        # è²èª¿ç›¸ä¼¼åº¦
        tone_sim = self.tone_analyzer.calculate_tone_similarity(
            query_char.tone_features.tone,
            candidate_char.tone_features.tone
        )
        
        # åŠ æ¬Šçµ„åˆ
        total_similarity = (
            consonant_sim * self.feature_weights["consonant"] +
            finals_sim * self.feature_weights["finals"] +
            tone_sim * self.feature_weights["tone"]
        )
        
        return total_similarity
    
    def _find_optimal_alignment(self, similarity_matrix: List[List[float]], 
                              query_len: int, candidate_len: int) -> float:
        """æ‰¾åˆ°æœ€ä½³å°é½Šæ–¹æ¡ˆçš„ç›¸ä¼¼åº¦"""
        if not similarity_matrix or query_len == 0 or candidate_len == 0:
            return 0.0
        
        # å‹•æ…‹è¦åŠƒæ‰¾æœ€ä½³å°é½Š
        dp = [[0.0] * (candidate_len + 1) for _ in range(query_len + 1)]
        
        for i in range(1, query_len + 1):
            for j in range(1, candidate_len + 1):
                # åŒ¹é…å¾—åˆ†
                match_score = similarity_matrix[i-1][j-1]
                
                # ä¸‰ç¨®æ“ä½œ
                match = dp[i-1][j-1] + match_score
                delete = dp[i-1][j] * 0.8  # åˆªé™¤æ‡²ç½°
                insert = dp[i][j-1] * 0.8  # æ’å…¥æ‡²ç½°
                
                dp[i][j] = max(match, delete, insert)
        
        # æ¨™æº–åŒ–åˆ†æ•¸
        max_possible = max(query_len, candidate_len)
        return dp[query_len][candidate_len] / max_possible
    
    def _calculate_length_penalty(self, len1: int, len2: int) -> float:
        """è¨ˆç®—é•·åº¦å·®ç•°æ‡²ç½°"""
        len_diff = abs(len1 - len2)
        
        if len_diff == 0:
            return 1.0
        elif len_diff == 1:
            return 0.92
        elif len_diff == 2:
            return 0.8
        elif len_diff == 3:
            return 0.65
        else:
            return 0.5
    
    def _assess_complexity(self, query_profile: WordPhoneticProfile,
                          candidate_profile: WordPhoneticProfile) -> float:
        """è©•ä¼°èªéŸ³è®Šç•°è¤‡é›œåº¦"""
        # åŸºæ–¼å¤šå€‹å› å­è©•ä¼°è¤‡é›œåº¦
        complexity_factors = []
        
        # 1. æ•´é«”ç›¸ä¼¼åº¦ (è¶Šä½è¶Šè¤‡é›œ)
        overall_sim = candidate_profile.overall_similarity
        complexity_factors.append(1.0 - overall_sim)
        
        # 2. é•·åº¦å·®ç•°å¾©é›œåº¦
        len_diff = abs(len(query_profile.word) - len(candidate_profile.word))
        length_complexity = min(len_diff * 0.2, 1.0)
        complexity_factors.append(length_complexity)
        
        # 3. ç‰¹å¾µç¶­åº¦ä¸ä¸€è‡´æ•¸
        feature_mismatches = 0
        total_comparisons = 0
        
        min_len = min(len(query_profile.character_features), 
                     len(candidate_profile.character_features))
        
        for i in range(min_len):
            q_char = query_profile.character_features[i]
            c_char = candidate_profile.character_features[i]
            
            # æª¢æŸ¥å„ç¶­åº¦æ˜¯å¦åŒ¹é…
            if q_char.consonant_features.consonant != c_char.consonant_features.consonant:
                feature_mismatches += 1
            if q_char.finals_features.finals != c_char.finals_features.finals:
                feature_mismatches += 1
            if q_char.tone_features.tone != c_char.tone_features.tone:
                feature_mismatches += 1
            
            total_comparisons += 3
        
        if total_comparisons > 0:
            feature_complexity = feature_mismatches / total_comparisons
            complexity_factors.append(feature_complexity)
        
        # ç¶œåˆè¤‡é›œåº¦è©•åˆ†
        return sum(complexity_factors) / len(complexity_factors)
    
    def _log_training_data(self, query_profile: WordPhoneticProfile,
                          all_candidates: List[WordPhoneticProfile],
                          selected_candidates: List[WordPhoneticProfile]):
        """è¨˜éŒ„æ©Ÿå™¨å­¸ç¿’è¨“ç·´æ•¸æ“š"""
        if not self.enable_data_logging:
            return
        
        training_entry = {
            "timestamp": time.time(),
            "query": query_profile.word,
            "query_features": self._serialize_profile(query_profile),
            "total_candidates": len(all_candidates),
            "selected_count": len(selected_candidates),
            "candidates": [
                {
                    "word": profile.word,
                    "features": self._serialize_profile(profile),
                    "similarity": profile.overall_similarity,
                    "complexity": profile.complexity_score,
                    "rank": i + 1,
                    "selected": profile in selected_candidates
                }
                for i, profile in enumerate(all_candidates[:100])  # é™åˆ¶è¨˜éŒ„æ•¸é‡
            ]
        }
        
        self.training_data.append(training_entry)
    
    def _serialize_profile(self, profile: WordPhoneticProfile) -> Dict[str, Any]:
        """åºåˆ—åŒ–èªéŸ³ç‰¹å¾µæª”æ¡ˆä¾›æ•¸æ“šè¨˜éŒ„"""
        return {
            "word": profile.word,
            "length": profile.word_length,
            "characters": [
                {
                    "char": char.character,
                    "consonant": char.consonant_features.consonant if char.consonant_features else "",
                    "consonant_group": char.consonant_features.consonant_group if char.consonant_features else "",
                    "finals": char.finals_features.finals if char.finals_features else "",
                    "finals_group": char.finals_features.finals_group if char.finals_features else "",
                    "tone": char.tone_features.tone if char.tone_features else 0,
                    "tone_name": char.tone_features.tone_name if char.tone_features else ""
                }
                for char in profile.character_features
            ]
        }
    
    def batch_rerank(self, queries: List[str], 
                    candidates_lists: List[List[str]], 
                    top_k: int = 50) -> Dict[str, List[str]]:
        """æ‰¹é‡åŸ·è¡ŒL3ç²¾æ’"""
        results = {}
        
        for query, candidates in zip(queries, candidates_lists):
            results[query] = self.rerank(query, candidates, top_k)
        
        return results
    
    def get_rerank_statistics(self) -> Dict[str, Any]:
        """ç²å–ç²¾æ’å™¨æ€§èƒ½çµ±è¨ˆ"""
        stats = self.rerank_stats.copy()
        
        if stats["total_queries"] > 0:
            stats["avg_input_candidates"] = stats["total_candidates_input"] / stats["total_queries"]
            stats["avg_output_candidates"] = stats["total_candidates_output"] / stats["total_queries"]
            stats["avg_processing_time_ms"] = stats["total_time_ms"] / stats["total_queries"]
            stats["avg_reduction_ratio"] = (stats["total_candidates_output"] / 
                                          max(stats["total_candidates_input"], 1))
            stats["cache_hit_rate"] = stats["cache_hits"] / stats["total_queries"]
        
        # è¤‡é›œåº¦åˆ†ä½ˆç™¾åˆ†æ¯”
        total_cases = sum(stats["complexity_distribution"].values())
        if total_cases > 0:
            stats["complexity_percentages"] = {
                level: count / total_cases 
                for level, count in stats["complexity_distribution"].items()
            }
        
        stats["cache_size"] = len(self.result_cache)
        stats["training_data_size"] = len(self.training_data) if self.training_data else 0
        
        return stats
    
    def get_training_data(self) -> List[Dict[str, Any]]:
        """ç²å–è¨“ç·´æ•¸æ“šè¨˜éŒ„"""
        return self.training_data.copy() if self.training_data else []
    
    def clear_cache(self):
        """æ¸…é™¤ç·©å­˜"""
        self.result_cache.clear()
        self.logger.info("L3 reranker cache cleared")
    
    def save_training_data(self, filepath: str):
        """ä¿å­˜è¨“ç·´æ•¸æ“šåˆ°æ–‡ä»¶"""
        if not self.training_data:
            self.logger.warning("No training data to save")
            return
        
        import json
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.training_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"Saved {len(self.training_data)} training entries to {filepath}")


def test_l3_full_phonetic_reranker():
    """æ¸¬è©¦FullPhoneticRerankeråŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦FullPhoneticRerankeråŠŸèƒ½")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–ä¾è³´çµ„ä»¶
        dict_manager = SuperDictionaryManager(
            super_dict_path="data/super_dicts/super_dict_combined.json",
            super_dict_reversed_path="data/super_dicts/super_dict_reversed.json"
        )
        
        consonant_classifier = PhoneticClassifier()
        finals_analyzer = FinalsAnalyzer()
        tone_analyzer = ToneAnalyzer()
        
        l3_reranker = FullPhoneticReranker(
            dict_manager, consonant_classifier, finals_analyzer, tone_analyzer
        )
        
        # æ¸¬è©¦æ¡ˆä¾‹
        test_cases = [
            ("çŸ¥é“", ["çŸ¥é“", "è³‡é“", "æŒ‡å°", "åˆ¶å°", "æ™ºæ…§", "åƒé£¯", "ç¡è¦º", "ä¾†äº†", "é›»è…¦", "æ‰‹æ©Ÿ"] * 10),  # 100å€™é¸
            ("åƒé£¯", ["åƒé£¯", "æ¬¡é£¯", "åƒå®Œ", "æ¬¡å®Œ", "é£¯èœ", "çŸ¥é“", "ç¡è¦º", "å·¥ä½œ", "å®‰å…¨", "é›»è…¦"] * 10),
        ]
        
        print("ğŸ“Š L3ç²¾æ’æ¸¬è©¦:")
        print("-" * 40)
        
        for query, candidates in test_cases:
            print(f"\næŸ¥è©¢: '{query}'")
            print(f"è¼¸å…¥å€™é¸: {len(candidates)} å€‹")
            
            # ç²¾æ’
            start_time = time.time()
            reranked = l3_reranker.rerank(query, candidates, top_k=10)
            processing_time = (time.time() - start_time) * 1000
            
            print(f"ç²¾æ’çµæœ: {len(reranked)} å€‹")
            print(f"è™•ç†æ™‚é–“: {processing_time:.1f}ms")
            print(f"å‰5å€‹: {reranked[:5]}")
            
            # è¤‡é›œåº¦åˆ†æ
            stats = l3_reranker.get_rerank_statistics()
            if "complexity_percentages" in stats:
                comp_perc = stats["complexity_percentages"]
                print(f"è¤‡é›œåº¦åˆ†ä½ˆ: ç°¡å–®={comp_perc.get('simple', 0):.1%}, "
                      f"ä¸­ç­‰={comp_perc.get('medium', 0):.1%}, "
                      f"è¤‡é›œ={comp_perc.get('complex', 0):.1%}")
        
        # æ€§èƒ½çµ±è¨ˆ
        print("\nğŸ“Š L3ç²¾æ’å™¨çµ±è¨ˆ:")
        print("-" * 40)
        
        final_stats = l3_reranker.get_rerank_statistics()
        print(f"ç¸½æŸ¥è©¢æ•¸: {final_stats['total_queries']}")
        print(f"å¹³å‡è¼¸å…¥å€™é¸æ•¸: {final_stats.get('avg_input_candidates', 0):.0f}")
        print(f"å¹³å‡è¼¸å‡ºå€™é¸æ•¸: {final_stats.get('avg_output_candidates', 0):.0f}")
        print(f"å¹³å‡è™•ç†æ™‚é–“: {final_stats.get('avg_processing_time_ms', 0):.2f}ms")
        print(f"ç·©å­˜å‘½ä¸­ç‡: {final_stats.get('cache_hit_rate', 0):.1%}")
        print(f"è¨“ç·´æ•¸æ“šæ¢ç›®: {final_stats['training_data_size']}")
        
        # é©—æ”¶æ¨™æº–æª¢æŸ¥
        print("\nğŸ“Š L3ç²¾æ’å™¨é©—æ”¶æ¨™æº–æª¢æŸ¥:")
        print("-" * 40)
        
        avg_time = final_stats.get('avg_processing_time_ms', 0)
        avg_output = final_stats.get('avg_output_candidates', 0)
        
        print(f"âœ“ å¹³å‡è™•ç†æ™‚é–“: {avg_time:.1f}ms "
              f"{'< 100ms âœ…' if avg_time < 100 else '>= 100ms âŒ'}")
        print(f"âœ“ è¼¸å‡ºå€™é¸æ•¸: {avg_output:.0f} å€‹ "
              f"{'â‰¤50 âœ…' if avg_output <= 50 else '>50 âŒ'}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(level=logging.INFO)
    
    # åŸ·è¡Œæ¸¬è©¦
    success = test_l3_full_phonetic_reranker()
    print(f"\næ¸¬è©¦ {'âœ… PASSED' if success else 'âŒ FAILED'}")