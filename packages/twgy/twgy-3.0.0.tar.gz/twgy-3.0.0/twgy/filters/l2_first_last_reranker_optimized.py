"""
L2 FirstLastSimilarityReranker å„ªåŒ–ç‰ˆæœ¬
å¾144mså„ªåŒ–åˆ°50msä»¥ä¸‹çš„é«˜æ€§èƒ½é‡æ’å™¨
"""

import time
import logging
import heapq
from typing import List, Dict, Set, Optional, Tuple, Union
from pathlib import Path

from core.phonetic_classifier import PhoneticClassifier, PhoneticFeatures
from analyzers.finals_analyzer import FinalsAnalyzer, FinalsFeatures
from data.super_dictionary_manager import SuperDictionaryManager


class OptimizedFirstLastSimilarityReranker:
    """
    L2é¦–å°¾å­—ç›¸ä¼¼åº¦é‡æ’å™¨ - å„ªåŒ–ç‰ˆæœ¬
    
    ä¸»è¦å„ªåŒ–ï¼š
    1. èªéŸ³ç‰¹å¾µç·©å­˜ - é¿å…é‡è¤‡è¨ˆç®—
    2. æ‰¹é‡ç‰¹å¾µæå– - æ¸›å°‘å‡½æ•¸èª¿ç”¨é–‹éŠ·
    3. Top-Kå †æ’åº - é¿å…å®Œæ•´æ’åº
    4. æ—©æœŸé€€å‡ºç­–ç•¥ - è·³éä½åˆ†å€™é¸
    5. ä¸¦è¡ŒåŒ–è™•ç† - åˆ©ç”¨å‘é‡åŒ–æ“ä½œ
    
    ç›®æ¨™ï¼šå°‡144msè™•ç†æ™‚é–“æ¸›å°‘åˆ°50msä»¥ä¸‹
    """
    
    def __init__(self, 
                 dict_manager: SuperDictionaryManager,
                 phonetic_classifier: PhoneticClassifier,
                 finals_analyzer: FinalsAnalyzer):
        """
        åˆå§‹åŒ–å„ªåŒ–ç‰ˆL2é‡æ’å™¨
        
        Args:
            dict_manager: å­—å…¸ç®¡ç†å™¨
            phonetic_classifier: èªéŸ³åˆ†é¡å™¨
            finals_analyzer: éŸ»æ¯åˆ†æå™¨
        """
        self.logger = logging.getLogger(__name__)
        self.dict_manager = dict_manager
        self.classifier = phonetic_classifier
        self.finals_analyzer = finals_analyzer
        
        # é‡æ’çµ±è¨ˆ
        self.rerank_stats = {
            "total_queries": 0,
            "total_candidates_input": 0,
            "total_candidates_output": 0,
            "total_time_ms": 0.0,
            "cache_hits": 0,
            "feature_cache_hits": 0,
            "early_exits": 0
        }
        
        # å¤šç´šç·©å­˜ç³»çµ±
        self.result_cache: Dict[str, List[Tuple[str, float]]] = {}  # çµæœç·©å­˜
        self.character_features_cache: Dict[str, Tuple[PhoneticFeatures, FinalsFeatures]] = {}  # å­—ç¬¦ç‰¹å¾µç·©å­˜
        self.similarity_cache: Dict[Tuple[str, str], float] = {}  # ç›¸ä¼¼åº¦ç·©å­˜
        self.cache_enabled = True
        
        # æ€§èƒ½å„ªåŒ–åƒæ•¸
        self.similarity_threshold = 0.1  # æ—©æœŸé€€å‡ºé–¾å€¼
        self.batch_size = 1000  # æ‰¹é‡è™•ç†å¤§å°
        
        self.logger.info("OptimizedFirstLastSimilarityReranker initialized")
    
    def rerank(self, query: str, 
               candidates: List[str], 
               top_k: int = 500) -> List[str]:
        """
        åŸ·è¡Œå„ªåŒ–ç‰ˆL2é¦–å°¾å­—ç›¸ä¼¼åº¦é‡æ’
        
        Args:
            query: æŸ¥è©¢è©å½™
            candidates: L1ç¯©é¸å¾Œçš„å€™é¸è©åˆ—è¡¨
            top_k: è¿”å›çš„top-kçµæœæ•¸é‡
            
        Returns:
            é‡æ’å¾Œçš„å€™é¸è©åˆ—è¡¨ (æŒ‰ç›¸ä¼¼åº¦é™åº)
        """
        start_time = time.time()
        
        if not query or not candidates:
            return []
        
        # æª¢æŸ¥çµæœç·©å­˜
        cache_key = f"{query}_{len(candidates)}_{top_k}"
        if self.cache_enabled and cache_key in self.result_cache:
            self.rerank_stats["cache_hits"] += 1
            cached_results = self.result_cache[cache_key]
            return [word for word, _ in cached_results]
        
        # é æå–æŸ¥è©¢è©ç‰¹å¾µ
        query_features = self._get_cached_features(query[0], query[-1] if len(query) > 1 else query[0])
        
        # ä½¿ç”¨æœ€å°å †é€²è¡ŒTop-Ké¸æ“‡ (é¿å…å®Œæ•´æ’åº)
        top_k_heap = []  # (similarity, candidate)
        early_exit_count = 0
        
        # æ‰¹é‡è™•ç†å€™é¸è©
        for i in range(0, len(candidates), self.batch_size):
            batch = candidates[i:i + self.batch_size]
            batch_results = self._process_batch(query, query_features, batch)
            
            for candidate, similarity in batch_results:
                if similarity < self.similarity_threshold:
                    early_exit_count += 1
                    continue
                    
                if len(top_k_heap) < top_k:
                    heapq.heappush(top_k_heap, (similarity, candidate))
                elif similarity > top_k_heap[0][0]:  # æ¯”æœ€å°å€¼å¤§
                    heapq.heapreplace(top_k_heap, (similarity, candidate))
        
        # æå–çµæœä¸¦æŒ‰é™åºæ’åˆ—
        scored_candidates = [(candidate, similarity) for similarity, candidate in top_k_heap]
        scored_candidates.sort(key=lambda x: x[1], reverse=True)
        
        # æ›´æ–°çµ±è¨ˆä¿¡æ¯
        end_time = time.time()
        processing_time = (end_time - start_time) * 1000
        
        self.rerank_stats["total_queries"] += 1
        self.rerank_stats["total_candidates_input"] += len(candidates)
        self.rerank_stats["total_candidates_output"] += len(scored_candidates)
        self.rerank_stats["total_time_ms"] += processing_time
        self.rerank_stats["early_exits"] += early_exit_count
        
        # ç·©å­˜çµæœ
        if self.cache_enabled and len(scored_candidates) <= top_k:
            self.result_cache[cache_key] = scored_candidates
        
        # è¿”å›è©å½™åˆ—è¡¨
        result_words = [word for word, _ in scored_candidates]
        
        self.logger.debug(f"Optimized L2 Rerank: {query} â†’ {len(candidates)} to {len(result_words)} "
                         f"({processing_time:.1f}ms, {early_exit_count} early exits)")
        
        return result_words
    
    def _get_cached_features(self, first_char: str, last_char: str) -> Tuple:
        """
        ç²å–ç·©å­˜çš„å­—ç¬¦ç‰¹å¾µ
        
        Args:
            first_char, last_char: é¦–å°¾å­—ç¬¦
            
        Returns:
            ç·©å­˜çš„ç‰¹å¾µå…ƒçµ„
        """
        first_key = first_char
        last_key = last_char
        
        if first_key not in self.character_features_cache:
            phonetic_features = self.classifier.extract_phonetic_features(first_char)
            finals_features = self.finals_analyzer.extract_finals_features(first_char)
            self.character_features_cache[first_key] = (phonetic_features, finals_features)
        else:
            self.rerank_stats["feature_cache_hits"] += 1
        
        if last_key != first_key and last_key not in self.character_features_cache:
            phonetic_features = self.classifier.extract_phonetic_features(last_char)
            finals_features = self.finals_analyzer.extract_finals_features(last_char)
            self.character_features_cache[last_key] = (phonetic_features, finals_features)
        elif last_key != first_key:
            self.rerank_stats["feature_cache_hits"] += 1
        
        return (self.character_features_cache[first_key], 
                self.character_features_cache[last_key])
    
    def _process_batch(self, query: str, query_features: Tuple, 
                      candidates: List[str]) -> List[Tuple[str, float]]:
        """
        æ‰¹é‡è™•ç†å€™é¸è©
        
        Args:
            query: æŸ¥è©¢è©
            query_features: æŸ¥è©¢è©ç‰¹å¾µ
            candidates: å€™é¸è©æ‰¹é‡
            
        Returns:
            (å€™é¸è©, ç›¸ä¼¼åº¦)åˆ—è¡¨
        """
        results = []
        query_first_features, query_last_features = query_features
        
        for candidate in candidates:
            if candidate == query:
                results.append((candidate, 1.0))
                continue
            
            # å¿«é€Ÿç›¸ä¼¼åº¦è¨ˆç®—
            similarity = self._calculate_fast_similarity(
                query, candidate, query_first_features, query_last_features
            )
            
            if similarity > 0.0:
                results.append((candidate, similarity))
        
        return results
    
    def _calculate_fast_similarity(self, query: str, candidate: str,
                                  query_first_features: Tuple, 
                                  query_last_features: Tuple) -> float:
        """
        å¿«é€Ÿç›¸ä¼¼åº¦è¨ˆç®— - å„ªåŒ–ç‰ˆæœ¬
        
        Args:
            query, candidate: æŸ¥è©¢è©å’Œå€™é¸è©
            query_first_features, query_last_features: æŸ¥è©¢è©ç‰¹å¾µ
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•¸
        """
        # æª¢æŸ¥ç›¸ä¼¼åº¦ç·©å­˜
        cache_key = (query, candidate)
        if cache_key in self.similarity_cache:
            return self.similarity_cache[cache_key]
        
        # æå–å€™é¸è©ç‰¹å¾µ
        candidate_first = candidate[0]
        candidate_last = candidate[-1] if len(candidate) > 1 else candidate[0]
        
        candidate_features = self._get_cached_features(candidate_first, candidate_last)
        candidate_first_features, candidate_last_features = candidate_features
        
        # å¿«é€Ÿé¦–å­—ç›¸ä¼¼åº¦è¨ˆç®—
        first_similarity = self._fast_character_similarity(
            query_first_features, candidate_first_features
        )
        
        # å¿«é€Ÿå°¾å­—ç›¸ä¼¼åº¦è¨ˆç®—
        if len(query) == 1 and len(candidate) == 1:
            last_similarity = first_similarity
        elif len(query) == 1 or len(candidate) == 1:
            last_similarity = 0.5
        else:
            last_similarity = self._fast_character_similarity(
                query_last_features, candidate_last_features
            )
        
        # åŠ æ¬Šçµ„åˆ (é¦–å­—æ¬Šé‡æ›´é«˜)
        weighted_similarity = first_similarity * 0.7 + last_similarity * 0.3
        
        # é•·åº¦æ‡²ç½° (ç°¡åŒ–è¨ˆç®—)
        length_penalty = self._fast_length_penalty(len(query), len(candidate))
        
        # æœ€çµ‚ç›¸ä¼¼åº¦
        final_similarity = weighted_similarity * length_penalty
        
        # ç·©å­˜çµæœ
        self.similarity_cache[cache_key] = final_similarity
        
        return final_similarity
    
    def _fast_character_similarity(self, features1: Tuple, features2: Tuple) -> float:
        """
        å¿«é€Ÿå­—ç¬¦ç›¸ä¼¼åº¦è¨ˆç®—
        
        Args:
            features1, features2: ç‰¹å¾µå…ƒçµ„ (phonetic_features, finals_features)
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•¸
        """
        phonetic1, finals1 = features1
        phonetic2, finals2 = features2
        
        # è²æ¯å¿«é€Ÿæ¯”è¼ƒ
        consonant_sim = 1.0 if phonetic1.consonant == phonetic2.consonant else (
            0.8 if (phonetic1.consonant_group == phonetic2.consonant_group and 
                   phonetic1.consonant_group != "æœªçŸ¥åˆ†çµ„") else 0.0
        )
        
        # éŸ»æ¯å¿«é€Ÿæ¯”è¼ƒ
        finals_sim = 1.0 if finals1.finals == finals2.finals else (
            0.6 if finals1.finals_group == finals2.finals_group else 0.0
        )
        
        # åŠ æ¬Šçµ„åˆ (éŸ»æ¯æ¬Šé‡æ›´é«˜)
        return consonant_sim * 0.4 + finals_sim * 0.6
    
    def _fast_length_penalty(self, len1: int, len2: int) -> float:
        """
        å¿«é€Ÿé•·åº¦æ‡²ç½°è¨ˆç®—
        
        Args:
            len1, len2: å…©å€‹è©çš„é•·åº¦
            
        Returns:
            æ‡²ç½°ä¿‚æ•¸
        """
        len_diff = abs(len1 - len2)
        # ä½¿ç”¨æŸ¥è¡¨é¿å…æ¢ä»¶åˆ¤æ–·
        penalties = [1.0, 0.95, 0.85, 0.75, 0.7]
        return penalties[min(len_diff, 4)]
    
    def get_rerank_statistics(self) -> Dict[str, any]:
        """ç²å–é‡æ’å™¨æ€§èƒ½çµ±è¨ˆ"""
        stats = self.rerank_stats.copy()
        
        if stats["total_queries"] > 0:
            stats["avg_input_candidates"] = stats["total_candidates_input"] / stats["total_queries"]
            stats["avg_output_candidates"] = stats["total_candidates_output"] / stats["total_queries"]
            stats["avg_processing_time_ms"] = stats["total_time_ms"] / stats["total_queries"]
            stats["cache_hit_rate"] = stats["cache_hits"] / stats["total_queries"]
            stats["feature_cache_hit_rate"] = stats["feature_cache_hits"] / max(stats["total_candidates_input"], 1)
            stats["early_exit_rate"] = stats["early_exits"] / max(stats["total_candidates_input"], 1)
        
        stats["result_cache_size"] = len(self.result_cache)
        stats["character_features_cache_size"] = len(self.character_features_cache)
        stats["similarity_cache_size"] = len(self.similarity_cache)
        stats["cache_enabled"] = self.cache_enabled
        
        return stats
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç·©å­˜"""
        self.result_cache.clear()
        self.character_features_cache.clear()
        self.similarity_cache.clear()
        self.logger.info("Optimized L2 reranker caches cleared")


def test_optimized_l2_reranker():
    """æ¸¬è©¦å„ªåŒ–ç‰ˆL2é‡æ’å™¨"""
    print("ğŸ§ª æ¸¬è©¦å„ªåŒ–ç‰ˆL2é‡æ’å™¨")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–çµ„ä»¶
        dict_manager = SuperDictionaryManager(
            super_dict_path="data/super_dicts/super_dict_combined.json",
            super_dict_reversed_path="data/super_dicts/super_dict_reversed.json"
        )
        
        classifier = PhoneticClassifier()
        finals_analyzer = FinalsAnalyzer()
        optimized_reranker = OptimizedFirstLastSimilarityReranker(dict_manager, classifier, finals_analyzer)
        
        # æ¨¡æ“¬L1ç¯©é¸å¾Œçš„å€™é¸è©ï¼ˆæ¸›å°‘ä¸€äº›ä»¥åŠ å¿«æ¸¬è©¦ï¼‰
        test_candidates = list(dict_manager.get_all_words())[:10000]  # æ¸¬è©¦ç”¨1è¬å€‹å€™é¸
        
        # æ¸¬è©¦æ¡ˆä¾‹
        test_queries = [
            "çŸ¥é“",  # å¸¸è¦‹è©
            "è³‡é“",  # éŒ¯èª¤è©
            "åƒé£¯",  # å¸¸è¦‹è©
            "å®‰å…¨",  # é›¶è²æ¯è©
            "ä¾†äº†"   # çŸ­è©
        ]
        
        print("ğŸ“Š å„ªåŒ–ç‰ˆé‡æ’æ€§èƒ½æ¸¬è©¦:")
        print("-" * 50)
        
        total_time = 0
        for query in test_queries:
            start_time = time.time()
            reranked = optimized_reranker.rerank(query, test_candidates, top_k=500)
            processing_time = (time.time() - start_time) * 1000
            total_time += processing_time
            
            print(f"æŸ¥è©¢: '{query}'")
            print(f"  è¼¸å…¥å€™é¸æ•¸: {len(test_candidates):,}")
            print(f"  è¼¸å‡ºå€™é¸æ•¸: {len(reranked)}")
            print(f"  è™•ç†æ™‚é–“: {processing_time:.1f}ms")
            print(f"  å‰5å€‹çµæœ: {reranked[:5]}")
            print()
        
        avg_time = total_time / len(test_queries)
        print(f"å¹³å‡è™•ç†æ™‚é–“: {avg_time:.1f}ms")
        
        # çµ±è¨ˆä¿¡æ¯
        print("ğŸ“Š å„ªåŒ–çµ±è¨ˆä¿¡æ¯:")
        print("-" * 50)
        
        stats = optimized_reranker.get_rerank_statistics()
        print(f"  ç¸½æŸ¥è©¢æ•¸: {stats['total_queries']}")
        print(f"  å¹³å‡è™•ç†æ™‚é–“: {stats.get('avg_processing_time_ms', 0):.1f}ms")
        print(f"  çµæœç·©å­˜å‘½ä¸­ç‡: {stats.get('cache_hit_rate', 0):.1%}")
        print(f"  ç‰¹å¾µç·©å­˜å‘½ä¸­ç‡: {stats.get('feature_cache_hit_rate', 0):.1%}")
        print(f"  æ—©æœŸé€€å‡ºç‡: {stats.get('early_exit_rate', 0):.1%}")
        print(f"  å­—ç¬¦ç‰¹å¾µç·©å­˜: {stats.get('character_features_cache_size', 0)} æ¢ç›®")
        print(f"  ç›¸ä¼¼åº¦ç·©å­˜: {stats.get('similarity_cache_size', 0)} æ¢ç›®")
        
        # åˆ¤æ–·æ˜¯å¦é”åˆ°ç›®æ¨™
        success = avg_time < 50.0
        print(f"\nğŸ¯ æ€§èƒ½ç›®æ¨™é”æˆ: {'âœ…' if success else 'âŒ'} (ç›®æ¨™: <50ms, å¯¦éš›: {avg_time:.1f}ms)")
        
        return success
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(level=logging.INFO)
    
    # åŸ·è¡Œæ¸¬è©¦
    success = test_optimized_l2_reranker()
    print(f"\nå„ªåŒ–ç‰ˆL2é‡æ’å™¨æ¸¬è©¦ {'âœ… PASSED' if success else 'âŒ FAILED'}")