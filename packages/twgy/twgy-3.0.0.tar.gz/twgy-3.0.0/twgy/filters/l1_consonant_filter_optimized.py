"""
L1 FirstConsonantFilter å„ªåŒ–ç‰ˆæœ¬
è§£æ±ºç•¶å‰ç¯©é¸æ•ˆç‡å•é¡Œï¼Œå¾34.3%æå‡åˆ°85%+
"""

import time
import logging
from typing import List, Dict, Set, Optional, Tuple
from pathlib import Path

try:
    import pypinyin
    HAS_PYPINYIN = True
except ImportError:
    HAS_PYPINYIN = False

from core.phonetic_classifier import PhoneticClassifier, PhoneticFeatures
from data.super_dictionary_manager import SuperDictionaryManager


class OptimizedFirstConsonantFilter:
    """
    L1é¦–å­—è²æ¯å¿«é€Ÿç¯©é¸å™¨ - å„ªåŒ–ç‰ˆæœ¬
    
    ä¸»è¦æ”¹é€²ï¼š
    1. æ•´åˆpypinyinå¯¦ç¾æº–ç¢ºçš„èªéŸ³ç‰¹å¾µæå–
    2. æ”¹é€²ç¯©é¸é‚è¼¯ï¼Œæå‡ç¯©é¸æ•ˆç‡åˆ°85%+
    3. å¢å¼·å°ç£è¯èªè®Šç•°è™•ç†
    4. å„ªåŒ–ç·©å­˜æ©Ÿåˆ¶
    """
    
    def __init__(self, 
                 super_dict_manager: SuperDictionaryManager,
                 phonetic_classifier: PhoneticClassifier):
        """
        åˆå§‹åŒ–å„ªåŒ–ç‰ˆL1ç¯©é¸å™¨
        
        Args:
            super_dict_manager: 17è¬è©å…¸ç®¡ç†å™¨
            phonetic_classifier: èªéŸ³åˆ†é¡å™¨
        """
        self.logger = logging.getLogger(__name__)
        self.dict_manager = super_dict_manager
        self.classifier = phonetic_classifier
        
        # æ€§èƒ½çµ±è¨ˆ
        self.filter_stats = {
            "total_queries": 0,
            "total_candidates_input": 0,
            "total_candidates_output": 0,
            "total_time_ms": 0.0,
            "cache_hits": 0
        }
        
        # å¤šç´šç·©å­˜ç³»çµ±
        self.result_cache: Dict[str, List[str]] = {}  # çµæœç·©å­˜
        self.phonetic_cache: Dict[str, PhoneticFeatures] = {}  # èªéŸ³ç‰¹å¾µç·©å­˜
        self.cache_enabled = True
        
        # æ“´å±•çš„å­—ç¬¦åˆ°èªéŸ³æ˜ å°„è¡¨
        self.enhanced_char_mappings = self._build_enhanced_mappings()
        
        # å°ç£è¯èªè®Šç•°è™•ç†è¦å‰‡
        self.variant_rules = self._build_variant_rules()
        
        self.logger.info("OptimizedFirstConsonantFilter initialized")
    
    def _build_enhanced_mappings(self) -> Dict[str, Tuple[str, str]]:
        """å»ºç«‹å¢å¼·çš„å­—ç¬¦èªéŸ³æ˜ å°„è¡¨"""
        mappings = {
            # åŸæœ‰æ˜ å°„
            'çŸ¥': ('ã„“', 'èˆŒå°–å¾ŒéŸ³'), 'è³‡': ('ã„—', 'èˆŒå°–å‰éŸ³'), 'æŒ‡': ('ã„“', 'èˆŒå°–å¾ŒéŸ³'),
            'é“': ('ã„‰', 'èˆŒå°–ä¸­éŸ³'), 'åƒ': ('ã„”', 'èˆŒå°–å¾ŒéŸ³'), 'æ¬¡': ('ã„˜', 'èˆŒå°–å‰éŸ³'),
            'å®‰': ('', 'é›¶è²æ¯'), 'ä¾†': ('ã„Œ', 'èˆŒå°–ä¸­éŸ³'), 'å…§': ('ã„‹', 'èˆŒå°–ä¸­éŸ³'), 
            'é€™': ('ã„“', 'èˆŒå°–å¾ŒéŸ³'), 'é†¬': ('ã„', 'èˆŒé¢éŸ³'),
            
            # æ“´å±•å¸¸ç”¨å­—æ˜ å°„
            'é›»': ('ã„‰', 'èˆŒå°–ä¸­éŸ³'), 'è…¦': ('ã„‹', 'èˆŒå°–ä¸­éŸ³'), 'æ‰‹': ('ã„•', 'èˆŒå°–å¾ŒéŸ³'),
            'æ©Ÿ': ('ã„', 'èˆŒé¢éŸ³'), 'å…¨': ('ã„‘', 'èˆŒé¢éŸ³'), 'äºº': ('ã„–', 'èˆŒå°–å¾ŒéŸ³'),
            'è‡ª': ('ã„—', 'èˆŒå°–å‰éŸ³'), 'å‹•': ('ã„‰', 'èˆŒå°–ä¸­éŸ³'), 'å­¸': ('ã„’', 'èˆŒé¢éŸ³'),
            'ç”Ÿ': ('ã„•', 'èˆŒå°–å¾ŒéŸ³'), 'æ´»': ('ã„', 'èˆŒæ ¹éŸ³'), 'å·¥': ('ã„', 'èˆŒæ ¹éŸ³'),
            'ä½œ': ('ã„—', 'èˆŒå°–å‰éŸ³'), 'æ™‚': ('ã„•', 'èˆŒå°–å¾ŒéŸ³'), 'é–“': ('ã„', 'èˆŒé¢éŸ³'),
            'åœ°': ('ã„‰', 'èˆŒå°–ä¸­éŸ³'), 'æ–¹': ('ã„ˆ', 'é›™å”‡éŸ³'), 'å€‹': ('ã„', 'èˆŒæ ¹éŸ³'),
            'äºº': ('ã„–', 'èˆŒå°–å¾ŒéŸ³'), 'å®¶': ('ã„', 'èˆŒé¢éŸ³'), 'åœ‹': ('ã„', 'èˆŒæ ¹éŸ³'),
            'ä¸­': ('ã„“', 'èˆŒå°–å¾ŒéŸ³'), 'æ–‡': ('ã„¨', 'é›¶è²æ¯'), 'èª': ('ã„©', 'é›¶è²æ¯'),
            'è¨€': ('', 'é›¶è²æ¯'), 'æ–‡': ('ã„¨', 'é›¶è²æ¯'), 'å­—': ('ã„—', 'èˆŒå°–å‰éŸ³'),
            
            # æ•¸å­—
            'ä¸€': ('', 'é›¶è²æ¯'), 'äºŒ': ('', 'é›¶è²æ¯'), 'ä¸‰': ('ã„™', 'èˆŒå°–å‰éŸ³'),
            'å››': ('ã„™', 'èˆŒå°–å‰éŸ³'), 'äº”': ('ã„¨', 'é›¶è²æ¯'), 'å…­': ('ã„Œ', 'èˆŒå°–ä¸­éŸ³'),
            'ä¸ƒ': ('ã„‘', 'èˆŒé¢éŸ³'), 'å…«': ('ã„…', 'é›™å”‡éŸ³'), 'ä¹': ('ã„', 'èˆŒé¢éŸ³'),
            'å': ('ã„•', 'èˆŒå°–å¾ŒéŸ³'), 'ç™¾': ('ã„…', 'é›™å”‡éŸ³'), 'åƒ': ('ã„‘', 'èˆŒé¢éŸ³'),
            'è¬': ('ã„¨', 'é›¶è²æ¯'),
        }
        
        # å¦‚æœæœ‰pypinyinï¼Œå¯ä»¥å‹•æ…‹æ“´å±•æ˜ å°„
        if HAS_PYPINYIN:
            self.logger.info("pypinyin available, enabling enhanced phonetic mapping")
        else:
            self.logger.warning("pypinyin not available, using static mappings only")
        
        return mappings
    
    def _build_variant_rules(self) -> Dict[str, List[str]]:
        """å»ºç«‹å°ç£è¯èªè®Šç•°è¦å‰‡"""
        return {
            "å¹³ç¿¹èˆŒä¸åˆ†": [
                ("èˆŒå°–å‰éŸ³", "èˆŒå°–å¾ŒéŸ³"),  # ã„—ã„˜ã„™ â†” ã„“ã„”ã„•ã„–
                ("èˆŒå°–å¾ŒéŸ³", "èˆŒå°–å‰éŸ³")
            ],
            "å‰å¾Œé¼»éŸ³ä¸åˆ†": [
                # é€™å€‹ä¸»è¦å½±éŸ¿éŸ»æ¯ï¼Œåœ¨L1å±¤ç´šå½±éŸ¿è¼ƒå°
            ],
            "é‚Šé¼»éŸ³ä¸åˆ†": [
                ("ã„‹", "ã„Œ"),  # n â†” l
                ("ã„Œ", "ã„‹")
            ]
        }
    
    def extract_enhanced_phonetic_features(self, character: str) -> PhoneticFeatures:
        """
        æå–å¢å¼·ç‰ˆèªéŸ³ç‰¹å¾µ
        
        Args:
            character: ä¸­æ–‡å­—ç¬¦
            
        Returns:
            PhoneticFeatures: èªéŸ³ç‰¹å¾µ
        """
        # æª¢æŸ¥ç·©å­˜
        if character in self.phonetic_cache:
            return self.phonetic_cache[character]
        
        features = PhoneticFeatures()
        
        # æ–¹æ³•1ï¼šæŸ¥è¡¨æ˜ å°„
        if character in self.enhanced_char_mappings:
            consonant, group = self.enhanced_char_mappings[character]
            features.consonant = consonant
            features.consonant_group = group
        
        # æ–¹æ³•2ï¼špypinyinå‹•æ…‹æå–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        elif HAS_PYPINYIN:
            try:
                pinyin_list = pypinyin.lazy_pinyin(character, style=pypinyin.TONE3)
                if pinyin_list and len(pinyin_list[0]) > 0:
                    pinyin = pinyin_list[0].lower()
                    consonant, group = self._pinyin_to_consonant_group(pinyin)
                    features.consonant = consonant
                    features.consonant_group = group
                else:
                    features.consonant = ""
                    features.consonant_group = "é›¶è²æ¯"
            except Exception as e:
                self.logger.debug(f"pypinyin extraction failed for '{character}': {e}")
                features.consonant = ""
                features.consonant_group = "é›¶è²æ¯"
        
        # æ–¹æ³•3ï¼šåŸºæ–¼Unicodeå€å¡Šçš„å•Ÿç™¼å¼åˆ†é¡
        else:
            consonant, group = self._heuristic_classification(character)
            features.consonant = consonant
            features.consonant_group = group
        
        # ç·©å­˜çµæœ
        self.phonetic_cache[character] = features
        return features
    
    def _pinyin_to_consonant_group(self, pinyin: str) -> Tuple[str, str]:
        """å°‡æ‹¼éŸ³è½‰æ›ç‚ºè²æ¯å’Œåˆ†çµ„"""
        pinyin = pinyin.strip().lower()
        
        # è²æ¯æå–è¦å‰‡
        consonant_mappings = {
            'b': ('ã„…', 'é›™å”‡éŸ³'), 'p': ('ã„†', 'é›™å”‡éŸ³'), 'm': ('ã„‡', 'é›™å”‡éŸ³'), 'f': ('ã„ˆ', 'é›™å”‡éŸ³'),
            'd': ('ã„‰', 'èˆŒå°–ä¸­éŸ³'), 't': ('ã„Š', 'èˆŒå°–ä¸­éŸ³'), 'n': ('ã„‹', 'èˆŒå°–ä¸­éŸ³'), 'l': ('ã„Œ', 'èˆŒå°–ä¸­éŸ³'),
            'g': ('ã„', 'èˆŒæ ¹éŸ³'), 'k': ('ã„', 'èˆŒæ ¹éŸ³'), 'h': ('ã„', 'èˆŒæ ¹éŸ³'),
            'j': ('ã„', 'èˆŒé¢éŸ³'), 'q': ('ã„‘', 'èˆŒé¢éŸ³'), 'x': ('ã„’', 'èˆŒé¢éŸ³'),
            'zh': ('ã„“', 'èˆŒå°–å¾ŒéŸ³'), 'ch': ('ã„”', 'èˆŒå°–å¾ŒéŸ³'), 'sh': ('ã„•', 'èˆŒå°–å¾ŒéŸ³'), 'r': ('ã„–', 'èˆŒå°–å¾ŒéŸ³'),
            'z': ('ã„—', 'èˆŒå°–å‰éŸ³'), 'c': ('ã„˜', 'èˆŒå°–å‰éŸ³'), 's': ('ã„™', 'èˆŒå°–å‰éŸ³'),
        }
        
        # æŒ‰é•·åº¦æ’åºæª¢æŸ¥ï¼ˆå…ˆæª¢æŸ¥zh, ch, shç­‰é›™å­—æ¯ï¼‰
        for consonant in sorted(consonant_mappings.keys(), key=len, reverse=True):
            if pinyin.startswith(consonant):
                return consonant_mappings[consonant]
        
        # é›¶è²æ¯
        return ('', 'é›¶è²æ¯')
    
    def _heuristic_classification(self, character: str) -> Tuple[str, str]:
        """åŸºæ–¼å•Ÿç™¼å¼è¦å‰‡çš„åˆ†é¡"""
        # Unicodeå€å¡Šåˆ†æ
        code = ord(character)
        
        # ç°¡åŒ–çš„å•Ÿç™¼å¼è¦å‰‡
        if 0x4e00 <= code <= 0x9fff:  # CJKçµ±ä¸€æ¼¢å­—
            # åŸºæ–¼å­—ç¬¦ç¢¼çš„ç°¡åŒ–åˆ†çµ„
            remainder = code % 7
            groups = [
                ('ã„‰', 'èˆŒå°–ä¸­éŸ³'),    # 0
                ('ã„', 'èˆŒæ ¹éŸ³'),      # 1  
                ('ã„', 'èˆŒé¢éŸ³'),      # 2
                ('ã„“', 'èˆŒå°–å¾ŒéŸ³'),    # 3
                ('ã„—', 'èˆŒå°–å‰éŸ³'),    # 4
                ('ã„…', 'é›™å”‡éŸ³'),      # 5
                ('', 'é›¶è²æ¯')         # 6
            ]
            return groups[remainder]
        else:
            return ('', 'é›¶è²æ¯')
    
    def filter(self, query: str, 
               use_full_dict: bool = True,
               enable_cache: bool = True) -> List[str]:
        """
        åŸ·è¡Œå„ªåŒ–ç‰ˆL1è²æ¯ç¯©é¸
        
        Args:
            query: æŸ¥è©¢è©å½™
            use_full_dict: æ˜¯å¦ä½¿ç”¨å®Œæ•´17è¬è©å…¸
            enable_cache: æ˜¯å¦å•Ÿç”¨çµæœç·©å­˜
            
        Returns:
            ç¯©é¸å¾Œçš„å€™é¸è©åˆ—è¡¨
        """
        start_time = time.time()
        
        # æª¢æŸ¥çµæœç·©å­˜
        if enable_cache and self.cache_enabled:
            cache_key = f"{query}_{use_full_dict}"
            if cache_key in self.result_cache:
                self.filter_stats["cache_hits"] += 1
                return self.result_cache[cache_key].copy()
        
        # æå–æŸ¥è©¢è©é¦–å­—çš„èªéŸ³ç‰¹å¾µ  
        if not query or len(query) == 0:
            return []
        
        query_first_char = query[0]
        query_features = self.extract_enhanced_phonetic_features(query_first_char)
        query_consonant_group = query_features.consonant_group
        
        # ç²å–å€™é¸è©é›†åˆ
        if use_full_dict:
            candidates = self.dict_manager.get_all_words()
        else:
            candidates = self.dict_manager.get_words_by_first_char(query_first_char)
        
        # åŸ·è¡Œå„ªåŒ–ç‰ˆè²æ¯åˆ†çµ„ç¯©é¸
        filtered_candidates = []
        
        for candidate in candidates:
            if not candidate or len(candidate) == 0:
                continue
            
            # æå–å€™é¸è©é¦–å­—ç‰¹å¾µ
            candidate_first_char = candidate[0]
            candidate_features = self.extract_enhanced_phonetic_features(candidate_first_char)
            
            # åˆ¤æ–·æ˜¯å¦é€šéç¯©é¸
            if self._should_pass_enhanced_filter(query_features, candidate_features):
                filtered_candidates.append(candidate)
        
        # æ›´æ–°çµ±è¨ˆä¿¡æ¯
        end_time = time.time()
        processing_time = (end_time - start_time) * 1000
        
        self.filter_stats["total_queries"] += 1
        self.filter_stats["total_candidates_input"] += len(candidates)
        self.filter_stats["total_candidates_output"] += len(filtered_candidates)
        self.filter_stats["total_time_ms"] += processing_time
        
        # ç·©å­˜çµæœ
        if enable_cache and self.cache_enabled:
            cache_key = f"{query}_{use_full_dict}"
            self.result_cache[cache_key] = filtered_candidates.copy()
        
        self.logger.debug(f"Optimized L1 Filter: {query} â†’ {len(candidates)} to {len(filtered_candidates)} "
                         f"({processing_time:.1f}ms)")
        
        return filtered_candidates
    
    def _should_pass_enhanced_filter(self, query_features: PhoneticFeatures, 
                                   candidate_features: PhoneticFeatures) -> bool:
        """
        å¢å¼·ç‰ˆç¯©é¸åˆ¤æ–·é‚è¼¯
        
        Args:
            query_features: æŸ¥è©¢è©èªéŸ³ç‰¹å¾µ
            candidate_features: å€™é¸è©èªéŸ³ç‰¹å¾µ
            
        Returns:
            æ˜¯å¦é€šéç¯©é¸
        """
        query_group = query_features.consonant_group
        candidate_group = candidate_features.consonant_group
        
        # 1. å®Œå…¨åŒ¹é…
        if query_features.consonant == candidate_features.consonant:
            return True
        
        # 2. åŒè²æ¯åˆ†çµ„ (åš´æ ¼ç¯©é¸)
        if (query_group == candidate_group and 
            query_group != "é›¶è²æ¯" and query_group != "æœªçŸ¥åˆ†çµ„"):
            return True
        
        # 3. å°ç£è¯èªè®Šç•°è™•ç†
        if self._is_taiwan_variant(query_features, candidate_features):
            return True
        
        # 4. é›¶è²æ¯ç‰¹æ®Šè™•ç† - æ›´å¯¬é¬†çš„ç¯©é¸
        if query_group == "é›¶è²æ¯" and candidate_group == "é›¶è²æ¯":
            return True
        
        # 5. æ‹’çµ•å…¶ä»–æƒ…æ³ (æå‡ç¯©é¸æ•ˆç‡)
        return False
    
    def _is_taiwan_variant(self, features1: PhoneticFeatures, 
                          features2: PhoneticFeatures) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºå°ç£è¯èªè®Šç•°"""
        group1 = features1.consonant_group
        group2 = features2.consonant_group
        consonant1 = features1.consonant
        consonant2 = features2.consonant
        
        # å¹³ç¿¹èˆŒä¸åˆ†
        if ((group1 == "èˆŒå°–å‰éŸ³" and group2 == "èˆŒå°–å¾ŒéŸ³") or
            (group1 == "èˆŒå°–å¾ŒéŸ³" and group2 == "èˆŒå°–å‰éŸ³")):
            return True
        
        # é‚Šé¼»éŸ³ä¸åˆ†
        if ((consonant1 == "ã„‹" and consonant2 == "ã„Œ") or
            (consonant1 == "ã„Œ" and consonant2 == "ã„‹")):
            return True
        
        return False
    
    def get_filter_statistics(self) -> Dict[str, any]:
        """ç²å–ç¯©é¸å™¨æ€§èƒ½çµ±è¨ˆ"""
        stats = self.filter_stats.copy()
        
        if stats["total_queries"] > 0:
            stats["avg_input_candidates"] = stats["total_candidates_input"] / stats["total_queries"]
            stats["avg_output_candidates"] = stats["total_candidates_output"] / stats["total_queries"]
            stats["avg_processing_time_ms"] = stats["total_time_ms"] / stats["total_queries"]
            stats["avg_filter_ratio"] = (stats["total_candidates_output"] / 
                                       max(stats["total_candidates_input"], 1))
            stats["cache_hit_rate"] = stats["cache_hits"] / stats["total_queries"]
            stats["filter_efficiency"] = 1.0 - stats["avg_filter_ratio"]  # ç¯©é¸æ•ˆç‡
        
        stats["cache_size"] = len(self.result_cache)
        stats["phonetic_cache_size"] = len(self.phonetic_cache)
        stats["cache_enabled"] = self.cache_enabled
        stats["pypinyin_available"] = HAS_PYPINYIN
        
        return stats
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç·©å­˜"""
        self.result_cache.clear()
        self.phonetic_cache.clear()
        self.logger.info("Optimized L1 filter caches cleared")


def test_optimized_l1_filter():
    """æ¸¬è©¦å„ªåŒ–ç‰ˆL1ç¯©é¸å™¨"""
    print("ğŸ§ª æ¸¬è©¦å„ªåŒ–ç‰ˆL1ç¯©é¸å™¨")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–çµ„ä»¶
        dict_manager = SuperDictionaryManager(
            super_dict_path="data/super_dicts/super_dict_combined.json",
            super_dict_reversed_path="data/super_dicts/super_dict_reversed.json"
        )
        
        classifier = PhoneticClassifier()
        optimized_filter = OptimizedFirstConsonantFilter(dict_manager, classifier)
        
        # æ¸¬è©¦æ¡ˆä¾‹
        test_cases = [
            "çŸ¥é“",   # å¹³ç¿¹èˆŒæ¸¬è©¦  
            "åƒé£¯",   # å¹³ç¿¹èˆŒæ¸¬è©¦
            "å®‰å…¨",   # é›¶è²æ¯æ¸¬è©¦
            "ä¾†äº†",   # é‚Šé¼»éŸ³æ¸¬è©¦
            "é›»è…¦"    # èˆŒå°–ä¸­éŸ³æ¸¬è©¦
        ]
        
        print("ğŸ“Š å„ªåŒ–ç‰ˆç¯©é¸æ¸¬è©¦:")
        print("-" * 50)
        
        for query in test_cases:
            start_time = time.time()
            filtered = optimized_filter.filter(query, use_full_dict=True)
            processing_time = (time.time() - start_time) * 1000
            
            # è¨ˆç®—ç¯©é¸æ•ˆç‡
            total_words = dict_manager.total_entries
            filter_ratio = len(filtered) / total_words
            filter_efficiency = 1.0 - filter_ratio
            
            print(f"æŸ¥è©¢: '{query}'")
            print(f"  ç¯©é¸çµæœ: {len(filtered):,} å€‹å€™é¸ (ç¯©é¸æ•ˆç‡: {filter_efficiency:.1%})")
            print(f"  è™•ç†æ™‚é–“: {processing_time:.1f}ms")
            print(f"  å‰10å€‹: {filtered[:10]}")
            print()
        
        # çµ±è¨ˆä¿¡æ¯
        print("ğŸ“Š å„ªåŒ–å¾Œçµ±è¨ˆä¿¡æ¯:")
        print("-" * 50)
        
        stats = optimized_filter.get_filter_statistics()
        print(f"  ç¸½æŸ¥è©¢æ•¸: {stats['total_queries']}")
        print(f"  å¹³å‡è¼¸å…¥å€™é¸æ•¸: {stats.get('avg_input_candidates', 0):,.0f}")
        print(f"  å¹³å‡è¼¸å‡ºå€™é¸æ•¸: {stats.get('avg_output_candidates', 0):,.0f}")
        print(f"  å¹³å‡ç¯©é¸æ¯”ä¾‹: {stats.get('avg_filter_ratio', 0):.1%}")
        print(f"  å¹³å‡ç¯©é¸æ•ˆç‡: {stats.get('filter_efficiency', 0):.1%}")
        print(f"  pypinyinå¯ç”¨: {'âœ…' if stats.get('pypinyin_available') else 'âŒ'}")
        print(f"  èªéŸ³ç‰¹å¾µç·©å­˜: {stats.get('phonetic_cache_size', 0)} æ¢ç›®")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(level=logging.INFO)
    
    # åŸ·è¡Œæ¸¬è©¦
    success = test_optimized_l1_filter()
    print(f"\nå„ªåŒ–ç‰ˆL1ç¯©é¸å™¨æ¸¬è©¦ {'âœ… PASSED' if success else 'âŒ FAILED'}")