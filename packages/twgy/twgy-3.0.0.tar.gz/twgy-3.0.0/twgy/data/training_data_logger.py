"""
TrainingDataLogger - æ©Ÿå™¨å­¸ç¿’è¨“ç·´æ•¸æ“šæ”¶é›†ç³»çµ±
æ”¶é›†100%èªéŸ³é‡æ’æ¡ˆä¾‹ï¼Œå»ºç«‹ç¬¬äºŒæœŸæ©Ÿå™¨å­¸ç¿’æ¨¡å‹è¨“ç·´æ•¸æ“šåº«
"""

import json
import time
import logging
import sqlite3
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import hashlib


@dataclass 
class TrainingCase:
    """å–®å€‹è¨“ç·´æ¡ˆä¾‹çµæ§‹"""
    case_id: str = ""
    timestamp: float = 0.0
    query: str = ""
    query_length: int = 0
    
    # L1ç¯©é¸çµæœ
    l1_candidates_count: int = 0
    l1_processing_time_ms: float = 0.0
    
    # L2é‡æ’çµæœ  
    l2_candidates_count: int = 0
    l2_processing_time_ms: float = 0.0
    
    # L3ç²¾æ’çµæœ
    l3_candidates_count: int = 0
    l3_processing_time_ms: float = 0.0
    
    # æœ€çµ‚çµæœ
    final_top_candidates: List[str] = None
    total_processing_time_ms: float = 0.0
    
    # è¤‡é›œåº¦è©•ä¼°
    complexity_level: str = ""  # simple/medium/complex
    complexity_score: float = 0.0
    
    # èªéŸ³ç‰¹å¾µ
    phonetic_features: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.final_top_candidates is None:
            self.final_top_candidates = []
        if self.phonetic_features is None:
            self.phonetic_features = {}


@dataclass
class SessionSummary:
    """æœƒè©±çµ±è¨ˆæ‘˜è¦"""
    session_id: str = ""
    start_time: float = 0.0
    end_time: float = 0.0
    total_queries: int = 0
    complexity_distribution: Dict[str, int] = None
    performance_stats: Dict[str, float] = None
    
    def __post_init__(self):
        if self.complexity_distribution is None:
            self.complexity_distribution = {"simple": 0, "medium": 0, "complex": 0}
        if self.performance_stats is None:
            self.performance_stats = {}


class TrainingDataLogger:
    """
    æ©Ÿå™¨å­¸ç¿’è¨“ç·´æ•¸æ“šæ”¶é›†ç³»çµ±
    
    åŠŸèƒ½ï¼š
    1. è¨˜éŒ„100%èªéŸ³é‡æ’è™•ç†æ¡ˆä¾‹
    2. æ”¶é›†å®Œæ•´ç®¡é“çš„æ€§èƒ½å’Œçµæœæ•¸æ“š
    3. è©•ä¼°å’Œåˆ†é¡æ¡ˆä¾‹è¤‡é›œåº¦
    4. å»ºç«‹çµæ§‹åŒ–æ•¸æ“šåº«ä¾›æ©Ÿå™¨å­¸ç¿’ä½¿ç”¨
    5. æ”¯æ´æ•¸æ“šå°å‡ºå’Œåˆ†æåŠŸèƒ½
    """
    
    def __init__(self, 
                 data_dir: str = "data/training_logs",
                 db_name: str = "phonetic_training.db",
                 enable_file_logging: bool = True,
                 enable_db_logging: bool = True):
        """
        åˆå§‹åŒ–è¨“ç·´æ•¸æ“šè¨˜éŒ„å™¨
        
        Args:
            data_dir: æ•¸æ“šå­˜å„²ç›®éŒ„
            db_name: SQLiteæ•¸æ“šåº«æ–‡ä»¶å
            enable_file_logging: æ˜¯å¦å•Ÿç”¨æ–‡ä»¶è¨˜éŒ„
            enable_db_logging: æ˜¯å¦å•Ÿç”¨æ•¸æ“šåº«è¨˜éŒ„
        """
        self.logger = logging.getLogger(__name__)
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.db_path = self.data_dir / db_name
        self.enable_file_logging = enable_file_logging
        self.enable_db_logging = enable_db_logging
        
        # ç•¶å‰æœƒè©±
        self.session_id = self._generate_session_id()
        self.session_start_time = time.time()
        
        # å…§å­˜ç·©å­˜
        self.training_cases: List[TrainingCase] = []
        self.session_stats = {
            "total_queries": 0,
            "complexity_counts": {"simple": 0, "medium": 0, "complex": 0},
            "total_processing_time": 0.0,
            "avg_l1_time": 0.0,
            "avg_l2_time": 0.0,
            "avg_l3_time": 0.0
        }
        
        # åˆå§‹åŒ–æ•¸æ“šåº«
        if self.enable_db_logging:
            self._init_database()
        
        self.logger.info(f"TrainingDataLogger initialized (Session: {self.session_id})")
    
    def _generate_session_id(self) -> str:
        """ç”Ÿæˆæœƒè©±ID"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"session_{timestamp}"
    
    def _init_database(self):
        """åˆå§‹åŒ–SQLiteæ•¸æ“šåº«"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # å‰µå»ºè¨“ç·´æ¡ˆä¾‹è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS training_cases (
                    case_id TEXT PRIMARY KEY,
                    session_id TEXT,
                    timestamp REAL,
                    query TEXT,
                    query_length INTEGER,
                    l1_candidates_count INTEGER,
                    l1_processing_time_ms REAL,
                    l2_candidates_count INTEGER,
                    l2_processing_time_ms REAL,
                    l3_candidates_count INTEGER,
                    l3_processing_time_ms REAL,
                    final_top_candidates TEXT,
                    total_processing_time_ms REAL,
                    complexity_level TEXT,
                    complexity_score REAL,
                    phonetic_features TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # å‰µå»ºæœƒè©±æ‘˜è¦è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS session_summaries (
                    session_id TEXT PRIMARY KEY,
                    start_time REAL,
                    end_time REAL,
                    total_queries INTEGER,
                    complexity_distribution TEXT,
                    performance_stats TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # å‰µå»ºç´¢å¼•
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON training_cases(timestamp)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_complexity ON training_cases(complexity_level)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_session ON training_cases(session_id)')
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"Database initialized: {self.db_path}")
            
        except Exception as e:
            self.logger.error(f"Database initialization failed: {e}")
            raise
    
    def log_training_case(self, 
                         query: str,
                         l1_result: Dict[str, Any],
                         l2_result: Dict[str, Any], 
                         l3_result: Dict[str, Any],
                         phonetic_features: Dict[str, Any] = None) -> str:
        """
        è¨˜éŒ„ä¸€å€‹å®Œæ•´çš„è¨“ç·´æ¡ˆä¾‹
        
        Args:
            query: æŸ¥è©¢è©å½™
            l1_result: L1ç¯©é¸çµæœ
            l2_result: L2é‡æ’çµæœ
            l3_result: L3ç²¾æ’çµæœ
            phonetic_features: èªéŸ³ç‰¹å¾µæ•¸æ“š
            
        Returns:
            æ¡ˆä¾‹ID
        """
        # ç”Ÿæˆå”¯ä¸€æ¡ˆä¾‹ID
        case_id = self._generate_case_id(query)
        
        # å‰µå»ºè¨“ç·´æ¡ˆä¾‹
        training_case = TrainingCase(
            case_id=case_id,
            timestamp=time.time(),
            query=query,
            query_length=len(query),
            
            l1_candidates_count=l1_result.get("candidates_count", 0),
            l1_processing_time_ms=l1_result.get("processing_time_ms", 0.0),
            
            l2_candidates_count=l2_result.get("candidates_count", 0),
            l2_processing_time_ms=l2_result.get("processing_time_ms", 0.0),
            
            l3_candidates_count=l3_result.get("candidates_count", 0),
            l3_processing_time_ms=l3_result.get("processing_time_ms", 0.0),
            
            final_top_candidates=l3_result.get("top_candidates", []),
            total_processing_time_ms=(
                l1_result.get("processing_time_ms", 0.0) +
                l2_result.get("processing_time_ms", 0.0) +
                l3_result.get("processing_time_ms", 0.0)
            ),
            
            complexity_level=l3_result.get("complexity_level", "medium"),
            complexity_score=l3_result.get("complexity_score", 0.5),
            
            phonetic_features=phonetic_features or {}
        )
        
        # æ·»åŠ åˆ°å…§å­˜ç·©å­˜
        self.training_cases.append(training_case)
        
        # æ›´æ–°æœƒè©±çµ±è¨ˆ
        self._update_session_stats(training_case)
        
        # å¯«å…¥æ•¸æ“šåº«
        if self.enable_db_logging:
            self._save_to_database(training_case)
        
        # å¯«å…¥æ–‡ä»¶
        if self.enable_file_logging:
            self._save_to_file(training_case)
        
        self.logger.debug(f"Training case logged: {case_id}")
        return case_id
    
    def _generate_case_id(self, query: str) -> str:
        """ç”Ÿæˆæ¡ˆä¾‹å”¯ä¸€ID"""
        timestamp = str(time.time())
        content = f"{query}_{timestamp}_{self.session_id}"
        case_hash = hashlib.md5(content.encode()).hexdigest()[:12]
        return f"case_{case_hash}"
    
    def _update_session_stats(self, case: TrainingCase):
        """æ›´æ–°æœƒè©±çµ±è¨ˆ"""
        self.session_stats["total_queries"] += 1
        self.session_stats["complexity_counts"][case.complexity_level] += 1
        self.session_stats["total_processing_time"] += case.total_processing_time_ms
        
        # æ›´æ–°å¹³å‡æ™‚é–“
        total = self.session_stats["total_queries"]
        self.session_stats["avg_l1_time"] = (
            (self.session_stats["avg_l1_time"] * (total - 1) + case.l1_processing_time_ms) / total
        )
        self.session_stats["avg_l2_time"] = (
            (self.session_stats["avg_l2_time"] * (total - 1) + case.l2_processing_time_ms) / total
        )
        self.session_stats["avg_l3_time"] = (
            (self.session_stats["avg_l3_time"] * (total - 1) + case.l3_processing_time_ms) / total
        )
    
    def _save_to_database(self, case: TrainingCase):
        """ä¿å­˜æ¡ˆä¾‹åˆ°SQLiteæ•¸æ“šåº«"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO training_cases (
                    case_id, session_id, timestamp, query, query_length,
                    l1_candidates_count, l1_processing_time_ms,
                    l2_candidates_count, l2_processing_time_ms,
                    l3_candidates_count, l3_processing_time_ms,
                    final_top_candidates, total_processing_time_ms,
                    complexity_level, complexity_score, phonetic_features
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                case.case_id, self.session_id, case.timestamp,
                case.query, case.query_length,
                case.l1_candidates_count, case.l1_processing_time_ms,
                case.l2_candidates_count, case.l2_processing_time_ms,
                case.l3_candidates_count, case.l3_processing_time_ms,
                json.dumps(case.final_top_candidates, ensure_ascii=False),
                case.total_processing_time_ms,
                case.complexity_level, case.complexity_score,
                json.dumps(case.phonetic_features, ensure_ascii=False)
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Database save failed: {e}")
    
    def _save_to_file(self, case: TrainingCase):
        """ä¿å­˜æ¡ˆä¾‹åˆ°JSONæ–‡ä»¶"""
        try:
            # æŒ‰æ—¥æœŸçµ„ç¹”æ–‡ä»¶
            date_str = datetime.fromtimestamp(case.timestamp).strftime("%Y%m%d")
            file_path = self.data_dir / f"training_cases_{date_str}.jsonl"
            
            # è¿½åŠ å¯«å…¥JSONLæ ¼å¼
            with open(file_path, 'a', encoding='utf-8') as f:
                json.dump(asdict(case), f, ensure_ascii=False)
                f.write('\n')
                
        except Exception as e:
            self.logger.error(f"File save failed: {e}")
    
    def finalize_session(self) -> SessionSummary:
        """çµæŸç•¶å‰æœƒè©±ä¸¦ç”Ÿæˆæ‘˜è¦"""
        session_end_time = time.time()
        
        # å‰µå»ºæœƒè©±æ‘˜è¦
        summary = SessionSummary(
            session_id=self.session_id,
            start_time=self.session_start_time,
            end_time=session_end_time,
            total_queries=self.session_stats["total_queries"],
            complexity_distribution=self.session_stats["complexity_counts"].copy(),
            performance_stats={
                "total_time_ms": self.session_stats["total_processing_time"],
                "avg_l1_time_ms": self.session_stats["avg_l1_time"],
                "avg_l2_time_ms": self.session_stats["avg_l2_time"],
                "avg_l3_time_ms": self.session_stats["avg_l3_time"],
                "session_duration_sec": session_end_time - self.session_start_time
            }
        )
        
        # ä¿å­˜æœƒè©±æ‘˜è¦
        if self.enable_db_logging:
            self._save_session_summary(summary)
        
        self.logger.info(f"Session finalized: {self.session_id} "
                        f"({summary.total_queries} queries)")
        
        return summary
    
    def _save_session_summary(self, summary: SessionSummary):
        """ä¿å­˜æœƒè©±æ‘˜è¦åˆ°æ•¸æ“šåº«"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO session_summaries (
                    session_id, start_time, end_time, total_queries,
                    complexity_distribution, performance_stats
                ) VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                summary.session_id, summary.start_time, summary.end_time,
                summary.total_queries,
                json.dumps(summary.complexity_distribution, ensure_ascii=False),
                json.dumps(summary.performance_stats, ensure_ascii=False)
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Session summary save failed: {e}")
    
    def export_training_data(self, 
                           output_path: str,
                           format: str = "json",
                           complexity_filter: Optional[str] = None,
                           limit: Optional[int] = None) -> int:
        """
        å°å‡ºè¨“ç·´æ•¸æ“š
        
        Args:
            output_path: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
            format: è¼¸å‡ºæ ¼å¼ (json/csv)
            complexity_filter: è¤‡é›œåº¦ç¯©é¸ (simple/medium/complex)
            limit: å°å‡ºæ•¸é‡é™åˆ¶
            
        Returns:
            å°å‡ºçš„è¨˜éŒ„æ•¸
        """
        try:
            conn = sqlite3.connect(self.db_path)
            
            # æ§‹å»ºæŸ¥è©¢
            query = "SELECT * FROM training_cases"
            params = []
            
            if complexity_filter:
                query += " WHERE complexity_level = ?"
                params.append(complexity_filter)
            
            query += " ORDER BY timestamp DESC"
            
            if limit:
                query += " LIMIT ?"
                params.append(limit)
            
            cursor = conn.cursor()
            cursor.execute(query, params)
            rows = cursor.fetchall()
            columns = [desc[0] for desc in cursor.description]
            
            conn.close()
            
            # å°å‡ºæ•¸æ“š
            if format.lower() == "json":
                data = [dict(zip(columns, row)) for row in rows]
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
            
            elif format.lower() == "csv":
                import pandas as pd
                df = pd.DataFrame(rows, columns=columns)
                df.to_csv(output_path, index=False, encoding='utf-8')
            
            self.logger.info(f"Exported {len(rows)} records to {output_path}")
            return len(rows)
            
        except Exception as e:
            self.logger.error(f"Export failed: {e}")
            return 0
    
    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–è¨“ç·´æ•¸æ“šçµ±è¨ˆä¿¡æ¯"""
        stats = {
            "session_id": self.session_id,
            "session_stats": self.session_stats.copy(),
            "memory_cache_size": len(self.training_cases),
            "data_dir": str(self.data_dir),
            "db_path": str(self.db_path),
            "logging_enabled": {
                "file": self.enable_file_logging,
                "database": self.enable_db_logging
            }
        }
        
        # æ•¸æ“šåº«çµ±è¨ˆ
        if self.enable_db_logging and self.db_path.exists():
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute("SELECT COUNT(*) FROM training_cases")
                stats["total_cases_in_db"] = cursor.fetchone()[0]
                
                cursor.execute("SELECT COUNT(DISTINCT session_id) FROM training_cases")
                stats["total_sessions"] = cursor.fetchone()[0]
                
                cursor.execute("""
                    SELECT complexity_level, COUNT(*) 
                    FROM training_cases 
                    GROUP BY complexity_level
                """)
                complexity_dist = dict(cursor.fetchall())
                stats["db_complexity_distribution"] = complexity_dist
                
                conn.close()
                
            except Exception as e:
                stats["db_error"] = str(e)
        
        return stats


def test_training_data_logger():
    """æ¸¬è©¦TrainingDataLoggeråŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦TrainingDataLoggeråŠŸèƒ½")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–æ•¸æ“šè¨˜éŒ„å™¨
        logger = TrainingDataLogger(
            data_dir="data/training_logs",
            db_name="test_training.db"
        )
        
        # æ¨¡æ“¬è¨“ç·´æ¡ˆä¾‹
        test_cases = [
            ("çŸ¥é“", "simple", 0.9),
            ("è³‡é“", "medium", 0.6),
            ("åƒé£¯", "simple", 0.85),
            ("å®‰å…¨", "complex", 0.3),
        ]
        
        print("ğŸ“Š è¨˜éŒ„è¨“ç·´æ¡ˆä¾‹:")
        print("-" * 40)
        
        for query, complexity, score in test_cases:
            # æ¨¡æ“¬L1/L2/L3çµæœ
            l1_result = {
                "candidates_count": 2500,
                "processing_time_ms": 85.0
            }
            
            l2_result = {
                "candidates_count": 500,
                "processing_time_ms": 45.0
            }
            
            l3_result = {
                "candidates_count": 10,
                "processing_time_ms": 8.0,
                "top_candidates": [query, f"{query}_1", f"{query}_2"],
                "complexity_level": complexity,
                "complexity_score": score
            }
            
            phonetic_features = {
                "query_length": len(query),
                "has_tone_variation": complexity != "simple"
            }
            
            case_id = logger.log_training_case(
                query, l1_result, l2_result, l3_result, phonetic_features
            )
            
            print(f"  è¨˜éŒ„æ¡ˆä¾‹: {query} â†’ {case_id} ({complexity})")
        
        # ç²å–çµ±è¨ˆä¿¡æ¯
        print("\nğŸ“Š æ•¸æ“šè¨˜éŒ„å™¨çµ±è¨ˆ:")
        print("-" * 40)
        
        stats = logger.get_statistics()
        session_stats = stats["session_stats"]
        
        print(f"æœƒè©±ID: {stats['session_id']}")
        print(f"ç¸½æŸ¥è©¢æ•¸: {session_stats['total_queries']}")
        print(f"å…§å­˜ç·©å­˜: {stats['memory_cache_size']} æ¢ç›®")
        print(f"æ•¸æ“šåº«ç¸½æ¡ˆä¾‹: {stats.get('total_cases_in_db', 'N/A')}")
        
        print(f"\nè¤‡é›œåº¦åˆ†ä½ˆ:")
        for level, count in session_stats["complexity_counts"].items():
            print(f"  {level}: {count}")
        
        print(f"\nå¹³å‡è™•ç†æ™‚é–“:")
        print(f"  L1: {session_stats['avg_l1_time']:.1f}ms")
        print(f"  L2: {session_stats['avg_l2_time']:.1f}ms")  
        print(f"  L3: {session_stats['avg_l3_time']:.1f}ms")
        
        # çµæŸæœƒè©±
        print("\nğŸ“Š çµæŸæœƒè©±:")
        print("-" * 40)
        
        summary = logger.finalize_session()
        print(f"æœƒè©±æ™‚é•·: {summary.performance_stats['session_duration_sec']:.1f}ç§’")
        print(f"ç¸½è™•ç†æ™‚é–“: {summary.performance_stats['total_time_ms']:.1f}ms")
        
        # æ¸¬è©¦æ•¸æ“šå°å‡º
        print("\nğŸ“Š æ¸¬è©¦æ•¸æ“šå°å‡º:")
        print("-" * 40)
        
        export_path = "data/training_logs/test_export.json"
        exported_count = logger.export_training_data(export_path, format="json")
        print(f"å°å‡ºè¨˜éŒ„æ•¸: {exported_count}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(level=logging.INFO)
    
    # åŸ·è¡Œæ¸¬è©¦
    success = test_training_data_logger()
    print(f"\næ¸¬è©¦ {'âœ… PASSED' if success else 'âŒ FAILED'}")