import atexit
import functools
import json
import os
import pickle
import socket
import threading
import time
from dataclasses import asdict, dataclass, field
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional

import torch
from torch import profiler

from atorch import local_rank, rank, world_size
from atorch.common.log_utils import default_logger as logger
from atorch.common.singleton import SingletonMeta
from atorch.utils.import_util import is_megatron_lm_available

from ._file_monitor import ThreadFileConfigMonitor, datetime_field

__all__ = ["init"]


def local_world_size():
    return int(os.getenv("LOCAL_WORLD_SIZE", 1))


@dataclass
class MegatronParallelConfig:
    tensor_model_parallel_size: int = 1
    pipeline_model_parallel_size: int = 1
    context_parallel_size: int = 1
    expert_model_parallel_size: int = 1
    expert_tensor_parallel_size: Optional[int] = None
    use_tp_pp_dp_mapping: bool = False
    encoder_tensor_model_parallel_size: int = 0
    encoder_pipeline_model_parallel_size: int = 0
    world_size: int = 1

    @classmethod
    def from_megatron_args(cls):
        if not is_megatron_lm_available():
            logger.warning("MegatronLM is not available, skip getting megatron parallel config")
            return None

        try:
            from megatron.training.global_vars import get_args  # type: ignore[attr-defined]
        except ImportError:
            logger.warning("Failed to import megatron.training.global_vars, " "skip getting megatron parallel config")
            return None

        args = get_args()

        return cls(
            tensor_model_parallel_size=args.tensor_model_parallel_size,
            pipeline_model_parallel_size=args.pipeline_model_parallel_size,
            context_parallel_size=args.context_parallel_size,
            expert_model_parallel_size=args.expert_model_parallel_size,
            expert_tensor_parallel_size=args.expert_tensor_parallel_size,
            use_tp_pp_dp_mapping=args.use_tp_pp_dp_mapping,
            encoder_tensor_model_parallel_size=args.encoder_tensor_model_parallel_size,
            encoder_pipeline_model_parallel_size=args.encoder_pipeline_model_parallel_size,
            world_size=world_size(),
        )

    def to_dict(self) -> dict:
        return asdict(self)

    @classmethod
    def from_dict(cls, data: dict) -> "MegatronParallelConfig":
        return cls(**data)


def active_kineto() -> bool:
    return os.getenv("KINETO_USE_DAEMON", None) is not None


if not active_kineto():
    if torch.__version__ >= "2.0.0":
        _origin_patch_step_function = torch.optim.Optimizer._patch_step_function
    elif torch.__version__ >= "1.8.0":
        _origin_patch_step_function = torch.optim.Optimizer._hook_for_profile  # type: ignore[attr-defined]


def no_exception_func(default_ret=None):
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            try:
                result = func(*args, **kwargs)
            except Exception as ex:
                logger.error(f"Call {func.__name__} failed. Exception: {str(ex)}")
                return default_ret
            return result

        return wrapper

    return decorator


@dataclass(frozen=True)
class OnDemandProfilerConfig:
    """Immutable profiler configuration."""

    # output config
    output_dir: str = ""
    use_gzip: bool = True

    # mode in ["trace", "dump"]
    mode: str = "trace"

    # use start step or start time, if both are set, config will be ignored
    start_step: int = 0
    start_time: Optional[datetime] = datetime_field(date_format="%Y%m%d%H%M")

    # schedule config
    schedule_wait: int = 0
    schedule_warmup: int = 0
    schedule_active: int = 0

    # profiler config
    with_stack: bool = False
    with_flops: bool = False
    with_modules: bool = False
    record_shapes: bool = False
    profile_memory: bool = False
    activities: list = field(default_factory=list)
    meta_data: dict = field(default_factory=dict)
    profile_ranks: list = field(default_factory=list)

    # dump snapshot config
    enabled: str = "all"
    context: str = "all"
    stacks: str = "all"
    max_entries: int = 100000
    # # torch old version..
    # device: "Device" = None
    # record_context_cpp: bool = False
    # clear_history: bool = False
    # compile_context: bool = False
    # global_record_annotations: bool = False

    # x_config_update_time will be injected by file monitor
    x_config_update_time: datetime = datetime_field(date_format="%Y%m%d%H%M")
    # profile session id, if not specified by config file, it will be generated by
    # config update time with format %Y%m%d%H%M%S
    session_id: str = ""

    def __post_init__(self):
        # convert activities to list of ProfilerActivity
        activities = self.activities
        new_activities = []
        for activity in activities:
            prof_activity = getattr(torch.profiler.ProfilerActivity, activity.upper(), None)
            if prof_activity is None:
                logger.warning("Invalid profiler activity: %s", activity)
            else:
                new_activities.append(prof_activity)
        object.__setattr__(self, "activities", new_activities)

        # generate session id if not specified
        if self.session_id == "":
            object.__setattr__(
                self,
                "session_id",
                self.x_config_update_time.strftime("%Y%m%d%H%M%S"),
            )

    def is_valid(self) -> bool:
        """Check if the configuration is valid."""
        if self.start_step > 0 and self.start_time is not None:
            logger.warning("start_step and start_time can not be set at the same time")
            return False
        if self.start_step == 0 and self.start_time is None:
            logger.warning("start_step and start_time can not be set to 0")
            return False

        if self.profile_memory:
            if not self.with_stack or not self.record_shapes:
                logger.warning("Profile memory is enabled, but with_stack or record_shapes is not enabled")
                return False

        # Check mode in ["trace", "dump"]
        if self.mode not in ["trace", "dump"]:
            logger.warning(f"Profile mode `{self.mode}` only support `trace` or `dump`")
            return False

        return (
            self.schedule_active > 0
            and self.output_dir != ""
            and len(self.activities) > 0
            and len(self.profile_ranks) > 0
        )


class LocalShmStore:
    def __init__(self, store_path: str) -> None:
        self.store_path = store_path
        self.base_dir = os.path.join("/dev/shm", f"{store_path}")

    def set_value(self, key: str, value: str):
        path = os.path.join(self.base_dir, f"{key}")
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w") as f:
            f.write(value)

    def set_bytes(self, key: str, value: bytes):
        path = os.path.join(self.base_dir, f"{key}")
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "wb") as f:
            f.write(value)

    def get_bytes(self, key: str) -> bytes:
        path = os.path.join(self.base_dir, f"{key}")
        with open(path, "rb") as f:
            return f.read()

    def get_value(self, key: str) -> str:
        path = os.path.join(self.base_dir, f"{key}")
        if not os.path.exists(path):
            return ""
        with open(os.path.join(self.base_dir, f"{key}"), "r") as f:
            return f.read()

    def list_keys(self, key: str = "") -> List[str]:
        path = os.path.join(self.base_dir, key)
        if not os.path.exists(path):
            return []
        # list all keys in the path
        return [f for f in os.listdir(path)]

    def exists(self, key: str = "") -> bool:
        path = os.path.join(self.base_dir, key)
        return os.path.exists(path)


class SessionState(Enum):
    IDLE = "idle"
    WAITING = "waiting"
    SKIPPED = "skipped"
    SCHEDULED = "scheduled"
    DONE = "done"


class ProfileRankState:
    def __init__(
        self,
        store: LocalShmStore,
        local_rank: int,
        rank: int,
        config: OnDemandProfilerConfig,
    ) -> None:
        self.store = store
        self.local_rank = local_rank
        self.rank = rank
        self.config = config
        self._update_state(SessionState.WAITING)
        self.profiler: Optional[profiler.profile] = None

        # self._total_step = config.schedule_active + config.schedule_warmup + config.schedule_wait
        if self.config.mode == "trace":
            self._total_step = config.schedule_active + config.schedule_warmup + config.schedule_wait
        else:
            self._total_step = config.schedule_active

    def can_not_schedule(self, cur_step: int) -> bool:
        return self.config.start_step > 0 and cur_step > self.config.start_step

    def step(self, cur_step: int) -> bool:
        """
        Return True if profiling is done after step.
        """
        if self.state == SessionState.WAITING:
            if (self.config.start_step > 0 and cur_step == self.config.start_step) or (
                self.config.start_time is not None and datetime.now() >= self.config.start_time
            ):
                if self.config.profile_ranks[0] == -1 or self.rank in self.config.profile_ranks:
                    self._update_state(SessionState.SCHEDULED)
                    if self.config.mode == "trace":
                        # start dynamic profiler
                        self.start_profile()
                        logger.info(
                            f"Rank {self.rank}, local rank {self.local_rank}, "
                            f"Start Dynamic Profiler at {cur_step} step."
                        )
                    else:
                        # start dump snapshot
                        self.start_dump()
                        logger.info(
                            f"Rank {self.rank}, local rank {self.local_rank}, "
                            f"Start Dump Snapshot at {cur_step} step."
                        )
                    self.config.meta_data["start_step"] = cur_step
                else:
                    self._update_state(SessionState.SKIPPED)
            return False

        if self.state == SessionState.SCHEDULED or self.state == SessionState.SKIPPED:
            self._total_step -= 1
            if self.config.mode == "trace":
                if self.state == SessionState.SCHEDULED and self.profiler is not None:
                    self.profiler.step()

                if self._total_step == 0:
                    self._update_state(SessionState.DONE)
                    if self.profiler is not None:
                        self.profiler.stop()
                        self.profiler = None
                    return True
                else:
                    return False
            else:
                # schedule dump
                if self._total_step == 0:
                    self._update_state(SessionState.DONE)
                    # Only dump for profile ranks
                    if self.config.profile_ranks[0] == -1 or self.rank in self.config.profile_ranks:
                        self.stop_dump()
                    return True
                else:
                    return False

        return True

    def _update_state(self, state: SessionState):
        self.store.set_value(f"state_{self.local_rank}", state.value)
        self.state = state

    def start_profile(self):
        def trace_handler(prof):
            try:
                tb_handler = torch.profiler.tensorboard_trace_handler(
                    self.config.output_dir,
                    worker_name=(
                        f"trace_local_rank_{self.local_rank}_"
                        f"rank_{self.rank}_"
                        f"{socket.gethostname()}_{os.getpid()}"
                    ),
                    use_gzip=self.config.use_gzip,
                )
                tb_handler(prof)

                if self.config.profile_memory:
                    memory_tl_file = os.path.join(
                        self.config.output_dir,
                        (
                            f"memory_local_rank_{self.local_rank}_"
                            f"rank_{self.rank}_"
                            f"{socket.gethostname()}_{os.getpid()}.html"
                        ),
                    )
                    logger.info("Export memory timeline to %s", memory_tl_file)
                    prof.export_memory_timeline(memory_tl_file)
            except Exception as handler_error:
                logger.error("Failed to handler trace profiler: %s", handler_error)

        try:
            self.profiler = profiler.profile(
                activities=self.config.activities,
                schedule=profiler.schedule(
                    wait=self.config.schedule_wait,
                    warmup=self.config.schedule_warmup,
                    active=self.config.schedule_active,
                ),
                record_shapes=self.config.record_shapes,
                profile_memory=self.config.profile_memory,
                with_stack=self.config.with_stack,
                with_flops=self.config.with_flops,
                with_modules=self.config.with_modules,
                on_trace_ready=trace_handler,
            )
        except Exception as e:
            logger.error("Failed to start profiler: %s", e)
            self.profiler = None

        if self.profiler is not None:
            self.profiler.start()
            for key, value in self.config.meta_data.items():
                self.profiler.add_metadata_json(str(key), json.dumps(value))

    def start_dump(self):
        """
        Start memory dump snapshot
        """

        try:
            logger.info("dump snapshot config %s", str(self.config))
            torch.cuda.memory._record_memory_history(
                enabled=self.config.enabled,
                context=self.config.context,
                stacks=self.config.stacks,
                max_entries=self.config.max_entries,
                # device=self.config.device,
                # clear_history=self.config.clear_history,
                # compile_context=self.config.compile_context,
                # global_record_annotations=self.config.global_record_annotations,
            )
        except Exception as e:
            logger.error("Failed to start dump snapshot: %s", e)

    def stop_dump(self):
        """
        Save pickle and stop memory dump snapshot
        """

        try:
            # save dump pickle
            os.makedirs(self.config.output_dir, exist_ok=True)
            dump_file = os.path.join(
                self.config.output_dir,
                (
                    f"snap_local_rank_{self.local_rank}_"
                    f"rank_{self.rank}_"
                    f"{socket.gethostname()}_{os.getpid()}.pickle"
                ),
            )
            torch.cuda.memory._dump_snapshot(dump_file)
            logger.info("Success to save pickle snapshot to %s", dump_file)
        except Exception as save_error:
            logger.error("Failed to save pickle snapshot: %s", save_error)

        # stop dump snapshot
        try:
            torch.cuda.memory._record_memory_history(
                enabled=None,
            )
            logger.info("Success to stop dump snapshot")
        except Exception as enabled_error:
            logger.error("Failed to stop dump snapshot: %s", enabled_error)


class ProfileSession:
    def __init__(
        self,
        store: LocalShmStore,
        config: OnDemandProfilerConfig,
        local_world_size: int,
    ) -> None:
        self.store = store
        self.session_id = config.session_id
        self.config = config
        self.local_world_size = local_world_size

    def get_rank_states(self) -> Dict[int, SessionState]:
        states = {}
        for lrank in range(self.local_world_size):
            state = self.store.get_value(f"state_{lrank}")
            if not state:
                states[lrank] = SessionState.IDLE
            else:
                states[lrank] = SessionState(state)
        return states

    def to_store(self):
        self.store.set_bytes("config", pickle.dumps(self.config))
        self.store.set_value("local_world_size", str(self.local_world_size))

    @classmethod
    def from_store(cls, store: LocalShmStore) -> "ProfileSession":
        config = pickle.loads(store.get_bytes("config"))
        local_world_size = int(store.get_value("local_world_size"))
        return cls(store, config, local_world_size)


class ProfilerNodeStateManager:
    LOCAL_CONFIG_PATH = "/dev/shm/atorch_dp/local_config.json"

    def __init__(self) -> None:
        self.store = LocalShmStore("atorch_dp")

    def get_dynamic_profile_enabled(self) -> bool:
        return self.store.get_value("dynamic_profile_enabled") == "1"

    def set_dynamic_profile_enabled(self):
        self.store.set_value("dynamic_profile_enabled", "1")

    def get_megatron_parallel_config(self) -> Optional[MegatronParallelConfig]:
        megatron_parallel_config = self.store.get_value("megatron_parallel_config")
        try:
            return MegatronParallelConfig.from_dict(json.loads(megatron_parallel_config))
        except Exception as e:
            logger.error(f"Failed to parse megatron parallel config from local shm store: {e}")
            return None

    def set_megatron_parallel_config(self, config: MegatronParallelConfig):
        self.store.set_value(
            "megatron_parallel_config",
            json.dumps(config.to_dict(), indent=4),
        )

    def create_session(
        self,
        config: OnDemandProfilerConfig,
    ) -> ProfileSession:
        store = LocalShmStore(f"atorch_dp/sessions/{config.session_id}")
        return ProfileSession(store, config, local_world_size())

    def get_session(self, session_id: str) -> Optional[ProfileSession]:
        store = LocalShmStore(f"atorch_dp/sessions/{session_id}")
        if not store.exists():
            return None
        return ProfileSession.from_store(store)

    def profile_rank_state(self, local_rank: int, rank: int, config: OnDemandProfilerConfig) -> ProfileRankState:
        store = LocalShmStore(f"atorch_dp/sessions/{config.session_id}")
        return ProfileRankState(store, local_rank, rank, config)

    def list_sessions(self) -> List[ProfileSession]:
        return [
            ProfileSession.from_store(LocalShmStore(f"atorch_dp/sessions/{session_id}"))
            for session_id in self.store.list_keys("atorch_dp/sessions/")
        ]

    def set_steps(self, steps: int):
        self.store.set_value("steps", str(steps))

    def get_steps(self) -> int:
        return int(self.store.get_value("steps"))


class _DynamicProfile(metaclass=SingletonMeta):
    def __init__(self) -> None:

        self.cur_step = 0
        self._optimizer_id = 0
        self._dynamic_monitor: Optional[ThreadFileConfigMonitor[OnDemandProfilerConfig]] = None
        self._node_state_manager = ProfilerNodeStateManager()

        self._state: Optional[ProfileRankState] = None
        # hook step function
        if torch.__version__ >= "2.0.0":
            torch.optim.Optimizer._patch_step_function = _DynamicProfile.patch_step_function  # type: ignore[assignment]

        elif torch.__version__ >= "1.8.0":
            torch.optim.Optimizer._hook_for_profile = _DynamicProfile.patch_step_function  # type: ignore[attr-defined]

    def init(self, cfg_path: str):
        logger.info("init dynamic profile with cfg path: %s", cfg_path)
        self.rank = rank()
        self.local_rank = local_rank()
        self._dynamic_monitor = ThreadFileConfigMonitor(
            config_paths=[
                self._node_state_manager.LOCAL_CONFIG_PATH,
                cfg_path,
            ],
            config_class=OnDemandProfilerConfig,
        )
        self._dynamic_monitor.start()
        atexit.register(self._clean_resource)
        if self.local_rank == 0:
            # save megatron parallel config if available
            megatron_config = MegatronParallelConfig.from_megatron_args()
            if megatron_config is not None:
                self._node_state_manager.set_megatron_parallel_config(megatron_config)

            # set dynamic profile enabled
            self._node_state_manager.set_dynamic_profile_enabled()

            # start a thread to update steps every 1 minute
            self._steps_updater = threading.Thread(target=self._update_steps, daemon=True)
            self._steps_updater.start()

    def _update_steps(self):
        while True:
            self._node_state_manager.set_steps(self.cur_step)
            time.sleep(60)

    def _clean_resource(self):
        self._dynamic_monitor.stop()

    def step(self):
        self.cur_step += 1

        if self._state is not None:
            if self._state.step(self.cur_step):
                self._state = None
                logger.info(
                    f"Rank {self.rank}, local rank {self.local_rank}, "
                    f"Stop Dynamic Profiler or dump snapshot at {self.cur_step} step."
                )
            return

        if self._dynamic_monitor is None:
            return

        # check if config is modified
        config = self._dynamic_monitor.get_config_if_modified()
        if config is None:
            return

        # create profile rank state
        state = self._node_state_manager.profile_rank_state(self.local_rank, self.rank, config)
        if state.can_not_schedule(self.cur_step):
            logger.info(
                "start step is set to %d, but current step is %d, skip profiling",
                config.start_step,
                self.cur_step,
            )
            return

        self._state = state
        # store session info if local rank is 0
        if self.local_rank == 0:
            session = self._node_state_manager.create_session(config)
            session.to_store()

        # start profiler if start step or start time is set
        self._state.step(self.cur_step)

    @staticmethod
    def patch_step_function(optimizer: torch.optim.Optimizer):
        dp = _DynamicProfile()

        def step_wrapper(func):
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                out = func(*args, **kwargs)
                optimizer, *_ = args
                if id(optimizer) == dp._optimizer_id:
                    dp.step()
                return out

            return wrapper

        _origin_patch_step_function(optimizer)

        # hook optimizer step function
        dp._optimizer_id = id(optimizer)
        step_hooked = getattr(optimizer.__class__.step, "step_hooked", None)
        if not step_hooked:
            optimizer.__class__.step = step_wrapper(optimizer.__class__.step)  # type: ignore[assignment]
            optimizer.__class__.step.step_hooked = True  # type: ignore[attr-defined]


# hook optimizer step function when _DynamicProfile is initialized
if not active_kineto():
    _dynamic_profile = _DynamicProfile()


@no_exception_func()
def init(path: str):
    if active_kineto():
        logger.warning("Dynamic profiler is not supported in dynolog mode, skip init.")
        return

    # init dynamic profiler
    _dynamic_profile.init(cfg_path=path)
