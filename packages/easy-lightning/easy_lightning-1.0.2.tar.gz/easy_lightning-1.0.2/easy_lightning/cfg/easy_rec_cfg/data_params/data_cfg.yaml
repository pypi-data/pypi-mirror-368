£name:
  default: ml-100k #ml-100k
  values: [amazon_beauty, ml-1m, foursquare-nyc] #[amazon_beauty, ml-100k, ml-1m, steam] #the name of the dataset
/data_folder: ../data/raw/  #path of the dataset

# preprocessing
# whether to keep items with low rating
min_rating: 0 #the minimum rating of the dataset

#whether to filter the items with low frequency
min_items_per_user: 5 #the minimum number of items rated by a user
min_users_per_item: 5 #the minimum number of users that have rated an item

densify_index: True #whether to densify the index of the dataset

dataset_params:
  split_keys:
      train: [sid, uid]
      val: [sid, uid]
      test: [sid, uid]
collator_params:
  sequential_keys: [sid] #timestamp, rating #the sequential keys of the dataset
  padding_value: 0 #the padding value for the dataset
  £lookback: #the lookback of the dataset
    default: 10
    values: [5] #, 10, 20]
    # tune:
    #   name: choice
    #   params:
    #     categories: [5, 10, 20]
  lookforward: 1 #the lookforward of the dataset
  simultaneous_lookforward: 1 #the simultaneous lookforward of the dataset
  out_seq_len: # Number of predictions to keep (i.e. not masked as padding) --> to avoid train/test leakage
    train: null #the output sequence length of the training set
    val: &val_size 1 #the output sequence length of the validation set
    test: &test_size 1 #the output sequence length of the test set
  num_negatives:
    train: 1
    val: 100 #1. #10
    test: 100 #1. #100
  negatives_distribution: uniform
    # train: dynamic
    # val: uniform
    # test: uniform

split_method: leave_n_out #the split method of the dataset, including 'leave_n_out', 'hold_out', 'k_fold'
test_sizes: [*test_size,*val_size] #"n" for leave_n_out the number of (positive) samples for each user in the test set

# random_state: 42 #the random seed for splitting the dataset