"""
ä¸€ä¸ªé«˜åº¦é€šç”¨çš„æ–‡ä»¶åˆ°æ•°æ®åº“ETLï¼ˆæå–ã€è½¬æ¢ã€åŠ è½½ï¼‰å·¥å…·ã€‚å®ƒé€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„ run() æ–¹æ³•ï¼Œ
å¯ä»¥æ™ºèƒ½å¤„ç†å•ä¸ªæ–‡ä»¶æˆ–æ•´ä¸ªç›®å½•ï¼Œæ”¯æŒâ€œæ•´æ–‡ä»¶JSONâ€å’Œâ€œé€è¡ŒJSONâ€ä¸¤ç§æ¨¡å¼ï¼Œå¹¶æä¾›äº†æ–­ç‚¹ç»­ä¼ ã€
å¯è§†åŒ–è¿›åº¦æ¡å’Œäº‹ä»¶å›è°ƒç­‰é«˜çº§åŠŸèƒ½ã€‚
"""
import json as std_json
import os
import ast
import sys
import io
import re
from abc import ABC, abstractmethod
from contextlib import redirect_stdout
from datetime import datetime
from typing import Dict, Callable, List, Optional, Any

from MignonFramework.MySQLManager import MysqlManager
from MignonFramework.CountLinesInFolder import count_lines_in_single_file
from MignonFramework.ConfigReader import ConfigManager
from MignonFramework.BaseWriter import BaseWriter


class Rename:
    """ä¸€ä¸ªè¾…åŠ©ç±»ï¼Œåœ¨modifier_functionä¸­ç”¨äºæ˜ç¡®è¡¨ç¤ºé‡å‘½åæ“ä½œã€‚"""

    def __init__(self, new_key_name: str):
        self.new_key_name = new_key_name


class GenericFileProcessor:
    """
    ä¸€ä¸ªé€šç”¨çš„ã€å¯å®šåˆ¶çš„JSONæ–‡ä»¶å¤„ç†å™¨ï¼Œç”¨äºå°†æ–‡ä»¶å†…å®¹æ‰¹é‡å†™å…¥æŒ‡å®šç›®æ ‡ã€‚
    æ”¯æŒé›¶é…ç½®å¯åŠ¨ï¼Œä¼šè‡ªåŠ¨å¼•å¯¼ç”¨æˆ·åˆ›å»ºé…ç½®æ–‡ä»¶ã€‚
    """

    def __init__(self,
                 writer: Optional[BaseWriter] = None,
                 table_name: Optional[str] = None,
                 mode: str = 'line',
                 modifier_function: Optional[Callable[[Dict], Dict]] = None,
                 filter_function: Optional[Callable[[Dict, int], bool]] = None,
                 exclude_keys: Optional[List[str]] = None,
                 default_values: Optional[Dict[str, Any]] = None,
                 batch_size: int = 1000,
                 callBack: Optional[Callable[[bool, List[Dict], str, Optional[int]], None]] = None,
                 print_mapping_table: bool = True,
                 on_error: str = 'stop'):
        """
        åˆå§‹åŒ–å¤„ç†å™¨ã€‚

        Args:
            writer (BaseWriter, optional): æ•°æ®å†™å…¥å™¨å®ä¾‹ã€‚å¦‚æœä¸ºNoneï¼Œå°†å°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½MysqlManagerã€‚
            table_name (str, optional): ç›®æ ‡è¡¨åã€‚å¦‚æœä¸ºNoneï¼Œå°†å°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½ã€‚
            mode (str): æ–‡ä»¶å¤„ç†æ¨¡å¼ã€‚å¯é€‰å€¼ä¸º 'line' (é»˜è®¤) æˆ– 'file'ã€‚
            modifier_function (Callable, optional): è‡ªå®šä¹‰ä¿®æ”¹å‡½æ•°ã€‚
            filter_function (Callable[[Dict, int], bool], optional): æ•°æ®è¿‡æ»¤å‡½æ•°ï¼Œæ¥æ”¶æ•°æ®å’Œè¡Œå·ï¼Œè¿”å›Falseåˆ™è·³è¿‡ã€‚
            exclude_keys (List[str], optional): éœ€è¦æ’é™¤çš„æºJSONé”®åˆ—è¡¨ã€‚
            default_values (Dict[str, Any], optional): ä¸ºç¼ºå¤±æˆ–ä¸ºNoneçš„é”®æä¾›é»˜è®¤å€¼ã€‚
            batch_size (int): æ¯æ‰¹æäº¤çš„è®°å½•æ•°ã€‚
            callBack (Callable, optional): æ‰¹å¤„ç†å®Œæˆåçš„å›è°ƒå‡½æ•°ã€‚
            print_mapping_table (bool): æ˜¯å¦åœ¨è¿è¡Œå‰æ‰“å°å­—æ®µæ˜ å°„å¯¹ç…§è¡¨ã€‚
            on_error (str): é”™è¯¯å¤„ç†ç­–ç•¥ ('continue', 'stop', 'log_to_file')ã€‚
        """
        self.is_ready = True
        self.path_from_config = None

        if writer is None:
            self._init_from_config()
        else:
            self.writer = writer
            self.table_name = table_name

        if not self.is_ready:
            return

        if not isinstance(self.writer, BaseWriter):
            raise TypeError("writer å¿…é¡»æ˜¯ BaseWriter çš„ä¸€ä¸ªå®ä¾‹ã€‚")
        if mode not in ['file', 'line']:
            raise ValueError("mode å‚æ•°å¿…é¡»æ˜¯ 'file' æˆ– 'line'ã€‚")

        self.modifier_function = modifier_function
        self.filter_function = filter_function
        self.exclude_keys = set(exclude_keys) if exclude_keys else set()
        self.default_values = default_values if default_values else {}
        self.mode = mode
        self.batch_size = batch_size
        self.callBack = callBack
        self.print_mapping_table = print_mapping_table
        self.on_error = on_error

    def _init_from_config(self):
        """ä»é…ç½®æ–‡ä»¶åˆå§‹åŒ–å¤„ç†å™¨ã€‚å¦‚æœé…ç½®ä¸å®Œæ•´ï¼Œåˆ™å¼•å¯¼ç”¨æˆ·åˆ›å»ºã€‚"""
        config = ConfigManager(filename='./resources/config/generic.ini', section='GenericProcessor')
        config_data = config.get_all_fields()
        required_keys = ['host', 'user', 'password', 'database', 'table_name', 'path']

        if config_data is None or any(
                not config_data.get(key) or 'YOUR_' in str(config_data.get(key)) for key in required_keys):
            print("\n" + "=" * 60)
            print("ğŸš€ æ¬¢è¿ä½¿ç”¨ GenericFileProcessor é›¶é…ç½®å‘å¯¼ï¼")
            print("å¤„ç†å™¨æ£€æµ‹åˆ°é…ç½®ä¸å®Œæ•´ï¼Œå°†ä¸ºæ‚¨åˆ›å»ºæˆ–æ›´æ–°é…ç½®æ–‡ä»¶ã€‚")
            print(f"é…ç½®æ–‡ä»¶è·¯å¾„: {os.path.abspath('./resources/config/generic.ini')}")
            print("è¯·åœ¨è¯¥æ–‡ä»¶ä¸­å¡«å†™æ‚¨çš„æ•°æ®åº“ä¿¡æ¯å’Œè¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„ã€‚")
            print("=" * 60 + "\n")

            placeholders = {
                'host': 'YOUR_DATABASE_HOST',
                'user': 'YOUR_USERNAME',
                'password': 'YOUR_PASSWORD',
                'database': 'YOUR_DATABASE_NAME',
                'table_name': 'YOUR_TARGET_TABLE',
                'path': 'PATH_TO_YOUR_FILE_OR_DIRECTORY'
            }
            for key in required_keys:
                if not config_data or not config_data.get(key) or 'YOUR_' in str(config_data.get(key)):
                    config.update_field(key, placeholders[key])

            self.is_ready = False
            return

        db_config = {k: config_data[k] for k in ['host', 'user', 'password', 'database']}
        # é»˜è®¤ä½¿ç”¨ MysqlManager ä½œä¸ºå†™å…¥å™¨
        self.writer = MysqlManager(**db_config)
        if not self.writer.is_connected():
            print(f"[ERROR] ä½¿ç”¨ generic.ini ä¸­çš„é…ç½®è¿æ¥æ•°æ®åº“å¤±è´¥ã€‚")
            self.is_ready = False
            return

        self.table_name = config_data['table_name']
        self.path_from_config = config_data['path']

    def _to_snake_case(self, name: str) -> str:
        if not isinstance(name, str) or not name:
            return ""
        s1 = re.sub(r'(.)([A-Z][a-z]+)', r'\1_\2', name)
        return re.sub(r'([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

    def _safe_json_load(self, text: str) -> Optional[Dict]:
        try:
            return std_json.loads(text)
        except std_json.JSONDecodeError:
            try:
                return ast.literal_eval(text)
            except (ValueError, SyntaxError, MemoryError, TypeError):
                return None

    def _finalize_types(self, data_dict: dict) -> dict:
        final_data = {}
        for key, value in data_dict.items():
            if value is None:
                final_data[key] = ''
            elif isinstance(value, (dict, list)):
                final_data[key] = std_json.dumps(value, ensure_ascii=False)
            else:
                final_data[key] = value
        return final_data

    def _process_single_item(self, json_data: dict) -> Optional[Dict]:
        # æ­¥éª¤ 1: åˆ›å»ºä¸€ä¸ªåº”ç”¨äº†é»˜è®¤å€¼çš„åŸºç¡€å­—å…¸
        data_with_defaults = {}
        all_original_keys = set(json_data.keys()) | set(self.default_values.keys())
        for key in all_original_keys:
            value = json_data.get(key)
            # ä¿®æ­£ï¼šå¦‚æœæºå€¼æ˜¯Noneæˆ–ç©ºå­—ç¬¦ä¸²ï¼Œå¹¶ä¸”å­˜åœ¨é»˜è®¤å€¼ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
            if (value is None or value == '') and key in self.default_values:
                data_with_defaults[key] = self.default_values[key]
            else:
                data_with_defaults[key] = value

        # æ­¥éª¤ 2: åŸºäºèåˆäº†é»˜è®¤å€¼çš„æ•°æ®è¿›è¡Œè‡ªåŠ¨è§£æ
        parsed_data = {}
        for original_key, final_value in data_with_defaults.items():
            if original_key in self.exclude_keys:
                continue
            new_key = self._to_snake_case(original_key)
            parsed_data[new_key] = final_value

        # æ­¥éª¤ 3: å°†èåˆäº†é»˜è®¤å€¼çš„æ•°æ®ä¼ é€’ç»™ä¿®æ”¹å™¨ï¼Œå¹¶åº”ç”¨â€œè¡¥ä¸â€
        if self.modifier_function:
            patch_dict = self.modifier_function(data_with_defaults)
            for original_key, instruction in patch_dict.items():
                auto_key = self._to_snake_case(original_key)
                if isinstance(instruction, Rename):
                    if auto_key in parsed_data:
                        parsed_data[instruction.new_key_name] = parsed_data.pop(auto_key)
                elif isinstance(instruction, tuple) and len(instruction) == 2:
                    if auto_key in parsed_data:
                        del parsed_data[auto_key]
                    parsed_data[instruction[0]] = instruction[1]
                else:
                    parsed_data[auto_key] = instruction

        # æ­¥éª¤ 4: å¯¹æœ€ç»ˆç»“æœè¿›è¡Œç±»å‹è½¬æ¢
        return self._finalize_types(parsed_data)

    def _execute_batch(self, json_list: List[Dict], filename: str, line_num: Optional[int] = None):
        if not json_list:
            return
        f = io.StringIO()
        status = False
        with redirect_stdout(f):
            status = self.writer.upsert_batch(json_list, self.table_name)
        captured_output = f.getvalue().strip()
        if self.callBack:
            try:
                self.callBack(status, json_list, filename, line_num)
            except Exception as cb_e:
                print(f"\n[ERROR] å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {cb_e}")
        if not status:
            raise Exception(f"æ•°æ®å†™å…¥å¤±è´¥ã€‚è¯¦ç»†ä¿¡æ¯: {captured_output}")

    def _generate_and_print_mapping(self, sample_json: Dict[str, Any]):
        print("\n" + "=" * 102)
        print("--- å­—æ®µæ˜ å°„å¯¹ç…§è¡¨ (Field Mapping Table) ---")
        col_widths = (30, 30, 30)
        header = "| {:{w1}} | {:{w2}} | {:{w3}} |".format(
            "æºå­—æ®µ", "ç›®æ ‡å­—æ®µ", "ç¤ºä¾‹å€¼/é»˜è®¤å€¼", w1=col_widths[0], w2=col_widths[1], w3=col_widths[2]
        )
        print(header)
        print("-" * (sum(col_widths) + 7))

        # åŒæ ·ï¼Œå…ˆèåˆé»˜è®¤å€¼ï¼Œä»¥ç¡®ä¿ä¿®æ”¹å™¨èƒ½çœ‹åˆ°å®ƒä»¬
        sample_with_defaults = {}
        all_sample_keys = set(sample_json.keys()) | set(self.default_values.keys())
        for key in all_sample_keys:
            value = sample_json.get(key)
            # ä¿®æ­£ï¼šåŒæ ·åº”ç”¨æ›´ä¸¥æ ¼çš„é»˜è®¤å€¼é€»è¾‘
            if (value is None or value == '') and key in self.default_values:
                sample_with_defaults[key] = self.default_values[key]
            else:
                sample_with_defaults[key] = value

        patch = self.modifier_function(sample_with_defaults) if self.modifier_function else {}
        all_keys = sorted(list(set(sample_with_defaults.keys()) | set(patch.keys())))

        for key in all_keys:
            if key in self.exclude_keys:
                mapped_key, value_str = "SKIPPED", "N/A"
            else:
                instruction = patch.get(key)
                if isinstance(instruction, Rename):
                    mapped_key = instruction.new_key_name
                elif isinstance(instruction, tuple):
                    mapped_key = instruction[0]
                else:
                    mapped_key = self._to_snake_case(key)

                if instruction is not None:
                    if isinstance(instruction, tuple):
                        value_str = f"(mod) {instruction[1]}"
                    elif not isinstance(instruction, Rename):
                        value_str = f"(mod) {instruction}"
                    else:
                        value_str = str(sample_with_defaults.get(key, 'N/A'))
                elif key in sample_with_defaults and sample_with_defaults[key] is not None:
                    value_str = str(sample_with_defaults[key])
                else:
                    value_str = "N/A"

            value_str = (value_str[:25] + '...') if len(value_str) > 25 else value_str

            padding = [w - self._get_display_width(s) for w, s in zip(col_widths, [key, mapped_key, value_str])]
            print(f"| {key}{' ' * padding[0]} | {mapped_key}{' ' * padding[1]} | {value_str}{' ' * padding[2]} |")

        print("=" * 102 + "\n")

    def _get_display_width(self, s: str) -> int:
        return sum(2 if '\u4e00' <= char <= '\u9fff' else 1 for char in s)

    def run(self, path: Optional[str] = None, start_line: int = 1):
        if not self.is_ready:
            print("[INFO] å¤„ç†å™¨å°šæœªå°±ç»ªï¼Œè¯·æ ¹æ®æç¤ºå®Œæˆé…ç½®åå†æ¬¡è¿è¡Œã€‚")
            return

        target_path = path if path is not None else self.path_from_config
        if not target_path or not os.path.exists(target_path):
            print(f"[ERROR] ç›®æ ‡è·¯å¾„æ— æ•ˆæˆ–ä¸å­˜åœ¨: {target_path}")
            return

        files_to_process = [os.path.join(target_path, f) for f in os.listdir(target_path) if
                            os.path.isfile(os.path.join(target_path, f)) and f.lower().endswith(
                                ('.json', '.txt'))] if os.path.isdir(target_path) else [target_path]

        if not files_to_process:
            print(f"åœ¨è·¯å¾„ '{target_path}' ä¸­æœªæ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶ã€‚")
            return

        if self.print_mapping_table:
            try:
                composite_sample = {}
                lines_scanned = 0
                with open(files_to_process[0], 'r', encoding='utf-8') as f:
                    for line in f:
                        if lines_scanned >= 1000:
                            break
                        if line.strip():
                            if sample_data := self._safe_json_load(line):
                                composite_sample.update(sample_data)
                                lines_scanned += 1
                if composite_sample:
                    self._generate_and_print_mapping(composite_sample)
                else:
                    print("[WARNING] æœªèƒ½åœ¨æ–‡ä»¶å‰1000è¡Œæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ•°æ®æ¥ç”Ÿæˆå¯¹ç…§è¡¨ã€‚")
            except Exception as e:
                print(f"[WARNING] æ— æ³•ç”Ÿæˆå¯¹ç…§è¡¨: {e}")

        print(f"\n--- å¼€å§‹å¤„ç†è·¯å¾„: {target_path} (æ¨¡å¼: {self.mode}) ---")
        print(f"å‘ç° {len(files_to_process)} ä¸ªæ–‡ä»¶å¾…å¤„ç†...")

        for i, file_path in enumerate(files_to_process):
            filename, line_num = os.path.basename(file_path), 0
            print(f"\n[{i + 1}/{len(files_to_process)}] æ­£åœ¨å¤„ç†: {filename}")

            try:
                if self.mode == 'file':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    if content.strip():
                        if json_data := self._safe_json_load(content):
                            if not self.filter_function or self.filter_function(json_data, 1):
                                if parsed_dic := self._process_single_item(json_data):
                                    self._execute_batch([parsed_dic], filename)
                        else:
                            print(f"\n[WARNING] æ— æ³•è§£ææ–‡ä»¶å†…å®¹: {filename}")

                elif self.mode == 'line':
                    if start_line > 1:
                        print(f"  [INFO] ä»ç¬¬ {start_line} è¡Œå¼€å§‹å¤„ç†...")
                    total_lines = count_lines_in_single_file(file_path) or 0
                    json_list = []
                    with open(file_path, 'r', encoding='utf-8') as f:
                        for line_num, line in enumerate(f, 1):
                            if line_num < start_line or not line.strip():
                                continue

                            try:
                                json_data = self._safe_json_load(line)
                                if not json_data:
                                    raise ValueError("è§£æå¤±è´¥")
                                if self.filter_function and not self.filter_function(json_data, line_num):
                                    continue
                                if parsed_dic := self._process_single_item(json_data):
                                    json_list.append(parsed_dic)

                                if total_lines > 0:
                                    bar = 'â–ˆ' * int(40 * line_num / total_lines) + '-' * (
                                            40 - int(40 * line_num / total_lines))
                                    sys.stdout.write(
                                        f'\r|{bar}| {line_num / total_lines:.1%} ({line_num}/{total_lines})  æœ¬æ‰¹: [{len(json_list)}/{self.batch_size}]')
                                    sys.stdout.flush()

                                if len(json_list) >= self.batch_size:
                                    self._execute_batch(json_list, filename, line_num)
                                    json_list = []
                            except Exception as parse_e:
                                error_msg = f"\n[WARNING] å¤„ç†æ–‡ä»¶ {filename} ç¬¬ {line_num} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {parse_e}"
                                if self.on_error == 'stop' and not isinstance(parse_e, ValueError):
                                    raise
                                if self.on_error == 'log_to_file':
                                    with open('error.log', 'a', encoding='utf-8') as err_f:
                                        err_f.write(
                                            f"{datetime.now()} | {filename} | Line {line_num} | {parse_e}\n{line}\n")
                                print(error_msg)
                                print(f"  [FAILING LINE]: {line.strip()}")  # æ‰“å°å¤±è´¥çš„è¡Œ
                    print()
                    self._execute_batch(json_list, filename, line_num)
                print(f"  [æˆåŠŸ] æ–‡ä»¶å·²å¤„ç†ã€‚")
            except Exception as e:
                print(f"\n  [å¤±è´¥] å¤„ç†æ–‡ä»¶ {filename} æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}ã€‚")
                continue
        print("\n--- æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆ ---")


if __name__ == '__main__':
    from MignonFramework import GenericProcessor
    from datetime import datetime
    from MignonFramework.GenericProcessor import Rename


    def parseJson(dic: dict) -> dict:
        return {
            "PlanEndDate": Rename("plan_end_date"),  # ä»…æ”¹åç”¨æ¥å¯¹åº”å­—æ®µ
            "Fundingfloat": Rename("funding_float"),
            "Budgetfloat": ("budget_floats", dic.get("Budgetfloat")), # æ”¹ååŒæ—¶ä¿®æ”¹é€»è¾‘(æˆ–æ–°å¢)
            "Fundingfloats": dic.get("Fundingfloat") # ä»…æ”¹é€»è¾‘
        }


    def filterFun(dicts: dict, lineNo) -> bool:
        # è¿‡æ»¤å™¨æ–¹æ³• è§£æåæ‰§è¡Œ, å½“ä¸”ä»…å½“è¿”å›Trueæ—¶æ‰ä¼šinsert
        return True

    # é»˜è®¤å€¼
    defaultVal = {
        "PlanEndDate": datetime.now(),
        "CompleteDate": datetime.now(),
        "StartYear": "2025",
        "Fundingfloat": 0.0,
        "Budgetfloat": 0.0,
        "PlanStartDate": datetime.now(),
        "ApplyYear": "2025",
        "has_outcome": True
    }

    # æ’é™¤å­—æ®µ
    exclude = [
        "ForCodeForSearchs", "outComes", "AwardeeOrgState", "projectAbstract"
    ]
    # è¿˜å¯ä»¥åˆ‡æ¢è¯»å–çš„mode fileä¸ºæ•´ä¸ªæ–‡ä»¶éƒ½ä¸ºjson, é»˜è®¤line æ¯ä¸€è¡Œéƒ½æ˜¯json
    GenericProcessor.GenericFileProcessor(modifier_function=parseJson, default_values=defaultVal, filter_function=filterFun,
                                          exclude_keys=exclude).run()
