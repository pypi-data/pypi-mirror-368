# API LLM Rate Limit Configuration

# OpenAI Models
# https://platform.openai.com/docs/guides/rate-limits
openai:
  gpt-4o-mini-2024-07-18:
    usage_tier=0: # free tier
      RPM: 3
      TPM: 40000
    usage_tier=1:
      RPM: 500
      TPM: 200000
    usage_tier=2:
      RPM: 5000
      TPM: 2000000
    usage_tier=3:
      RPM: 5000
      TPM: 4000000
  gpt-4o-2024-11-20:
    usage_tier=1:
      RPM: 500
      TPM: 30000
    usage_tier=2:
      RPM: 5000
      TPM: 450000
    usage_tier=3:
      RPM: 5000
      TPM: 800000

# Gemini Models
# https://ai.google.dev/gemini-api/docs/rate-limits
gemini:
  gemini-1.5-flash-002:
    usage_tier=0: # free tier
      RPM: 15
      TPM: 1000000
    usage_tier=1:
      RPM: 2000
      TPM: 4000000
  gemini-1.5-pro-002:
    usage_tier=0: # free tier
      RPM: 2
      TPM: 32000000
    usage_tier=1:
      RPM: 1000
      TPM: 4000000
  gemini-1.5-flash-8b-001:
    usage_tier=0: # free tier
      RPM: 15
      TPM: 1000000
    usage_tier=1:
      RPM: 4000
      TPM: 4000000
  gemini-2.0-flash-exp:
    usage_tier=0: # free tier
      RPM: 10
      TPM: 4000000

# Anthropic Models
# https://docs.anthropic.com/en/api/rate-limits
anthropic:
  claude-3-5-sonnet-20241022:
    usage_tier=1:
      RPM: 50
      TPM: 40000
    usage_tier=2:
      RPM: 1000
      TPM: 80000
  claude-3-5-haiku-20241022:
    usage_tier=1:
      RPM: 50
      TPM: 50000
    usage_tier=2:
      RPM: 1000
      TPM: 100000

# All Together models have the same limit, no need for a separate entry for each model
# https://docs.together.ai/docs/rate-limits
together:
  default:
    usage_tier=0: # free tier
      RPM: 60
      TPM: 60000
    usage_tier=1:
      RPM: 600
      TPM: 180000
    usage_tier=2:
      RPM: 1800
      TPM: 250000
  deepseek-ai/DeepSeek-R1:
    usage_tier=0: # free tier
      RPM: 6
      TPM: 60000
    usage_tier=1:
      RPM: 12
      TPM: 180000
    usage_tier=2:
      RPM: 1800
      TPM: 250000

# Llama Models
# https://llama.developer.meta.com/docs/rate-limits
llama:
  llama-4-scout-17b-16e-instruct-fp8:
    usage_tier=1:
      RPM: 3000
      TPM: 1000000
  llama-4-maverick-17b-128e-instruct-fp8:
    usage_tier=1:
      RPM: 3000
      TPM: 1000000
  llama-3.3-70b-instruct:
    usage_tier=1:
      RPM: 3000
      TPM: 1000000
  llama-3.3-8b-instruct:
    usage_tier=1:
      RPM: 3000
      TPM: 1000000
