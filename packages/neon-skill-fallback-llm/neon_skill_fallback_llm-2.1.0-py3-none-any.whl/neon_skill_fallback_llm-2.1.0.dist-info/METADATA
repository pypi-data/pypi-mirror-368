Metadata-Version: 2.1
Name: neon-skill-fallback-llm
Version: 2.1.0
Home-page: https://github.com/NeonGeckoCom/skill-fallback_llm
Author: Neongecko
Author-email: developers@neon.ai
License: BSD-3-Clause
Description-Content-Type: text/markdown
License-File: LICENSE
License-File: LICENSE.md
Requires-Dist: neon-utils~=1.12
Requires-Dist: ovos-utils>=0.0.28,~=0.0
Requires-Dist: ovos-bus-client>=0.0.3,~=0.0
Requires-Dist: ovos-workshop~=0.1
Requires-Dist: neon-mq-connector~=0.7
Provides-Extra: test
Requires-Dist: neon-minerva[padatious]~=0.3; extra == "test"

# <img src='./logo.svg' card_color="#FF8600" width="50" style="vertical-align:bottom" style="vertical-align:bottom">LLM Fallback  
  
## Summary
Get an LLM response from the Neon Diana backend.

## Description
Converse with an LLM and enable LLM responses when Neon doesn't have a better
response.

To send a single query to an LLM, you can ask Neon to "ask Chat GPT <something>".
To start conversing with an LLM, ask to "talk to Chat GPT" and have all of your input
sent to an LLM until you say goodbye or stop talking for a while.

Enable fallback behavior by asking to "enable LLM fallback skill" or disable it
by asking to "disable LLM fallback".

To have a copy of LLM interactions sent via email, ask Neon to 
"email me a copy of our conversation".

## Examples 

* "Explain quantum computing in simple terms"
* "Ask chat GPT what an LLM is"
* "Talk to chat GPT"
* "Enable LLM fallback skill"
* "Disable LLM fallback skill"
* "Email me a copy of our conversation"
