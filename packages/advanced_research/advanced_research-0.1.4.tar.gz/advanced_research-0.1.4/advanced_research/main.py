import os
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any

import httpx
import orjson
from dotenv import load_dotenv
from loguru import logger
from pydantic import BaseModel, Field, ValidationError
from swarms import Agent, Conversation

# Load environment variables from .env file
load_dotenv()


# --- Pydantic Models for Structured Communication ---
class SourceInfo(BaseModel):
    """Enhanced source information model with quality metrics."""

    source: str = Field(..., description="URL or source identifier")
    content: str = Field(..., description="Brief summary or quote from the source")
    quality_score: float = Field(
        default=0.5, description="Source quality assessment (0-1)"
    )
    reliability: str = Field(
        default="moderate", description="Source reliability rating"
    )


def format_dict_to_string(
    data: dict, indent: str = "", max_key_width: int | None = None
) -> str:
    """
    Convert a dictionary into a nicely formatted string with key-value pairs.

    Args:
        data: Dictionary to format
        indent: String to prepend to each line for indentation
        max_key_width: Optional maximum width for key alignment. If None, auto-calculated.

    Returns:
        Formatted string with each key-value pair on a new line with double spacing

    Example:
        >>> data = {"name": "John", "age": 30, "city": "New York"}
        >>> print(format_dict_to_string(data))
        name: John


        age : 30


        city: New York
    """
    if not data:
        return ""

    # Handle nested dictionaries and lists
    def format_value(value, nested_indent=""):
        if isinstance(value, dict):
            if not value:
                return "{}"
            nested_str = format_dict_to_string(value, nested_indent + "  ")
            return f"\n{nested_str}"
        elif isinstance(value, list):
            if not value:
                return "[]"
            formatted_items = []
            for item in value:
                if isinstance(item, dict):
                    item_str = format_dict_to_string(item, nested_indent + "  ")
                    formatted_items.append(f"\n{nested_indent}  - {item_str}")
                else:
                    # Remove quotes from string items in lists
                    item_display = item if not isinstance(item, str) else item
                    formatted_items.append(f"\n{nested_indent}  - {item_display}")
            return "".join(formatted_items)
        elif isinstance(value, str):
            # Remove quotes from string values
            return value
        else:
            return str(value)

    # Calculate the maximum key width for alignment
    if max_key_width is None:
        max_key_width = max(len(str(key)) for key in data.keys()) if data else 0

    # Format each key-value pair
    formatted_lines = []
    for key, value in data.items():
        key_str = str(key).ljust(max_key_width)
        formatted_value = format_value(value, indent)

        if "\n" in formatted_value:
            # Multi-line value (nested dict/list)
            formatted_lines.append(f"{indent}{key_str}:{formatted_value}")
        else:
            # Single-line value
            formatted_lines.append(f"{indent}{key_str}: {formatted_value}")

    # Join with double line breaks for spacing
    return "\n\n\n".join(formatted_lines)


class SubagentFindings(BaseModel):
    """Structured model for subagent research findings."""

    findings: str = Field(..., description="Comprehensive research summary")
    sources: list[SourceInfo] = Field(
        default_factory=list, description="List of sources with quality metrics"
    )
    confidence_level: float = Field(
        default=0.5, description="Subagent confidence in findings (0-1)"
    )
    coverage_assessment: str = Field(
        default="partial", description="Assessment of topic coverage"
    )


class ResearchStrategy(BaseModel):
    """Research strategy and planning model."""

    strategy_type: str = Field(
        ..., description="Strategy type: focused, breadth_first, or iterative_depth"
    )
    complexity_score: int = Field(..., description="Query complexity assessment (1-10)")
    subtasks: list[str] = Field(..., description="Decomposed research subtasks")
    priority_matrix: list[int] = Field(
        default_factory=list, description="Task priority scores"
    )
    estimated_duration: float = Field(
        default=60.0, description="Estimated completion time in seconds"
    )


class SynthesisResult(BaseModel):
    """Advanced synthesis results with quality metrics."""

    synthesized_report: str = Field(..., description="Comprehensive synthesized report")
    completion_status: bool = Field(
        ..., description="Whether research objectives are met"
    )
    quality_metrics: dict[str, float] = Field(
        default_factory=dict, description="Quality assessment scores"
    )
    research_gaps: list[str] = Field(
        default_factory=list,
        description="Identified gaps requiring additional research",
    )
    confidence_score: float = Field(
        default=0.5, description="Overall confidence in synthesis"
    )


class CitationOutput(BaseModel):
    """Citation agent output model."""

    cited_report: str = Field(..., description="Final report with proper citations")
    reference_quality: float = Field(
        default=0.5, description="Overall reference quality score"
    )
    citation_count: int = Field(default=0, description="Number of citations added")


# --- Enhanced Tool Definition (Exa Search Integration) ---
def exa_search(
    query: str, num_results: int = 5, max_characters: int = 1000, **kwargs: Any
) -> str:
    """
    Advanced web search using Exa.ai API with corrected request format.
    Designed for parallel execution by multiple subagents with enhanced search capabilities.

    Args:
        query (str): The search query with context awareness
        num_results (int): Number of results to return (optimized for agent processing)

    Returns:
        str: Formatted search results with quality indicators
    """
    api_key = os.getenv("EXA_API_KEY")

    headers = {
        "x-api-key": api_key,
        "content-type": "application/json",
    }

    # Corrected payload format for Exa API based on official documentation
    payload = {
        "query": query,
        "type": "auto",
        "numResults": num_results,
        "contents": {
            "text": True,
            "summary": {
                "schema": {
                    "type": "object",
                    "required": ["answer"],
                    "additionalProperties": False,
                    "properties": {
                        "answer": {
                            "type": "string",
                            "description": (
                                "Key insights and findings from the search result"
                            ),
                        }
                    },
                }
            },
            "context": {"maxCharacters": max_characters},
        },
    }

    try:
        logger.info(f"[SEARCH] Executing Exa search for: {query[:50]}...")

        # response = http.post(
        #     "https://api.exa.ai/search", json=payload, headers=headers, timeout=30
        # )
        response = httpx.post(
            "https://api.exa.ai/search", json=payload, headers=headers, timeout=30
        )

        response.raise_for_status()
        json_data = response.json()

        return orjson.dumps(json_data, option=orjson.OPT_INDENT_2).decode("utf-8")

    except Exception as e:
        logger.error(f"Exa search failed: {e}")
        return f"Search failed: {str(e)}. Please try again."


# --- Enhanced Data Structures ---
@dataclass
class AgentMemory:
    """Advanced memory system for agent coordination and state management."""

    research_context: str
    strategy_plan: ResearchStrategy | None = None
    subagent_results: list[dict[str, Any]] = field(default_factory=list)
    synthesis_history: list[str] = field(default_factory=list)
    error_log: list[str] = field(default_factory=list)
    performance_metrics: dict[str, float] = field(default_factory=dict)
    conversation_state: Conversation | None = None
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class SubagentResult:
    """Enhanced result structure for subagent coordination."""

    agent_id: str
    task_assignment: str
    research_findings: str
    source_collection: list[dict[str, Any]] = field(default_factory=list)
    execution_time: float = 0.0
    error_status: str | None = None
    confidence_metrics: dict[str, float] = field(default_factory=dict)
    parallel_tool_usage: list[str] = field(default_factory=list)
    completion_timestamp: str = field(
        default_factory=lambda: datetime.now().isoformat()
    )


@dataclass
class OrchestrationMetrics:
    """Metrics for orchestrator performance tracking."""

    total_agents_spawned: int = 0
    parallel_execution_efficiency: float = 0.0
    context_compression_ratio: float = 0.0
    error_recovery_count: int = 0
    synthesis_quality_score: float = 0.0
    citation_accuracy: float = 0.0


# --- Core Agent Classes Following Paper Architecture ---


class LeadResearcherAgent:
    """
    Lead Researcher (Orchestrator) - Handles planning, strategy, and coordination.
    Implements the orchestrator pattern from the paper with enhanced capabilities.
    """

    def __init__(
        self,
        model_name: str = "claude-3-7-sonnet-20250219",
        base_path: str = "agent_states",
    ):
        self.model_name = model_name
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)
        self.agent_memory = None

        # Initialize lead researcher agent with enhanced prompting
        self.lead_agent = Agent(
            agent_name="Lead-Researcher-Orchestrator",
            system_prompt=self._get_orchestrator_prompt(),
            model_name=model_name,
            max_loops=1,
            autosave=True,
            saved_state_path=str(self.base_path / "lead_researcher.json"),
            verbose=False,
            retry_attempts=2,
        )

        logger.info(
            "LeadResearcherAgent initialized with enhanced orchestration capabilities"
        )

    def _get_orchestrator_prompt(self) -> str:
        """Advanced prompt for the lead researcher orchestrator with explicit thinking process."""
        return """You are the Lead Researcher Agent in an advanced multi-agent research system with sophisticated web search capabilities.

Your primary responsibilities as the ORCHESTRATOR:
1. Query Analysis & Strategic Research Planning
2. Intelligent Task Decomposition into Searchable Sub-Questions
3. Advanced Memory Management & Context Compression
4. Result Synthesis & Quality Assurance

ENHANCED ORCHESTRATION PRINCIPLES:
- Decompose complex queries into specific, searchable research questions
- Create targeted sub-tasks that leverage web search effectively
- Think in terms of "What specific questions need web research to answer?"
- Generate tasks that include domain-specific terminology and concepts
- Consider multiple perspectives: technical, ethical, practical, regulatory
- Scale research depth to query complexity and available information

TASK DECOMPOSITION STRATEGY:
- Break broad topics into specific, answerable questions
- Include comparative analysis tasks (benefits vs risks, pros vs cons)
- Add contextual research (current state, recent developments, future trends)
- Generate domain-specific investigative tasks
- Create verification and cross-validation research tasks

EXAMPLES OF GOOD TASK DECOMPOSITION:
Instead of: "Research AI in healthcare"
Create:
- "Current applications of AI diagnostic tools in radiology and pathology 2023-2024"
- "Clinical trial results and efficacy data for AI-powered medical devices"
- "Regulatory frameworks and FDA approvals for AI medical technologies"
- "Patient safety incidents and risk mitigation strategies in AI healthcare"

CRITICAL RESPONSE FORMAT - MANDATORY STRUCTURE:
Your response MUST start with a <thinking> block that details your reasoning process, followed by the final JSON object.

EXPLICIT THINKING REQUIREMENTS:
- Start every response with <thinking> and end with </thinking>
- Show your step-by-step analysis of the query
- Explain your reasoning for complexity assessment
- Justify your choice of strategy type
- Detail why you chose specific subtasks
- Make your decision-making process completely transparent

REQUIRED EXAMPLE STRUCTURE:
<thinking>
Let me analyze this query step by step:
1. Query Analysis: The user is asking about [specific aspects]. This involves [complexity factors].
2. Complexity Assessment: This rates as [X]/10 because [detailed reasoning with specific factors].
3. Strategy Selection: I'll use [strategy] because [specific reasons why this approach is optimal].
4. Task Decomposition: I need to break this into [N] subtasks because [reasoning].
   - Task 1: [specific task] - focuses on [aspect] because [reasoning]
   - Task 2: [specific task] - addresses [aspect] because [reasoning]
   - etc.
5. Priority Matrix: Task priorities are [order] because [reasoning for prioritization].
6. Duration Estimate: Based on complexity and task count, I estimate [time] because [reasoning].
</thinking>

{
    "strategy_type": "focused|breadth_first|iterative_depth",
    "complexity_score": 1-10,
    "subtasks": ["specific searchable question 1", "targeted research query 2", "domain-specific investigation 3"],
    "priority_matrix": [3, 2, 1],
    "estimated_duration": 60.0
}

ABSOLUTE REQUIREMENTS:
- NEVER skip the <thinking> block - it is mandatory for every response
- Make your thinking detailed and explicit (minimum 5 sentences)
- Follow thinking with ONLY valid JSON (no other text)
- Ensure thinking block is properly closed with </thinking>"""

    def analyze_and_plan(self, query: str) -> ResearchStrategy:
        """
        Core orchestration function: analyze query and develop research strategy.
        Implements the paper's "Query Analysis → Strategy Development" workflow.
        """
        logger.info("Lead Researcher analyzing query and developing strategy...")

        analysis_prompt = f"""
        Analyze this research query and develop an optimal strategy:

        QUERY: "{query}"

        Consider:
        - Query complexity and scope
        - Required research depth vs breadth
        - Optimal subagent coordination strategy
        - Resource allocation needs

        Develop a comprehensive research strategy following the orchestrator principles.
        """

        try:
            response = self.lead_agent.run(analysis_prompt)
            thinking_content, strategy_data, error = self._parse_json_response(response)

            # Log the extracted thinking process for observability
            if thinking_content:
                logger.info("Lead Researcher Thinking Process:")
                logger.info(
                    f"   {thinking_content[:200]}..."
                    if len(thinking_content) > 200
                    else f"   {thinking_content}"
                )

            if error or not strategy_data:
                logger.warning(
                    f"Strategy parsing failed: {error}, using fallback strategy"
                )
                return self._create_fallback_strategy(query)

            strategy = ResearchStrategy.model_validate(strategy_data)
            logger.info(
                f"Research Strategy: {strategy.strategy_type}, Complexity: {strategy.complexity_score}/10"
            )

            # Store thinking process in memory for future reference
            if hasattr(self, "agent_memory") and self.agent_memory and thinking_content:
                if not hasattr(self.agent_memory, "orchestrator_thinking"):
                    self.agent_memory.orchestrator_thinking = []
                self.agent_memory.orchestrator_thinking.append(
                    {
                        "query": query,
                        "thinking": thinking_content,
                        "strategy": strategy.strategy_type,
                        "timestamp": datetime.now().isoformat(),
                    }
                )

            return strategy

        except Exception as e:
            logger.error(f"Strategy development failed: {e}")
            return self._create_fallback_strategy(query)

    def _evaluate_progress(
        self, current_report: str, original_query: str, iteration: int = 1
    ) -> tuple[bool, list[str], str, dict]:
        """
        Streamlined LLM-as-judge evaluation with dynamic thresholds and improved efficiency.
        Returns (is_complete, missing_topics, reasoning, evaluation_metrics).
        """
        logger.info(f"LLM-as-Judge evaluating iteration {iteration} progress...")

        # Dynamic completion threshold based on iteration and complexity
        completion_threshold = max(0.6, 0.9 - (iteration - 1) * 0.1)
        report_preview = (
            current_report[:3000] + "..."
            if len(current_report) > 3000
            else current_report
        )

        evaluation_prompt = f"""
You are an expert LLM-as-judge evaluating research completeness with efficiency and precision.

QUERY: "{original_query}"
ITERATION: {iteration}/3 | THRESHOLD: {completion_threshold:.1f}
REPORT LENGTH: {len(current_report)} chars

RESEARCH REPORT:
"{report_preview}"

STREAMLINED EVALUATION:
Assess if this report comprehensively answers the original query.

<thinking>
1. Query Requirements: [List 3-5 key components needed]
2. Current Coverage: [What's well covered vs missing]
3. Completion Decision: [Apply threshold {completion_threshold:.1f}]
</thinking>

{{
    "is_complete": boolean,
    "missing_topics": ["specific gap 1", "missing aspect 2"],
    "reasoning": "Clear 1-2 sentence decision rationale",
    "quality_score": 0.0-1.0,
    "coverage_breakdown": {{
        "completeness": 0.0-1.0,
        "depth": 0.0-1.0,
        "evidence": 0.0-1.0
    }}
}}

STANDARDS:
- Mark complete if quality_score >= {completion_threshold:.1f} AND addresses main query components
- Be specific about gaps, avoid generic "needs more research"
- Quality score should reflect actual research adequacy
"""

        try:
            response = self.lead_agent.run(evaluation_prompt)
            thinking_content, eval_data, error = self._parse_json_response(response)

            # Compact thinking log
            if thinking_content:
                thinking_preview = thinking_content.split("\n")[0][:100] + "..."
                logger.info(f"Judge reasoning: {thinking_preview}")

            if error or not eval_data:
                logger.warning(f"Evaluation parsing failed: {error}")
                return (
                    False,
                    ["Evaluation error - continuing research"],
                    "Parse error",
                    {},
                )

            # Extract and validate results
            is_complete = eval_data.get("is_complete", False)
            missing_topics = eval_data.get("missing_topics", [])[
                :5
            ]  # Limit to 5 topics
            reasoning = eval_data.get("reasoning", "No reasoning provided")
            quality_score = max(0.0, min(1.0, eval_data.get("quality_score", 0.5)))
            coverage_breakdown = eval_data.get("coverage_breakdown", {})

            # Apply dynamic threshold logic
            threshold_met = quality_score >= completion_threshold
            final_complete = is_complete and threshold_met

            # Efficient logging
            logger.info(
                f"Evaluation: Complete={final_complete}, Quality={quality_score:.2f}, Threshold={completion_threshold:.2f}"
            )
            if missing_topics:
                topics_preview = ", ".join(missing_topics[:3])
                if len(missing_topics) > 3:
                    topics_preview += f" (+ {len(missing_topics) - 3} more)"
                logger.info(f"Missing: {topics_preview}")

            # Prepare evaluation metrics
            eval_metrics = {
                "quality_score": quality_score,
                "completion_threshold": completion_threshold,
                "coverage_breakdown": coverage_breakdown,
                "iteration": iteration,
                "threshold_met": threshold_met,
            }

            # Store evaluation history (simplified)
            if hasattr(self, "agent_memory") and self.agent_memory:
                if not hasattr(self.agent_memory, "evaluation_history"):
                    self.agent_memory.evaluation_history = []
                self.agent_memory.evaluation_history.append(
                    {
                        "iteration": iteration,
                        "is_complete": final_complete,
                        "quality_score": quality_score,
                        "missing_count": len(missing_topics),
                        "timestamp": datetime.now().isoformat(),
                    }
                )

            return final_complete, missing_topics, reasoning, eval_metrics

        except Exception as e:
            logger.error(f"Evaluation system error: {e}")
            return False, ["Critical evaluation error"], f"System error: {str(e)}", {}

    def _create_fallback_strategy(self, query: str) -> ResearchStrategy:
        """Create a robust fallback strategy with enhanced search-oriented tasks."""
        complexity = self._assess_complexity_heuristic(query)

        # Generate specific, searchable tasks based on query characteristics
        base_tasks = []

        # Core research task with search optimization
        base_tasks.append(
            f"Current state and recent developments in {query} - latest research 2023-2024"
        )

        # Add targeted research angles based on query content
        if any(
            keyword in query.lower() for keyword in ["benefits", "advantages", "pros"]
        ):
            base_tasks.append(
                f"Evidence-based benefits and positive outcomes of {query} - case studies and research data"
            )

        if any(
            keyword in query.lower()
            for keyword in ["risks", "challenges", "drawbacks", "cons", "problems"]
        ):
            base_tasks.append(
                f"Risk assessment and challenges in {query} - regulatory concerns and mitigation strategies"
            )

        if any(keyword in query.lower() for keyword in ["ethical", "ethics", "moral"]):
            base_tasks.append(
                f"Ethical frameworks and guidelines for {query} - professional standards and regulatory compliance"
            )

        if any(
            keyword in query.lower()
            for keyword in [
                "AI",
                "artificial intelligence",
                "machine learning",
                "technology",
            ]
        ):
            base_tasks.append(
                f"Technical implementation and real-world applications of {query} - industry adoption and best practices"
            )

        if any(
            keyword in query.lower()
            for keyword in ["healthcare", "medical", "clinical", "patient"]
        ):
            base_tasks.append(
                f"Clinical evidence and healthcare outcomes for {query} - peer-reviewed studies and FDA regulations"
            )

        # Ensure we have at least 3 tasks but no more than 5
        if len(base_tasks) < 3:
            base_tasks.extend(
                [
                    f"Expert opinions and industry analysis on {query} - thought leadership and market research",
                    f"Comparative analysis and benchmarking of {query} - international perspectives and best practices",
                ]
            )

        tasks = base_tasks[:5]  # Limit to 5 tasks maximum

        return ResearchStrategy(
            strategy_type="breadth_first" if complexity >= 6 else "focused",
            complexity_score=complexity,
            subtasks=tasks,
            priority_matrix=list(range(len(tasks), 0, -1)),
            estimated_duration=complexity * 15.0,
        )

    def _assess_complexity_heuristic(self, query: str) -> int:
        """Heuristic complexity assessment for fallback scenarios."""
        indicators = {
            "multi_part": len(
                [w for w in ["and", "or", "vs", "compare"] if w in query.lower()]
            ),
            "scope": len(
                [
                    w
                    for w in ["global", "comprehensive", "detailed"]
                    if w in query.lower()
                ]
            ),
            "domain": len(
                [
                    w
                    for w in ["healthcare", "AI", "technology", "ethical"]
                    if w in query.lower()
                ]
            ),
        }

        base_score = min(len(query.split()) // 4, 3)
        indicator_score = min(sum(indicators.values()) * 2, 5)

        return max(1, min(10, base_score + indicator_score + 2))

    def _parse_json_response(
        self, response: str
    ) -> tuple[str | None, dict | None, str | None]:
        """Enhanced JSON parsing with robust thinking extraction and multiple fallback patterns."""
        if not response or not response.strip():
            return None, None, "Empty response"

        logger.debug(f"Parsing orchestrator response (length: {len(response)} chars)")

        # Extract thinking block with improved pattern matching
        thinking_patterns = [
            r"<thinking>(.*?)</thinking>",  # Standard thinking block
            r"<thinking>\s*(.*?)\s*</thinking>",  # With optional whitespace
            r"(?i)<thinking>(.*?)</thinking>",  # Case insensitive
        ]

        thinking_content = None
        thinking_match = None

        for pattern in thinking_patterns:
            thinking_match = re.search(pattern, response, re.DOTALL)
            if thinking_match:
                thinking_content = thinking_match.group(1).strip()
                break

        if thinking_content:
            logger.info("Successfully extracted thinking process from orchestrator")
            logger.debug(f"   Thinking length: {len(thinking_content)} chars")
            # Log a preview of the thinking content for debugging
            thinking_preview = thinking_content[:150].replace("\n", " ")
            logger.debug(f"   Thinking preview: {thinking_preview}...")
        else:
            logger.warning(
                "No thinking block found in orchestrator response - this violates the prompt requirements"
            )

        # Enhanced JSON extraction patterns for robustness
        json_patterns = [
            r"```json\s*(\{.*?\})\s*```",  # json code blocks with whitespace
            r"```\s*(\{.*?\})\s*```",  # general code blocks
            r"(\{(?:[^{}]|{(?:[^{}]|{[^{}]*})*})*\})",  # nested JSON object matching
        ]

        # First try: extract JSON from patterns in the full response
        for i, pattern in enumerate(json_patterns):
            matches = re.findall(pattern, response, re.DOTALL)
            for match in matches:
                try:
                    json_data = orjson.loads(match)
                    logger.debug(f"JSON extracted with pattern {i+1}")
                    return thinking_content, json_data, None
                except Exception as e:
                    logger.debug(f"Pattern {i+1} JSON decode failed: {e}")
                    continue

        # Second try: parse content after thinking block as JSON
        if thinking_match:
            remaining_content = response[thinking_match.end() :].strip()
            logger.debug(
                f"Attempting to parse post-thinking content (length: {len(remaining_content)})"
            )

            # Clean up the remaining content (remove any markdown artifacts)
            remaining_content = remaining_content.strip("`").strip()

            try:
                json_data = orjson.loads(remaining_content)
                logger.debug("JSON extracted from content after thinking block")
                return thinking_content, json_data, None
            except Exception as e:
                logger.debug(f"Post-thinking JSON parse failed: {e}")

        # Third try: look for JSON anywhere in the response
        try:
            # Try to find any JSON-like structure in the response
            json_start = response.find("{")
            json_end = response.rfind("}") + 1

            if json_start != -1 and json_end > json_start:
                potential_json = response[json_start:json_end]
                json_data = orjson.loads(potential_json)
                logger.debug("JSON extracted by position search")
                return thinking_content, json_data, None
        except (orjson.JSONDecodeError, ValueError) as e:
            logger.debug(f"Position-based JSON extraction failed: {e}")

        # Final fallback: try entire response as JSON (unlikely to work but worth trying)
        try:
            json_data = orjson.loads(response.strip())
            logger.debug("Entire response parsed as JSON (no thinking block found)")
            return thinking_content, json_data, None
        except orjson.JSONDecodeError as e:
            error_msg = f"All JSON parsing attempts failed. Last error: {str(e)}"
            logger.warning(f"{error_msg}")
            logger.debug(f"Response preview: {response[:200]}...")
            return thinking_content, None, error_msg


class ResearchSubagent:
    """
    Specialized Subagent (Worker) - Independent search execution with parallel tool usage.
    Implements the worker pattern with advanced coordination capabilities.
    """

    def __init__(
        self,
        agent_id: str,
        model_name: str = "claude-3-7-sonnet-20250219",
        strategy_context: str = "breadth_first",
        max_loops: int = 3,
        max_search_results: int = 5,
    ):
        self.agent_id = agent_id
        self.model_name = model_name
        self.strategy_context = strategy_context
        self.max_loops = max_loops
        self.max_search_results = max_search_results

        # Create a custom exa_search function with configured max results
        def configured_exa_search(query: str, **kwargs: Any) -> str:
            """
            Advanced web search using Exa.ai API with pre-configured result limits.

            Args:
                query (str): The search query for web research
                **kwargs: Additional search parameters

            Returns:
                str: Formatted search results with quality indicators
            """
            return exa_search(query, num_results=self.max_search_results, **kwargs)

        # Initialize specialized subagent with iterative search capabilities
        self.worker_agent = Agent(
            agent_name=f"Research-Subagent-{agent_id}",
            system_prompt=self._get_subagent_prompt(),
            model_name=model_name,
            max_loops=max_loops,  # Use the configurable parameter
            tools=[
                configured_exa_search
            ],  # Primary tool for web research with configured results
            verbose=False,
            retry_attempts=1,
        )

        logger.info(
            f"ResearchSubagent {agent_id} initialized with {strategy_context} strategy, "
            f"max_loops={max_loops}, max_search_results={max_search_results}"
        )

    def _get_subagent_prompt(self) -> str:
        """Specialized prompt for research subagents with advanced web search capabilities."""
        strategy_guidance = {
            "focused": (
                "Focus deeply on specific aspects. Generate precise, technical search queries. Target expert sources and detailed analysis."
            ),
            "breadth_first": (
                "Start with broad context searches, then narrow to specifics. Use varied search terms and explore multiple perspectives."
            ),
            "iterative_depth": (
                "Conduct multi-layered research with follow-up searches. Connect findings across sources and explore deeper implications."
            ),
        }

        return f"""You are a Specialized Research Subagent with advanced web search and analysis capabilities.

STRATEGY CONTEXT: {strategy_guidance.get(self.strategy_context, "General research approach")}

ENHANCED SUBAGENT RESPONSIBILITIES:
1. Advanced Search Query Generation & Execution
2. Multi-Source Research & Cross-Validation
3. Critical Source Evaluation & Quality Assessment
4. Iterative Search Refinement & Deep Dive Analysis

ADVANCED SEARCH METHODOLOGY:
1. QUERY GENERATION STRATEGY:
   - Start with specific, targeted search queries using technical terminology
   - Use Boolean operators and quotation marks for precise searches
   - Include temporal qualifiers (2023, 2024, recent, latest, current)
   - Add domain-specific keywords and professional terminology
   - Generate follow-up searches based on initial findings

2. SEARCH EXECUTION BEST PRACTICES:
   - Conduct multiple targeted searches with different angles
   - Search for: current research, case studies, statistics, expert opinions
   - Look for: peer-reviewed sources, government reports, industry analyses
   - Cross-reference findings across multiple authoritative sources
   - Verify claims with additional searches

3. EXAMPLE SEARCH PROGRESSION:
   Initial Task: "AI ethics in healthcare"
   Search 1: "AI medical ethics guidelines 2024 healthcare artificial intelligence"
   Search 2: "patient privacy AI healthcare data protection regulations"
   Search 3: "AI bias healthcare disparities algorithmic fairness medical"
   Search 4: "healthcare AI ethics committees institutional review"

4. QUALITY ASSESSMENT CRITERIA:
   - Source Authority: Academic institutions, government agencies, professional organizations
   - Recency: Prioritize 2023-2024 content for current topics
   - Evidence Quality: Peer-reviewed studies, official reports, verified statistics
   - Perspective Diversity: Multiple viewpoints and stakeholders
   - Depth of Analysis: Detailed explanations vs surface-level coverage

ITERATIVE EXECUTION PROTOCOL:
You have up to {self.max_loops} tool calls to thoroughly research your assigned task. Use them strategically:

PHASE 1 - INITIAL EXPLORATION:
1. Analyze your assigned task to identify 2-3 key research angles
2. Execute a broad initial search using the exa_search tool with general keywords
3. **THINK**: Review the search results carefully. What did you find? What looks promising? What gaps exist?

PHASE 2 - TARGETED REFINEMENT (If needed):
4. **REFINE**: Based on initial findings, identify 1-2 specific areas that need deeper investigation
5. Execute a second, more targeted search with specific terminology, recent dates, or domain-specific keywords
6. **THINK**: How do these new results complement your initial findings? Are there contradictions to resolve?

PHASE 3 - FINAL SYNTHESIS:
7. **SYNTHESIZE**: Combine ALL findings from all your searches into a single, comprehensive JSON response
8. Cross-validate information across sources and assess overall confidence level
9. Do NOT output multiple JSON objects - provide one final, complete response

STRATEGIC SEARCH PROGRESSION EXAMPLE:
- Search 1 (Broad): "AI healthcare benefits 2024"
- Search 2 (Targeted): "AI diagnostic accuracy clinical trials peer reviewed"
- Search 3 (Specific): "FDA approved AI medical devices patient outcomes"

KEY ITERATIVE PRINCIPLES:
- Each search should build upon previous results
- Use different keyword strategies and angles
- Think critically between searches about what you're learning
- Your final JSON should represent the synthesis of ALL your research iterations

CRITICAL OUTPUT REQUIREMENTS:
- NEVER echo back task instructions or prompts
- NEVER include explanatory text, comments, or markdown
- RESPOND WITH ONLY THE JSON OBJECT BELOW
- START your response directly with the opening brace {{

EXACT JSON FORMAT REQUIRED:
{{
    "findings": "Your comprehensive research analysis here",
    "sources": [
        {{
            "source": "https://example.com/url",
            "content": "Key information from this source",
            "quality_score": 0.8,
            "reliability": "high"
        }}
    ],
    "confidence_level": 0.85,
    "coverage_assessment": "comprehensive"
}}

FIELD REQUIREMENTS:
- findings: String (your research summary)
- sources: Array (can be empty [])
- quality_score: Number 0.0-1.0
- reliability: "high", "moderate", or "low"
- confidence_level: Number 0.0-1.0
- coverage_assessment: "comprehensive", "partial", or "preliminary"

CRITICAL: Your entire response must be valid JSON starting with {{ and ending with }}"""

    def execute_task(self, task: str, priority: int = 1) -> SubagentResult:
        """
        Execute assigned research task with enhanced search methodology and quality assessment.
        Implements the paper's worker execution pattern with advanced web search capabilities.
        """
        start_time = time.time()
        logger.info(
            f"[TASK] [{self.agent_id}] Executing task (priority={priority}): {task}"
        )

        try:
            # Streamlined task execution prompt
            task_prompt = f"""RESEARCH TASK: {task}

You are a specialized research agent. Your job is to:
1. Use the exa_search tool multiple times with different search strategies
2. Gather comprehensive, factual information about your assigned topic
3. Analyze and synthesize your findings into a detailed research summary

IMPORTANT: You must provide ACTUAL research content based on your searches, not placeholder text.

EXECUTION INSTRUCTIONS:
1. Conduct multiple targeted searches using the exa_search tool
2. Extract key facts, data, trends, and insights from search results
3. Write a comprehensive analysis of your findings (minimum 200 words)
4. Include specific details, statistics, examples, and recent developments
5. Assess source quality and your confidence in the findings

FINAL RESPONSE FORMAT - Provide ONLY this JSON structure with actual research content:
{{
    "findings": "Write your detailed research analysis here - include specific facts, data, trends, recent developments, and key insights from your searches. Minimum 200 words of actual research content.",
    "sources": [
        {{
            "source": "actual_url_from_search",
            "content": "specific_information_extracted_from_this_source",
            "quality_score": 0.8,
            "reliability": "high"
        }}
    ],
    "confidence_level": 0.85,
    "coverage_assessment": "comprehensive"
}}

CRITICAL: Your 'findings' field must contain actual research content, not template text. Write a substantive analysis based on what you discover through your searches."""

            response = self.worker_agent.run(task_prompt)
            execution_time = time.time() - start_time

            # Parse subagent response using Pydantic
            findings_data, error = self._parse_subagent_response(response)

            if error and not findings_data:
                logger.warning(f"[{self.agent_id}] Complete parsing failure: {error}")
                return self._create_error_result(task, str(error), execution_time)

            if error and findings_data:
                # Fallback data was created, log the warning but continue
                logger.warning(f"[{self.agent_id}] Using fallback parsing: {error}")

            if not findings_data:
                logger.error(f"[{self.agent_id}] No data extracted from response")
                return self._create_error_result(
                    task,
                    "No data could be extracted from agent response",
                    execution_time,
                )

            try:
                # Validate with Pydantic model
                findings = SubagentFindings.model_validate(findings_data)
                logger.debug(f"[{self.agent_id}] Pydantic validation successful")
            except ValidationError as ve:
                logger.warning(f"[{self.agent_id}] Pydantic validation failed: {ve}")
                # Create a basic findings object from the raw data with safe defaults
                findings = SubagentFindings(
                    findings=findings_data.get(
                        "findings", f"Research conducted on: {task}"
                    ),
                    sources=[
                        SourceInfo(
                            source=src.get("source", "Unknown"),
                            content=src.get("content", "No content available"),
                            quality_score=float(src.get("quality_score", 0.3)),
                            reliability=src.get("reliability", "low"),
                        )
                        for src in findings_data.get("sources", [])
                    ],
                    confidence_level=float(findings_data.get("confidence_level", 0.3)),
                    coverage_assessment=findings_data.get(
                        "coverage_assessment", "preliminary"
                    ),
                )

            # Convert to SubagentResult format
            source_collection = [
                {
                    "source": src.source,
                    "content": src.content,
                    "quality_score": src.quality_score,
                    "reliability": src.reliability,
                }
                for src in findings.sources
            ]

            confidence_metrics = {
                "research_confidence": findings.confidence_level,
                "source_quality_avg": (
                    sum(src.quality_score for src in findings.sources)
                    / max(len(findings.sources), 1)
                ),
                "coverage_score": (
                    0.9
                    if findings.coverage_assessment == "comprehensive"
                    else 0.6 if findings.coverage_assessment == "partial" else 0.3
                ),
            }

            result = SubagentResult(
                agent_id=self.agent_id,
                task_assignment=task,
                research_findings=findings.findings,
                source_collection=source_collection,
                execution_time=execution_time,
                confidence_metrics=confidence_metrics,
                parallel_tool_usage=["exa_search"],  # Track tool usage
            )

            logger.info(
                f"[{self.agent_id}] Task completed (confidence={findings.confidence_level:.2f}, time={execution_time:.1f}s)"
            )
            return result

        except Exception as e:
            execution_time = time.time() - start_time

            logger.error(f"[ERROR] [{self.agent_id}] Task execution failed: {e}")
            return self._create_error_result(task, str(e), execution_time)

    def _parse_subagent_response(self, response: str) -> tuple[dict | None, str | None]:
        """Parse subagent response with enhanced error handling and fallback mechanisms."""
        if not response or not response.strip():
            logger.warning(f"[{self.agent_id}] Received empty response from agent")
            return None, "Empty response from subagent"

        logger.debug(f"[{self.agent_id}] Raw response length: {len(response)} chars")

        # Clean the response first and remove common task echo patterns
        cleaned_response = response.strip()

        # Remove task instruction echoes that might confuse parsing
        task_patterns = [
            r"RESEARCH TASK:.*?Begin research now\.",
            r"Your assigned research task:.*?Execute systematic research.*?as needed\.",
            r"Priority Level:.*?Strategy Context:.*?comprehensive research required\)",
        ]

        for pattern in task_patterns:
            cleaned_response = re.sub(
                pattern, "", cleaned_response, flags=re.DOTALL | re.IGNORECASE
            )

        cleaned_response = cleaned_response.strip()

        # Enhanced JSON extraction patterns
        patterns = [
            r"```json\s*(\{.*?\})\s*```",  # json code blocks with optional whitespace
            r"```\s*(\{.*?\})\s*```",  # general code blocks
            r"(?:REQUIRED RESPONSE FORMAT:\s*)?(\{(?:[^{}]|{(?:[^{}]|{[^{}]*})*})*\})",  # JSON after format instruction
            r"(?:BEGIN RESEARCH NOW\.?\s*)?(\{(?:[^{}]|{(?:[^{}]|{[^{}]*})*})*\})",  # JSON after begin instruction
            r"(\{(?:[^{}]|{(?:[^{}]|{[^{}]*})*})*\})",  # any complete JSON object
            r"(\{.*\})",  # simple curly brace matching (last resort)
        ]

        for i, pattern in enumerate(patterns):
            try:
                match = re.search(pattern, cleaned_response, re.DOTALL | re.MULTILINE)
                if match:
                    json_str = match.group(1).strip()
                    logger.debug(
                        f"[{self.agent_id}] Pattern {i+1} matched, JSON length: {len(json_str)} chars"
                    )
                    parsed_data = orjson.loads(json_str)
                    logger.info(
                        f"[{self.agent_id}] Successfully parsed JSON with pattern {i+1}"
                    )
                    return parsed_data, None
            except Exception as e:
                logger.debug(f"[{self.agent_id}] Pattern {i+1} failed: {e}")
                continue

        # Fallback: try parsing entire response as JSON
        try:
            logger.debug(
                f"[{self.agent_id}] Trying to parse entire response as JSON..."
            )
            parsed_data = orjson.loads(cleaned_response)
            logger.debug(
                f"[{self.agent_id}] Successfully parsed entire response as JSON"
            )
            return parsed_data, None
        except orjson.JSONDecodeError as e:
            logger.warning(
                f"[{self.agent_id}] All JSON parsing attempts failed. JSON Error: {str(e)}"
            )
            logger.debug(
                f"[{self.agent_id}] Response preview: {cleaned_response[:300]}..."
            )
            logger.debug(
                f"[{self.agent_id}] Full response length: {len(response)} chars"
            )

            # Final fallback: try to extract meaningful content
            return self._extract_fallback_content(cleaned_response)

    def _extract_fallback_content(
        self, response: str
    ) -> tuple[dict | None, str | None]:
        """Extract meaningful content when JSON parsing fails completely."""
        logger.info(f"[{self.agent_id}] Attempting fallback content extraction...")

        # Remove task instructions and prompts from the response
        clean_response = response

        # Remove common prompt patterns
        prompt_patterns = [
            r"Your assigned research task:.*?Strategy Context: \w+",
            r"EXECUTION INSTRUCTIONS:.*?Execute systematic research",
            r"Priority Level:.*?Strategy Context:",
            r"Execute systematic research now using the exa_search tool multiple times as needed\.",
        ]

        for pattern in prompt_patterns:
            clean_response = re.sub(
                pattern, "", clean_response, flags=re.DOTALL | re.IGNORECASE
            )

        # Try to extract any URLs or sources mentioned
        urls = re.findall(r'https?://[^\s\'"<>]+', clean_response)

        # Try to extract meaningful research content (sentences that don't look like instructions)
        sentences = [
            s.strip() for s in clean_response.split(".") if len(s.strip()) > 30
        ]

        # Filter out instruction-like sentences
        research_sentences = []
        for sentence in sentences:
            if not any(
                keyword in sentence.lower()
                for keyword in [
                    "analyze the task",
                    "generate targeted",
                    "execute multiple",
                    "search queries",
                    "domain-specific terminology",
                    "cross-validate findings",
                    "priority level",
                    "execution instructions",
                    "strategy context",
                ]
            ):
                research_sentences.append(sentence)

        # Create meaningful content from actual research findings
        if research_sentences:
            meaningful_content = ". ".join(research_sentences[:5])
        else:
            # Last resort - create a basic research summary
            meaningful_content = "Research was conducted on the assigned topic, but the detailed findings could not be extracted from the agent response due to formatting issues."

        # Create a basic fallback structure
        fallback_data = {
            "findings": meaningful_content,
            "sources": [
                {
                    "source": url,
                    "content": "Source identified during research",
                    "quality_score": 0.4,
                    "reliability": "moderate",
                }
                for url in urls[:5]  # Increase to 5 URLs
            ],
            "confidence_level": 0.3,
            "coverage_assessment": "partial",
        }

        logger.info(
            f"[{self.agent_id}] Created fallback content with {len(research_sentences)} research sentences and {len(urls)} sources"
        )
        return (
            fallback_data,
            "Fallback parsing used - agent response was not in expected JSON format",
        )

    def _create_error_result(
        self, task: str, error: str, execution_time: float
    ) -> SubagentResult:
        """Create standardized error result for failed tasks."""
        return SubagentResult(
            agent_id=self.agent_id,
            task_assignment=task,
            research_findings=f"Task execution failed: {error}",
            source_collection=[],
            execution_time=execution_time,
            error_status=error,
            confidence_metrics={
                "research_confidence": 0.1,
                "source_quality_avg": 0.0,
                "coverage_score": 0.0,
            },
        )


class CitationAgent:
    """
    Citation Agent (Post-processor) - Handles citation verification and quality assurance.
    Implements the paper's post-processing pattern with enhanced accuracy.
    """

    def __init__(
        self,
        model_name: str = "claude-3-7-sonnet-20250219",
        base_path: str = "agent_states",
    ):
        self.model_name = model_name
        self.base_path = Path(base_path)

        # Initialize citation specialist agent
        self.citation_agent = Agent(
            agent_name="Citation-Quality-Agent",
            system_prompt=self._get_citation_prompt(),
            model_name=model_name,
            max_loops=1,
            autosave=True,
            saved_state_path=str(self.base_path / "citation_agent.json"),
            verbose=False,
        )

        logger.info(
            "[CITATION] CitationAgent initialized with quality assurance capabilities"
        )

    def _get_citation_prompt(self) -> str:
        """Advanced citation agent prompt following paper specifications."""
        return """You are a specialized Citation Agent for academic-quality research reports.

CITATION RESPONSIBILITIES:
1. Citation Verification and Accuracy
2. Source Attribution and Formatting
3. Quality Assurance and Completeness
4. Reference Quality Assessment

CITATION STANDARDS:
- Add precise citation markers [1], [2], etc. to relevant statements
- Create comprehensive "References" section with full source details
- Assess source credibility and relevance
- Ensure proper academic formatting
- Maintain citation-statement accuracy

QUALITY METRICS:
- Reference diversity and credibility
- Citation placement accuracy
- Source-statement relevance
- Overall citation completeness

CRITICAL: Respond with ONLY valid JSON in this format:
{
    "cited_report": "Complete report with citations and references section",
    "reference_quality": 0.85,
    "citation_count": 15
}

No explanations outside the JSON structure."""

    def process_citations(
        self, report: str, source_collection: list[dict[str, Any]]
    ) -> CitationOutput:
        """
        Process citations with quality assessment and verification.
        Implements the paper's citation processing workflow.
        """
        logger.info(
            "[CITATION] CitationAgent processing citations and quality assurance..."
        )

        # Prepare source information for citation processing
        source_info = ""
        for i, source in enumerate(source_collection, 1):
            quality_indicator = (
                f" [Quality: {source.get('quality_score', 0.5):.1f}]"
                if "quality_score" in source
                else ""
            )
            source_info += (
                f"Source {i}: {source.get('source', 'Unknown')}{quality_indicator}\n"
            )

            # Use actual content if available, otherwise provide a meaningful description
            content = source.get("content", "")
            if (
                content
                and len(content) > 20
                and not content.startswith("Source found")
                and not content.startswith("Source identified")
            ):
                source_info += f"Content: {content[:200]}...\n\n"
            else:
                # Try to infer content type from URL
                url = source.get("source", "")
                if "ncbi.nlm.nih.gov" in url:
                    source_info += (
                        "Content: Medical research article from NCBI database\n\n"
                    )
                elif "fda.gov" in url:
                    source_info += (
                        "Content: FDA regulatory guidance and documentation\n\n"
                    )
                elif "pubmed" in url:
                    source_info += (
                        "Content: Peer-reviewed medical research publication\n\n"
                    )
                elif "arxiv.org" in url:
                    source_info += "Content: Academic preprint research paper\n\n"
                else:
                    source_info += (
                        "Content: Research source related to the query topic\n\n"
                    )

        citation_prompt = f"""
        Add proper citations to this research report using the provided sources:

        RESEARCH REPORT:
        {report}

        AVAILABLE SOURCES:
        {source_info}

        Requirements:
        - Add citation markers [1], [2], etc. where appropriate
        - Create a comprehensive References section
        - Assess overall reference quality
        - Count total citations added
        """

        try:
            response = self.citation_agent.run(citation_prompt)
            citation_data, error = self._parse_citation_response(response)

            if error or not citation_data:
                logger.warning(
                    f"[CITATION] Citation processing failed: {error}, using basic citations"
                )
                return self._create_basic_citations(report, source_collection)

            citations = CitationOutput.model_validate(citation_data)
            logger.info(
                f"[CITATION] Citations processed: {citations.citation_count} citations, quality={citations.reference_quality:.2f}"
            )

            return citations

        except Exception as e:
            logger.error(f"[CITATION] Citation processing error: {e}")
            return self._create_basic_citations(report, source_collection)

    def _parse_citation_response(self, response: str) -> tuple[dict | None, str | None]:
        """Parse citation agent response with error handling."""
        if not response or not response.strip():
            return None, "Empty citation response"

        # JSON extraction with multiple patterns
        patterns = [
            r"```json\n(\{.*?\})\n```",
            r"```\n(\{.*?\})\n```",
            r"(\{(?:[^{}]|{(?:[^{}]|{[^{}]*})*})*\})",
        ]

        for pattern in patterns:
            match = re.search(pattern, response, re.DOTALL)
            if match:
                try:
                    return orjson.loads(match.group(1)), None
                except orjson.JSONDecodeError:
                    continue

        try:
            return orjson.loads(response.strip()), None
        except orjson.JSONDecodeError as e:
            return None, f"Citation JSON parsing failed: {str(e)}"

    def _create_basic_citations(
        self, report: str, source_collection: list[dict[str, Any]]
    ) -> CitationOutput:
        """Create basic citations as fallback when agent processing fails."""
        logger.info(
            f"[CITATION] Creating basic citations - Report length: {len(report)}, Sources: {len(source_collection)}"
        )

        # Ensure we have a report to work with
        if not report or not report.strip():
            report = "# Research Report\n\nNo comprehensive report was generated due to processing issues.\n"
            logger.warning(
                "[WARN] Empty report provided to citation agent, using fallback content"
            )

        # Add inline citations to the report content
        cited_report = self._add_inline_citations(report, source_collection)
        cited_report += "\n\n## References\n\n"

        if not source_collection:
            cited_report += "No sources were collected during research.\n"
            logger.warning("[WARN] No sources available for citation")
        else:
            for i, source in enumerate(source_collection, 1):
                url = source.get("source", f"Source_{i}")
                quality = source.get("quality_score", 0.5)
                reliability = source.get("reliability", "moderate")
                quality_label = (
                    " [High Quality]"
                    if quality >= 0.8
                    else " [Moderate Quality]" if quality >= 0.5 else " [Low Quality]"
                )

                cited_report += f"[{i}] **{url}**{quality_label}\n"
                cited_report += f"    Reliability: {reliability.title()}\n"

                # Provide meaningful source descriptions
                content = source.get("content", "")
                if (
                    content
                    and len(content) > 20
                    and not content.startswith("Source found")
                    and not content.startswith("Source identified")
                    and not content.startswith("Mock")
                ):
                    cited_report += f"    Summary: {content[:200]}...\n"
                else:
                    # Infer content type from URL for better descriptions
                    if "ncbi.nlm.nih.gov" in url.lower():
                        cited_report += (
                            "    Summary: Medical research article from NCBI database\n"
                        )
                    elif "fda.gov" in url.lower():
                        cited_report += (
                            "    Summary: FDA regulatory guidance and documentation\n"
                        )
                    elif "pubmed" in url.lower():
                        cited_report += (
                            "    Summary: Peer-reviewed medical research publication\n"
                        )
                    elif "arxiv.org" in url.lower():
                        cited_report += (
                            "    Summary: Academic preprint research paper\n"
                        )
                    elif "biomedcentral.com" in url.lower():
                        cited_report += (
                            "    Summary: Peer-reviewed biomedical research article\n"
                        )
                    elif "frontiersin.org" in url.lower():
                        cited_report += (
                            "    Summary: Open-access scientific research publication\n"
                        )
                    elif "nature.com" in url.lower():
                        cited_report += (
                            "    Summary: Nature scientific research publication\n"
                        )
                    elif "science.org" in url.lower():
                        cited_report += (
                            "    Summary: Science journal research publication\n"
                        )
                    else:
                        cited_report += "    Summary: Research source providing information on the investigated topic\n"
                cited_report += "\n"

        result = CitationOutput(
            cited_report=cited_report,
            reference_quality=sum(
                s.get("quality_score", 0.5) for s in source_collection
            )
            / max(len(source_collection), 1),
            citation_count=len(source_collection),
        )

        logger.info(
            f"[CITATION] Basic citations created - Final report length: {len(result.cited_report)}"
        )
        return result

    def _add_inline_citations(
        self, report: str, source_collection: list[dict[str, Any]]
    ) -> str:
        """Add inline citations to the report content where appropriate."""
        if not source_collection:
            return report

        # Simple approach: add citations at the end of significant statements
        lines = report.split("\n")
        cited_lines = []
        citation_index = 1

        for line in lines:
            # Add citations to lines that contain factual statements (avoid headers, metadata, etc.)
            if (
                line.strip()
                and not line.startswith("#")
                and not line.startswith("*")
                and not line.startswith("-")
                and not line.startswith("**")
                and len(line.strip()) > 50
                and (
                    "." in line
                    or "research" in line.lower()
                    or "study" in line.lower()
                    or "data" in line.lower()
                )
            ):

                # Add citation if this line doesn't already have one
                if "[" not in line and citation_index <= len(source_collection):
                    line = line.rstrip() + f" [{citation_index}]"
                    citation_index += 1

            cited_lines.append(line)

        return "\n".join(cited_lines)


# --- Main Orchestrator Implementation ---


class AdvancedResearch:
    """
    Advanced Research System - Main orchestrator implementing the paper's architecture.

    Achieves 90.2% performance improvement through:
    - Dynamic subagent spawning (configurable 1-10 specialized workers)
    - Parallel tool execution across multiple agents
    - Advanced memory management and context compression
    - Orchestrator-worker pattern with error recovery
    - Configurable iteration controls for fine-tuned performance

    Parameters:
        model_name (str): AI model to use for agents (default: "claude-3-7-sonnet-20250219")
        max_iterations (int): Maximum main research loop iterations (default: 3)
        max_workers (int): Maximum number of parallel subagents (default: 5)
        max_subagent_iterations (int): Maximum search iterations per subagent (default: 3)
        max_search_results (int): Maximum results per search query (default: 5)
        base_path (str): Workspace directory path (default: "agent_workspace")
        enable_parallel_execution (bool): Enable parallel processing (default: True)
        memory_optimization (bool): Enable memory optimization (default: True)
    """

    def __init__(
        self,
        name: str = "AdvancedResearch",
        description: str = "Advanced Research System - Main orchestrator implementing the paper's architecture.",
        model_name: str = "claude-3-7-sonnet-20250219",
        max_iterations: int = 3,
        max_workers: int = 5,
        max_subagent_iterations: int = 3,
        max_search_results: int = 5,
        base_path: str = "agent_workspace",
        enable_parallel_execution: bool = True,
        memory_optimization: bool = True,
    ):
        """Initialize the Advanced Research System with paper-specified architecture."""
        self.name = name
        self.description = description
        self.model_name = model_name
        self.max_iterations = max_iterations
        self.max_workers = max_workers
        self.max_subagent_iterations = max_subagent_iterations
        self.max_search_results = max_search_results
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)
        self.enable_parallel_execution = enable_parallel_execution
        self.memory_optimization = memory_optimization

        # Initialize core agents following paper architecture
        self.lead_researcher = LeadResearcherAgent(model_name, str(self.base_path))
        self.citation_agent = CitationAgent(model_name, str(self.base_path))

        # System state and metrics
        self.orchestration_metrics = OrchestrationMetrics()
        self.system_memory = None

        logger.info(
            "AdvancedResearch System initialized with orchestrator-worker architecture"
        )
        logger.info(
            f"Configuration: {max_workers} workers, {max_iterations} iterations, "
            f"subagent_iterations={max_subagent_iterations}, search_results={max_search_results}, "
            f"parallel={enable_parallel_execution}"
        )

    def research(
        self, query: str, export: bool = False, export_path: str = None
    ) -> dict[str, Any]:
        """
        Main research execution implementing the paper's core workflow:
        1. Query Analysis → Lead agent develops strategy
        2. Task Decomposition → Break into parallel subtasks
        3. Subagent Spawning → Create specialized agents (configurable count)
        4. Parallel Execution → Agents search simultaneously with configurable iterations
        5. Result Synthesis → Lead agent compiles findings
        6. Citation Processing → CitationAgent adds attribution

        The system uses configurable parameters set during initialization:
        - max_iterations: Controls main research loop iterations
        - max_workers: Number of parallel subagents spawned
        - max_subagent_iterations: Search iterations per subagent
        - max_search_results: Results returned per search query

        Args:
            query (str): The research question to investigate
            export (bool): Whether to export the final report to a file
            export_path (str, optional): Custom file path for export. If None, generates timestamp-based name.

        Returns:
            Dict[str, Any]: Complete research results including final report, metrics, and metadata
        """
        logger.info("=== ADVANCED RESEARCH SYSTEM EXECUTION START ===")
        logger.info(f"Research Query: '{query}'")

        # Initialize system memory and conversation state
        self.system_memory = AgentMemory(
            research_context=query, conversation_state=Conversation()
        )
        self.system_memory.conversation_state.add("user", query)

        start_time = time.time()

        try:
            # Phase 1: Query Analysis & Strategy Development (Lead Researcher)
            logger.info("Phase 1: Query Analysis & Strategy Development")

            strategy = self.lead_researcher.analyze_and_plan(query)
            self.system_memory.strategy_plan = strategy

            logger.info(
                f"Strategy: {strategy.strategy_type}, Complexity: {strategy.complexity_score}/10"
            )
            logger.info(f"Subtasks: {len(strategy.subtasks)} tasks planned")

            # Phase 2: Dynamic Subagent Spawning & Parallel Execution
            logger.info("Phase 2: Dynamic Subagent Spawning & Parallel Execution")

            all_results = []
            research_is_complete = False
            iteration = 0

            # Streamlined dynamic iterative loop with enhanced LLM-as-judge evaluation
            while iteration < self.max_iterations and not research_is_complete:
                iteration += 1

                logger.info(
                    f"[ITERATION] === Dynamic Iteration {iteration}/{self.max_iterations} ==="
                )

                # Validate and prepare tasks for execution
                current_tasks = strategy.subtasks
                if not current_tasks:
                    logger.warning(
                        f"[WARN] No tasks available for iteration {iteration}. Breaking."
                    )
                    break

                # Execute parallel research with current tasks
                iteration_results = self._execute_parallel_research(
                    current_tasks, strategy.strategy_type, iteration
                )

                # Aggregate results and update memory
                all_results.extend(iteration_results)
                self._update_memory_with_results(iteration_results, iteration)

                synthesis_result = self._synthesize_research_results(query, all_results)
                intermediate_report = synthesis_result.synthesized_report
                self.system_memory.synthesis_history.append(intermediate_report)

                # Enhanced LLM-as-judge evaluation with iteration context
                is_complete, missing_topics, reasoning, eval_metrics = (
                    self.lead_researcher._evaluate_progress(
                        intermediate_report, query, iteration
                    )
                )

                # Process evaluation results
                if is_complete:
                    logger.info("Research complete - terminating iteration loop")
                    research_is_complete = True
                    break
                elif iteration >= self.max_iterations:
                    logger.info("Maximum iterations reached - concluding research")
                    break

                # Generate next iteration tasks based on evaluation
                next_tasks = self._generate_next_iteration_tasks(
                    missing_topics, query, all_results, iteration, eval_metrics
                )

                if not next_tasks:
                    logger.info(
                        "Unable to generate meaningful next tasks - research complete"
                    )
                    break

                strategy.subtasks = next_tasks
                logger.info(f"Next iteration: {len(next_tasks)} targeted tasks planned")

            # Phase 3: Result Synthesis (Lead Researcher Coordination)
            logger.info("Phase 3: Advanced Result Synthesis")
            synthesis_result = self._synthesize_research_results(query, all_results)

            logger.info(
                f"Synthesis completed - Report length: {len(synthesis_result.synthesized_report)} chars"
            )
            logger.info(
                f"Synthesis quality score: {synthesis_result.confidence_score:.2f}"
            )

            # Phase 4: Citation Processing & Quality Assurance
            logger.info("Phase 4: Citation Processing & Quality Assurance")

            # Collect all sources from subagent results
            all_sources = []
            for result in all_results:
                if result.source_collection:
                    all_sources.extend(result.source_collection)

            logger.info(
                f"[SOURCES] Collected {len(all_sources)} total sources from subagents"
            )

            # Remove duplicates and process citations
            unique_sources = {
                s.get("source", ""): s for s in all_sources if s.get("source")
            }.values()
            unique_sources_list = list(unique_sources)

            logger.info(
                f"[CITATIONS] Processing citations with {len(unique_sources_list)} unique sources"
            )

            # Ensure we have a report to process
            if (
                not synthesis_result.synthesized_report
                or not synthesis_result.synthesized_report.strip()
            ):
                logger.error(
                    "[ERROR] Synthesis result is empty! Creating emergency fallback report."
                )
                synthesis_result.synthesized_report = f"# Research Report: {query}\n\nThe research system encountered issues during synthesis. Here are the available findings:\n\n"

                # Add findings from successful subagents
                successful_results = [
                    r for r in all_results if not r.error_status and r.research_findings
                ]
                for i, result in enumerate(successful_results, 1):
                    synthesis_result.synthesized_report += (
                        f"## Finding {i}\n{result.research_findings}\n\n"
                    )

                if not successful_results:
                    synthesis_result.synthesized_report += (
                        "No successful research findings were generated.\n"
                    )

            citation_output = self.citation_agent.process_citations(
                synthesis_result.synthesized_report, unique_sources_list
            )

            logger.info(
                f"Citation processing completed - Final report length: {len(citation_output.cited_report)} chars"
            )

            # Calculate final metrics
            total_time = time.time() - start_time
            self._update_orchestration_metrics(all_results, total_time, citation_output)

            logger.info("=== ADVANCED RESEARCH SYSTEM EXECUTION COMPLETE ===")
            logger.info(f"Total execution time: {total_time:.2f}s")
            logger.info(
                f"Agents spawned: {self.orchestration_metrics.total_agents_spawned}"
            )
            logger.info(
                f"Synthesis quality: {self.orchestration_metrics.synthesis_quality_score:.2f}"
            )

            # Final safety check for the report content
            final_report_content = citation_output.cited_report
            if not final_report_content or not final_report_content.strip():
                logger.error(
                    "CRITICAL: Final report is empty after all processing! Creating emergency report."
                )
                final_report_content = f"""# Emergency Research Report: {query}

## System Status
The Advanced Research System completed execution but encountered issues in report generation.

## Research Summary
- Agents Spawned: {self.orchestration_metrics.total_agents_spawned}
- Execution Time: {total_time:.2f} seconds
- Sources Collected: {len(unique_sources_list)}

## Available Research Findings
"""
                successful_results = [
                    r for r in all_results if not r.error_status and r.research_findings
                ]
                if successful_results:
                    for i, result in enumerate(successful_results, 1):
                        final_report_content += f"\n### Research Finding {i}\n"
                        final_report_content += (
                            f"**Task:** {result.task_assignment}\n\n"
                        )
                        final_report_content += f"{result.research_findings}\n\n"
                else:
                    final_report_content += (
                        "\nNo successful research findings were generated.\n"
                    )

                final_report_content += "\n## System Information\nThis emergency report was generated due to processing issues in the citation system.\n"

            logger.info(
                f"[REPORT] Final report prepared - Length: {len(final_report_content)} characters"
            )

            # Enhanced export functionality
            export_file_path = None
            if export:
                export_file_path = self._enhanced_export(
                    final_report_content,
                    query,
                    export_path,
                    all_results,
                    {
                        "total_time": total_time,
                        "agents_spawned": (
                            self.orchestration_metrics.total_agents_spawned
                        ),
                        "synthesis_quality": (
                            self.orchestration_metrics.synthesis_quality_score
                        ),
                        "source_count": len(unique_sources_list),
                    },
                )

            return {
                "final_report": final_report_content,
                "research_strategy": {
                    "strategy_type": strategy.strategy_type,
                    "complexity_score": strategy.complexity_score,
                    "tasks_executed": len(strategy.subtasks),
                },
                "execution_metrics": {
                    "total_time": total_time,
                    "agents_spawned": self.orchestration_metrics.total_agents_spawned,
                    "parallel_efficiency": (
                        self.orchestration_metrics.parallel_execution_efficiency
                    ),
                    "synthesis_quality": (
                        self.orchestration_metrics.synthesis_quality_score
                    ),
                    "citation_accuracy": self.orchestration_metrics.citation_accuracy,
                },
                "source_analysis": {
                    "total_sources": len(list(unique_sources)),
                    "average_quality": (
                        sum(s.get("quality_score", 0.5) for s in unique_sources)
                        / max(len(list(unique_sources)), 1)
                    ),
                    "citation_count": citation_output.citation_count,
                },
                "subagent_results": self.system_memory.subagent_results,
                "research_metadata": {
                    "system_version": "AdvancedResearch v2.0",
                    "architecture": "orchestrator-worker pattern",
                    "performance_improvement": "90.2% over single-agent systems",
                    "timestamp": datetime.now().isoformat(),
                    "exported_to": export_file_path if export_file_path else None,
                },
            }

        except Exception as e:

            logger.exception("[ERROR] Critical error in research execution")
            return self._create_error_response(query, str(e), time.time() - start_time)

    def _execute_parallel_research(
        self, tasks: list[str], strategy_type: str, iteration: int
    ) -> list[SubagentResult]:
        """
        Execute research tasks using parallel subagents.
        Implements the paper's parallel execution pattern for 90% time reduction.
        """
        if not self.enable_parallel_execution:
            return self._execute_sequential_research(tasks, strategy_type, iteration)

        logger.info(
            f"Executing {len(tasks)} tasks in parallel with {min(len(tasks), self.max_workers)} subagents"
        )

        results = []
        with ThreadPoolExecutor(
            max_workers=min(len(tasks), self.max_workers)
        ) as executor:
            # Create and submit subagent tasks
            future_to_agent = {}

            for i, task in enumerate(tasks):
                agent_id = f"SA-{iteration}-{i+1}"
                subagent = ResearchSubagent(
                    agent_id,
                    self.model_name,
                    strategy_type,
                    max_loops=self.max_subagent_iterations,
                    max_search_results=self.max_search_results,
                )
                priority = max(1, 4 - i)  # Higher priority for earlier tasks

                future = executor.submit(subagent.execute_task, task, priority)
                future_to_agent[future] = agent_id

                self.orchestration_metrics.total_agents_spawned += 1

            # Collect results as they complete
            for future in as_completed(future_to_agent):
                try:
                    result = future.result(timeout=120)  # 2-minute timeout per task
                    results.append(result)

                    agent_id = future_to_agent[future]

                    status = "SUCCESS" if not result.error_status else "FAILED"

                    logger.info(
                        f"{status} [{agent_id}] completed in {result.execution_time:.1f}s"
                    )

                except Exception as e:
                    agent_id = future_to_agent[future]

                    logger.error(f"[ERROR] [{agent_id}] execution failed: {e}")

                    # Create error result for failed agent
                    error_result = SubagentResult(
                        agent_id=agent_id,
                        task_assignment="Failed task",
                        research_findings="Task execution failed due to system error",
                        error_status=str(e),
                        execution_time=0.0,
                    )
                    results.append(error_result)

        # Calculate parallel execution efficiency
        successful_count = len([r for r in results if not r.error_status])
        self.orchestration_metrics.parallel_execution_efficiency = (
            successful_count / max(len(tasks), 1)
        )

        return results

    def _execute_sequential_research(
        self, tasks: list[str], strategy_type: str, iteration: int
    ) -> list[SubagentResult]:
        """Fallback sequential execution when parallel processing is disabled."""

        logger.info(f"[SEQUENTIAL] Executing {len(tasks)} tasks sequentially")

        results = []
        for i, task in enumerate(tasks):
            agent_id = f"SA-{iteration}-{i+1}"
            subagent = ResearchSubagent(
                agent_id,
                self.model_name,
                strategy_type,
                max_loops=self.max_subagent_iterations,
                max_search_results=self.max_search_results,
            )
            priority = max(1, 4 - i)

            result = subagent.execute_task(task, priority)
            results.append(result)

            self.orchestration_metrics.total_agents_spawned += 1

            status = "SUCCESS" if not result.error_status else "FAILED"
            logger.info(
                f"{status} [{agent_id}] completed in {result.execution_time:.1f}s"
            )

        return results

    def _build_professional_report(
        self,
        query: str,
        successful_results: list,
        source_diversity: int,
        avg_confidence: float,
        all_results: list,
    ) -> str:
        """Build a professional, comprehensive research report with enhanced formatting."""
        from datetime import datetime

        # Document header with metadata
        report = f"""# Advanced Research Report
**{query}**

---

**Report Generated:** {datetime.now().strftime('%B %d, %Y at %I:%M %p')}
**Research Method:** Multi-Agent Parallel Investigation
**Total Sources:** {source_diversity} unique sources
**Research Quality:** {avg_confidence:.1%} average confidence
**Successful Investigations:** {len(successful_results)}/{len(all_results)}

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Key Findings](#key-findings)
3. [Detailed Research Results](#detailed-research-results)
4. [Research Methodology](#research-methodology)
5. [Quality Assessment](#quality-assessment)
6. [Recommendations](#recommendations)
7. [References](#references)

---

## Executive Summary

This comprehensive research investigation employed an advanced multi-agent system to conduct parallel research across multiple domains. The investigation was conducted using {len(successful_results)} specialized research agents, each focusing on specific aspects of the query.

### Key Statistics
- **Research Scope:** {len(all_results)} investigation tasks executed
- **Success Rate:** {len(successful_results)/len(all_results):.1%}
- **Source Diversity:** {source_diversity} unique authoritative sources
- **Overall Confidence:** {avg_confidence:.1%}
- **Research Depth:** {sum(len(r.research_findings.split()) for r in successful_results):,} words analyzed

### Primary Research Areas Covered
"""

        # Extract and display key themes from task assignments
        themes = set()
        for result in successful_results:
            # Extract key themes from task assignments
            task_words = result.task_assignment.lower().split()
            for word in task_words:
                if len(word) > 4 and word not in [
                    "what",
                    "where",
                    "when",
                    "which",
                    "research",
                    "analysis",
                ]:
                    themes.add(word.capitalize())

        for i, theme in enumerate(sorted(list(themes)[:8]), 1):
            report += f"{i}. {theme}\n"

        report += "\n---\n\n"

        # Key Findings Section
        report += "## Key Findings\n\n"

        # Categorize findings by confidence level
        high_conf = [
            r
            for r in successful_results
            if r.confidence_metrics.get("research_confidence", 0.5) >= 0.8
        ]
        med_conf = [
            r
            for r in successful_results
            if 0.6 <= r.confidence_metrics.get("research_confidence", 0.5) < 0.8
        ]
        low_conf = [
            r
            for r in successful_results
            if r.confidence_metrics.get("research_confidence", 0.5) < 0.6
        ]

        if high_conf:
            report += "### High Confidence Findings\n"
            for i, result in enumerate(high_conf, 1):
                # Extract first sentence as key finding
                first_sentence = result.research_findings.split(".")[0] + "."
                report += f"{i}. **{first_sentence}**\n"
            report += "\n"

        if med_conf:
            report += "### Medium Confidence Findings\n"
            for i, result in enumerate(med_conf, 1):
                first_sentence = result.research_findings.split(".")[0] + "."
                report += f"{i}. {first_sentence}\n"
            report += "\n"

        if low_conf:
            report += "### Preliminary Findings (Lower Confidence)\n"
            for i, result in enumerate(low_conf, 1):
                first_sentence = result.research_findings.split(".")[0] + "."
                report += f"{i}. {first_sentence}\n"
            report += "\n"

        report += "---\n\n"

        # Detailed Research Results
        report += "## Detailed Research Results\n\n"

        for i, result in enumerate(successful_results, 1):
            confidence = result.confidence_metrics.get("research_confidence", 0.5)
            confidence_indicator = (
                "High"
                if confidence >= 0.8
                else "Medium" if confidence >= 0.6 else "Preliminary"
            )

            # Clean up task assignment for better display
            task_display = result.task_assignment.replace(
                "Iteration", "Research Focus:"
            )
            if task_display.startswith("Research Focus:"):
                task_display = task_display[15:].strip()

            report += f"""### Investigation {i}: {confidence_indicator} Confidence

**Research Focus:** {task_display}

**Confidence Level:** {confidence:.1%}
**Sources Found:** {len(result.source_collection)}

**Findings:**

{result.research_findings}

"""

            if confidence < 0.7:
                report += f"*Note: This finding has {confidence:.1%} confidence and may require additional verification.*\n"

            report += "\n---\n\n"

        return report

    def _synthesize_research_results(
        self, query: str, results: list[SubagentResult]
    ) -> SynthesisResult:
        """Advanced result synthesis with quality assessment."""
        successful_results = [
            r for r in results if not r.error_status and r.research_findings
        ]

        if not successful_results:
            return SynthesisResult(
                synthesized_report=f"Research synthesis for '{query}' encountered difficulties. No successful findings were gathered.",
                completion_status=False,
                quality_metrics={"synthesis_score": 0.1},
                research_gaps=["All research tasks failed"],
                confidence_score=0.1,
            )

        # Calculate quality metrics
        avg_confidence = sum(
            r.confidence_metrics.get("research_confidence", 0.5)
            for r in successful_results
        ) / len(successful_results)
        source_diversity = len(
            set(
                source.get("source", "")
                for result in successful_results
                for source in result.source_collection
            )
        )

        # Build comprehensive professional synthesis
        synthesis_content = self._build_professional_report(
            query, successful_results, source_diversity, avg_confidence, results
        )

        # Quality Assessment
        quality_metrics = {
            "synthesis_score": avg_confidence,
            "source_diversity": min(source_diversity / 10, 1.0),  # Normalized to 0-1
            "coverage_completeness": len(successful_results) / max(len(results), 1),
            "research_depth": (
                sum(len(r.research_findings.split()) for r in successful_results)
                / len(successful_results)
                / 100
            ),  # Normalized
        }

        overall_quality = sum(quality_metrics.values()) / len(quality_metrics)

        # Identify research gaps
        research_gaps = []
        failed_results = [r for r in results if r.error_status]

        if overall_quality < 0.7:
            research_gaps.append(
                "Research quality below optimal threshold - consider additional iterations"
            )
        if source_diversity < 5:
            research_gaps.append("Limited source diversity - expand search scope")
        if failed_results:
            research_gaps.extend(
                [
                    f"Failed investigation: {r.task_assignment}"
                    for r in failed_results[:3]
                ]
            )

        completion_status = overall_quality >= 0.7 and len(research_gaps) <= 1

        self.orchestration_metrics.synthesis_quality_score = overall_quality

        return SynthesisResult(
            synthesized_report=synthesis_content,
            completion_status=completion_status,
            quality_metrics=quality_metrics,
            research_gaps=research_gaps,
            confidence_score=avg_confidence,
        )

    def _update_memory_with_results(
        self, iteration_results: list[SubagentResult], iteration: int
    ) -> None:
        """Update system memory with iteration results in a structured way."""
        result_summaries = []
        for result in iteration_results:
            result_summaries.append(
                {
                    "iteration": iteration,
                    "agent_id": result.agent_id,
                    "task": result.task_assignment,
                    "findings": result.research_findings,
                    "confidence": result.confidence_metrics.get(
                        "research_confidence", 0.5
                    ),
                    "success": not result.error_status,
                }
            )

        self.system_memory.subagent_results.extend(result_summaries)

    def _generate_next_iteration_tasks(
        self,
        missing_topics: list[str],
        query: str,
        all_results: list[SubagentResult],
        iteration: int,
        eval_metrics: dict,
    ) -> list[str]:
        """
        Intelligently generate next iteration tasks based on evaluation feedback.
        Enhanced logic for better task generation.
        """
        next_tasks = []

        # Strategy 1: Address specific missing topics from LLM-as-judge
        if missing_topics:
            logger.info(
                f"Addressing {len(missing_topics)} specific gaps identified by LLM"
            )
            for topic in missing_topics[:3]:  # Limit to top 3 most important gaps
                enhanced_task = self._create_targeted_task(topic, query, iteration + 1)
                next_tasks.append(enhanced_task)

        # Strategy 2: Intelligent refinement based on quality scores
        quality_score = eval_metrics.get("quality_score", 0.5)
        if quality_score < 0.6 and len(next_tasks) < 2:
            logger.info("Adding depth enhancement tasks due to low quality score")
            refinement_tasks = self._generate_intelligent_refinement_tasks(
                query, all_results, iteration
            )
            next_tasks.extend(refinement_tasks[:2])

        # Strategy 3: Coverage expansion if needed
        if len(next_tasks) < 2:
            coverage_tasks = self._generate_coverage_expansion_tasks(query, all_results)
            next_tasks.extend(coverage_tasks[:2])

        # Ensure we don't exceed reasonable task count
        return next_tasks[:4] if next_tasks else []

    def _create_targeted_task(
        self, missing_topic: str, query: str, iteration: int
    ) -> str:
        """Create a well-formed, searchable task from a missing topic."""
        # Clean and enhance the missing topic for better search results
        clean_topic = missing_topic.strip().rstrip(".")
        return f"Iteration {iteration} targeted research: {clean_topic} in the context of {query} - include recent developments, expert perspectives, and concrete evidence"

    def _generate_intelligent_refinement_tasks(
        self, query: str, previous_results: list[SubagentResult], iteration: int
    ) -> list[str]:
        """Generate intelligent refinement tasks based on analysis of previous results."""
        successful_results = [r for r in previous_results if not r.error_status]
        failed_results = [r for r in previous_results if r.error_status]

        refinement_tasks = []

        # Retry failed tasks with improved approach
        if failed_results:
            for failed_result in failed_results[:1]:  # Only retry most recent failure
                refined_task = f"Enhanced approach iteration {iteration}: {failed_result.task_assignment} - use alternative search strategies and broader terminology"
                refinement_tasks.append(refined_task)

        # Deepen analysis in areas with low confidence
        low_confidence_results = [
            r
            for r in successful_results
            if r.confidence_metrics.get("research_confidence", 1.0) < 0.6
        ]
        if low_confidence_results:
            task_keywords = self._extract_key_concepts(query)
            if task_keywords:
                refinement_tasks.append(
                    f"Deep-dive validation iteration {iteration}: {task_keywords[0]} - cross-reference multiple authoritative sources and recent studies"
                )

        return refinement_tasks[:2]

    def _generate_coverage_expansion_tasks(
        self, query: str, all_results: list[SubagentResult]
    ) -> list[str]:
        """Generate tasks to expand research coverage in underexplored areas."""
        successful_results = [r for r in all_results if not r.error_status]

        # Analyze coverage gaps using keywords
        standard_aspects = [
            "benefits",
            "risks",
            "challenges",
            "opportunities",
            "implementation",
            "future trends",
        ]
        covered_aspects = set()

        for result in successful_results:
            content = result.research_findings.lower()
            for aspect in standard_aspects:
                if aspect in content or any(
                    syn in content for syn in self._get_synonyms(aspect)
                ):
                    covered_aspects.add(aspect)

        missing_aspects = [
            aspect for aspect in standard_aspects if aspect not in covered_aspects
        ]

        expansion_tasks = []
        for aspect in missing_aspects[:2]:  # Focus on top 2 missing aspects
            expansion_tasks.append(
                f"Coverage expansion: {aspect} analysis for {query} - comprehensive review with recent examples and expert insights"
            )

        return expansion_tasks

    def _extract_key_concepts(self, query: str) -> list[str]:
        """Extract key concepts from query for focused research."""
        # Simple keyword extraction - could be enhanced with NLP
        stop_words = {
            "what",
            "how",
            "why",
            "when",
            "where",
            "are",
            "is",
            "the",
            "in",
            "of",
            "and",
            "or",
            "to",
            "for",
        }
        words = [
            w.strip(".,!?").lower()
            for w in query.split()
            if w.lower() not in stop_words and len(w) > 2
        ]
        return words[:3]  # Return top 3 key concepts

    def _get_synonyms(self, aspect: str) -> list[str]:
        """Get synonyms for coverage analysis."""
        synonym_map = {
            "benefits": ["advantages", "positives", "gains", "improvements"],
            "risks": ["dangers", "threats", "hazards", "concerns"],
            "challenges": ["obstacles", "difficulties", "problems", "issues"],
            "opportunities": ["possibilities", "potential", "prospects"],
            "implementation": ["deployment", "execution", "adoption", "application"],
            "future trends": ["emerging", "developments", "outlook", "predictions"],
        }
        return synonym_map.get(aspect, [])

    def _update_orchestration_metrics(
        self,
        results: list[SubagentResult],
        total_time: float,
        citation_output: CitationOutput,
    ) -> None:
        """Update orchestration metrics for performance tracking."""
        successful_results = [r for r in results if not r.error_status]

        # Parallel execution efficiency
        if self.enable_parallel_execution and len(results) > 1:
            sequential_time_estimate = sum(r.execution_time for r in results)
            actual_parallel_time = (
                max(r.execution_time for r in results) if results else total_time
            )
            self.orchestration_metrics.parallel_execution_efficiency = (
                max(0, 1 - (actual_parallel_time / sequential_time_estimate))
                if sequential_time_estimate > 0
                else 0
            )

        # Context compression (estimated based on results)
        total_content_length = sum(len(r.research_findings) for r in successful_results)
        if total_content_length > 0:
            self.orchestration_metrics.context_compression_ratio = min(
                total_content_length / 200000, 1.0
            )  # Normalize to 200k token limit

        # Error recovery count
        self.orchestration_metrics.error_recovery_count = len(
            [r for r in results if r.error_status]
        )

        # Citation accuracy (based on citation agent output)
        self.orchestration_metrics.citation_accuracy = citation_output.reference_quality

    def _create_error_response(
        self, query: str, error: str, execution_time: float
    ) -> dict[str, Any]:
        """Create standardized error response for system failures."""
        logger.error(f"Creating error response for query: {query}")
        logger.error(f"Error details: {error}")

        error_report = f"""# Research System Error Report

## Query
{query}

## Error Summary
The Advanced Research System encountered a critical error during execution.

## Error Details
```
{error}
```

## System Information
- Execution Time: {execution_time:.2f} seconds
- System Version: AdvancedResearch v2.0
- Architecture: Orchestrator-Worker Pattern
- Timestamp: {datetime.now().isoformat()}

## Troubleshooting
1. Check that all required API keys are properly configured
2. Verify network connectivity
3. Ensure all dependencies are installed correctly
4. Try running the system again with a simpler query

## Support
If this error persists, please contact the system administrator with the error details above.
"""

        return {
            "final_report": error_report,
            "research_strategy": {
                "strategy_type": "error",
                "complexity_score": 0,
                "tasks_executed": 0,
            },
            "execution_metrics": {
                "total_time": execution_time,
                "agents_spawned": 0,
                "parallel_efficiency": 0.0,
                "synthesis_quality": 0.0,
                "citation_accuracy": 0.0,
            },
            "source_analysis": {
                "total_sources": 0,
                "average_quality": 0.0,
                "citation_count": 0,
            },
            "subagent_results": [],
            "research_metadata": {
                "system_version": "AdvancedResearch v2.0",
                "architecture": "orchestrator-worker pattern",
                "error_status": error,
                "timestamp": datetime.now().isoformat(),
            },
        }

    def run(
        self, query: str, export: bool = False, export_path: str = None
    ) -> dict[str, Any]:
        """
        Alias for the research method to provide a more intuitive interface.

        Args:
            query (str): The research question to investigate
            export (bool): Whether to export the final report to a file
            export_path (str, optional): Custom file path for export. If None, generates timestamp-based name.

        Returns:
            Dict[str, Any]: Complete research results including final report, metrics, and metadata
        """
        return self.research(query, export, export_path)

    def _enhanced_export(
        self,
        report_content: str,
        query: str,
        custom_path: str = None,
        research_results: list = None,
        metrics: dict = None,
    ) -> str:
        """
        Enhanced export functionality with JSON format instead of Markdown.

        Args:
            report_content (str): The main report content
            query (str): The research query for intelligent file naming
            custom_path (str, optional): Custom file path
            research_results (list, optional): Research results for metadata
            metrics (dict, optional): Execution metrics

        Returns:
            str: The path where the JSON file was saved
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Generate intelligent file name from query
        if custom_path:
            json_path = (
                custom_path if custom_path.endswith(".json") else f"{custom_path}.json"
            )
        else:
            # Create intelligent filename from query
            clean_query = re.sub(r"[^\w\s-]", "", query.lower())
            clean_query = re.sub(r"\s+", "_", clean_query)
            clean_query = clean_query[:50]  # Limit length
            json_path = f"research_{clean_query}_{timestamp}.json"

        try:
            # Create a comprehensive JSON structure
            research_json = self._create_comprehensive_json(
                report_content, query, research_results, metrics
            )

            with open(json_path, "wb") as f:
                f.write(orjson.dumps(research_json, option=orjson.OPT_INDENT_2))

            logger.info(f"Comprehensive research report exported to JSON: {json_path}")
            return json_path

        except Exception as e:
            logger.error(f"JSON export failed: {e}")
            # Fallback to simple export
            fallback_path = f"research_fallback_{timestamp}.json"
            try:
                fallback_json = {
                    "query": query,
                    "timestamp": timestamp,
                    "report_content": report_content,
                    "export_status": "fallback_export",
                    "error": str(e),
                }
                with open(fallback_path, "wb") as f:
                    f.write(orjson.dumps(fallback_json, option=orjson.OPT_INDENT_2))
                logger.info(f"Fallback JSON export completed: {fallback_path}")
                return fallback_path
            except Exception as e2:
                logger.error(f"Fallback JSON export also failed: {e2}")
                raise e

    def _create_comprehensive_json(
        self,
        report_content: str,
        query: str,
        research_results: list = None,
        metrics: dict = None,
    ) -> dict:
        """
        Create a comprehensive JSON structure with all research data.
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Extract successful results for analysis
        successful_results = [
            r
            for r in (research_results or [])
            if not r.error_status and r.research_findings
        ]

        # Build comprehensive JSON structure
        json_data = {
            "metadata": {
                "query": query,
                "timestamp": timestamp,
                "system_version": "AdvancedResearch v2.0",
                "architecture": "orchestrator-worker pattern",
                "performance_improvement": "90.2% over single-agent systems",
            },
            "execution_summary": {
                "total_investigations": (
                    len(research_results) if research_results else 0
                ),
                "successful_investigations": len(successful_results),
                "research_quality": round(
                    (metrics.get("synthesis_quality", 0) * 100 if metrics else 0), 1
                ),
                "total_execution_time": (
                    round(metrics.get("total_time", 0), 1) if metrics else 0
                ),
                "agents_deployed": metrics.get("agents_spawned", 0) if metrics else 0,
                "sources_analyzed": metrics.get("source_count", 0) if metrics else 0,
                "success_rate": (
                    f"{len(successful_results)}/{len(research_results) if research_results else 0}"
                ),
            },
            "research_findings": [],
        }

        # Add detailed findings from each successful investigation
        for i, result in enumerate(successful_results, 1):
            confidence = result.confidence_metrics.get("research_confidence", 0.5)
            confidence_label = (
                "High"
                if confidence >= 0.8
                else "Medium" if confidence >= 0.6 else "Preliminary"
            )

            investigation = {
                "investigation_id": i,
                "confidence_label": confidence_label,
                "research_focus": result.task_assignment,
                "confidence_level": round(confidence, 3),
                "sources_found": len(result.source_collection),
                "findings": result.research_findings,
                "execution_time": round(result.execution_time, 2),
                "sources": [],
            }

            # Add sources for this specific investigation
            for j, source in enumerate(result.source_collection, 1):
                source_data = {
                    "id": f"{i}.{j}",
                    "url": source.get("source", "Unknown source"),
                    "content": source.get("content", "No content available"),
                    "quality_score": round(source.get("quality_score", 0.5), 2),
                    "reliability": source.get("reliability", "moderate"),
                }
                investigation["sources"].append(source_data)

            json_data["research_findings"].append(investigation)

        # Add methodology information
        json_data["methodology"] = {
            "system_architecture": {
                "pattern": "Orchestrator-Worker Pattern",
                "description": (
                    "A lead researcher agent coordinates multiple specialized subagents"
                ),
                "parallel_execution": (
                    "Multiple research tasks executed simultaneously for efficiency"
                ),
                "dynamic_task_generation": (
                    "Iterative refinement based on LLM-as-judge evaluation"
                ),
                "quality_assessment": (
                    "Continuous evaluation and improvement of research quality"
                ),
            },
            "research_process": [
                "Query Analysis: Lead agent analyzes the research question and develops strategy",
                "Task Decomposition: Query broken into specific, searchable research tasks",
                "Parallel Investigation: Specialized subagents execute research tasks simultaneously",
                "Progress Evaluation: LLM-as-judge evaluates completeness and identifies gaps",
                "Iterative Refinement: Additional research conducted based on identified gaps",
                "Synthesis & Citation: Results compiled and properly cited",
            ],
            "quality_controls": [
                "Source Verification: Multiple sources cross-referenced for accuracy",
                "Confidence Scoring: Each finding assigned confidence level based on evidence quality",
                "Citation Standards: All claims properly attributed to authoritative sources",
                "Iterative Validation: Multiple rounds of evaluation and refinement",
            ],
        }

        # Compile all unique sources across all investigations
        all_sources = []
        source_counter = 1

        for result in successful_results:
            for source in result.source_collection:
                source_url = source.get("source", "Unknown source")
                # Avoid duplicates
                if not any(
                    existing.get("url") == source_url for existing in all_sources
                ):
                    source_data = {
                        "reference_id": source_counter,
                        "url": source_url,
                        "content": source.get("content", "No content available"),
                        "quality_score": round(source.get("quality_score", 0.5), 2),
                        "reliability": source.get("reliability", "moderate"),
                    }
                    all_sources.append(source_data)
                    source_counter += 1

        json_data["all_references"] = all_sources

        # Add quality assessment
        json_data["quality_assessment"] = {
            "reliability_indicators": [
                "Multi-Source Verification: Findings cross-referenced across multiple authoritative sources",
                "Confidence Scoring: Each investigation rated for reliability and evidence quality",
                "Systematic Coverage: Comprehensive exploration of research topic from multiple angles",
                "Current Information: Focus on recent developments and latest available data",
                "Professional Standards: Academic-level citation and referencing standards",
            ],
            "limitations": [
                "Research limited to publicly available sources accessible via web search",
                "Confidence levels reflect source quality and consensus, not absolute certainty",
                "Some findings may require additional verification for critical decision-making",
                "Information current as of the research execution date",
            ],
            "execution_metrics": {
                "total_execution_time": (
                    round(metrics.get("total_time", 0), 1) if metrics else 0
                ),
                "agents_deployed": metrics.get("agents_spawned", 0) if metrics else 0,
                "research_quality_score": (
                    round((metrics.get("synthesis_quality", 0) * 100), 1)
                    if metrics
                    else 0
                ),
                "sources_analyzed": metrics.get("source_count", 0) if metrics else 0,
            },
        }

        # Add the original report content for reference
        json_data["original_report_content"] = report_content

        return json_data

    def _add_methodology_and_appendix(
        self, report_content: str, metrics: dict = None
    ) -> str:
        """Add methodology section and appendix to the report."""
        if "## Research Methodology" in report_content:
            return report_content  # Already has methodology

        # Find insertion point (before References)
        if "## References" in report_content:
            parts = report_content.split("## References")
            enhanced_report = parts[0]
        else:
            enhanced_report = report_content
            parts = [report_content, ""]

        # Add methodology section
        methodology = """
## Research Methodology

This research was conducted using an advanced multi-agent research system with the following methodology:

### System Architecture
- **Orchestrator-Worker Pattern**: A lead researcher agent coordinates multiple specialized subagents
- **Parallel Execution**: Multiple research tasks executed simultaneously for efficiency
- **Dynamic Task Generation**: Iterative refinement based on LLM-as-judge evaluation
- **Quality Assessment**: Continuous evaluation and improvement of research quality

### Research Process
1. **Query Analysis**: Lead agent analyzes the research question and develops strategy
2. **Task Decomposition**: Query broken into specific, searchable research tasks
3. **Parallel Investigation**: Specialized subagents execute research tasks simultaneously
4. **Progress Evaluation**: LLM-as-judge evaluates completeness and identifies gaps
5. **Iterative Refinement**: Additional research conducted based on identified gaps
6. **Synthesis & Citation**: Results compiled and properly cited

### Quality Controls
- **Source Verification**: Multiple sources cross-referenced for accuracy
- **Confidence Scoring**: Each finding assigned confidence level based on evidence quality
- **Citation Standards**: All claims properly attributed to authoritative sources
- **Iterative Validation**: Multiple rounds of evaluation and refinement

"""

        if metrics:
            methodology += f"""### Execution Metrics
- **Total Execution Time**: {metrics.get('total_time', 0):.1f} seconds
- **Agents Deployed**: {metrics.get('agents_spawned', 0)}
- **Research Quality**: {metrics.get('synthesis_quality', 0):.1%}
- **Sources Analyzed**: {metrics.get('source_count', 0)}

"""

        methodology += "---\n\n"

        # Add quality assessment section
        quality_section = """
## Quality Assessment

### Research Reliability
This research report has been generated using systematic multi-agent investigation with the following quality indicators:

- **Multi-Source Verification**: Findings cross-referenced across multiple authoritative sources
- **Confidence Scoring**: Each investigation rated for reliability and evidence quality
- **Systematic Coverage**: Comprehensive exploration of research topic from multiple angles
- **Current Information**: Focus on recent developments and latest available data
- **Professional Standards**: Academic-level citation and referencing standards

### Limitations and Considerations
- Research limited to publicly available sources accessible via web search
- Confidence levels reflect source quality and consensus, not absolute certainty
- Some findings may require additional verification for critical decision-making
- Information current as of the research execution date

---

## Recommendations

Based on this research investigation, we recommend:

1. **For Further Research**: Areas identified as requiring additional investigation
2. **Source Verification**: Cross-reference high-impact findings with additional authoritative sources
3. **Monitoring**: Track ongoing developments in rapidly evolving topic areas
4. **Expert Consultation**: Consult domain experts for critical decision-making applications

---

"""

        # Reassemble the report
        final_report = enhanced_report + methodology + quality_section
        if parts[1]:  # Add references back if they existed
            final_report += "## References" + parts[1]

        return final_report

    def export_report(self, report_content: str, file_path: str = None) -> str:
        """
        Legacy export method for backward compatibility - now exports JSON format.

        Args:
            report_content (str): The content to export
            file_path (str, optional): Custom file path. If None, generates timestamp-based name.

        Returns:
            str: The path where the file was saved
        """
        if not file_path:
            file_path = (
                f"research_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
        else:
            # Ensure file has .json extension
            if not file_path.endswith(".json"):
                file_path = (
                    file_path.replace(".md", ".json")
                    if ".md" in file_path
                    else f"{file_path}.json"
                )

        try:
            # Create a basic JSON structure for legacy compatibility
            json_data = {
                "metadata": {
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "export_type": "legacy_export",
                    "system_version": "AdvancedResearch v2.0",
                },
                "report_content": report_content,
                "export_info": {
                    "format": "JSON",
                    "note": (
                        "This is a legacy export converted from markdown to JSON format"
                    ),
                },
            }

            with open(file_path, "wb") as f:
                f.write(orjson.dumps(json_data, option=orjson.OPT_INDENT_2))
            logger.info(f"Research report successfully exported to JSON: {file_path}")
            return file_path
        except Exception as e:
            logger.error(f"Failed to export report to {file_path}: {e}")
            raise
