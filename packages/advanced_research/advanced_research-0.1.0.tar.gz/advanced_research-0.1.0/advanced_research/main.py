import json
import os
import re
import sys
import time
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any

import requests
from dotenv import load_dotenv
from loguru import logger
from pydantic import BaseModel, Field, ValidationError

# Swarms framework for multi-agent coordination
from swarms import Agent, Conversation

# Load environment variables from .env file
load_dotenv()


# --- Pydantic Models for Structured Communication ---
class SourceInfo(BaseModel):
    """Enhanced source information model with quality metrics."""

    source: str = Field(..., description="URL or source identifier")
    content: str = Field(..., description="Brief summary or quote from the source")
    quality_score: float = Field(
        default=0.5, description="Source quality assessment (0-1)"
    )
    reliability: str = Field(
        default="moderate", description="Source reliability rating"
    )


class SubagentFindings(BaseModel):
    """Structured model for subagent research findings."""

    findings: str = Field(..., description="Comprehensive research summary")
    sources: list[SourceInfo] = Field(
        default_factory=list, description="List of sources with quality metrics"
    )
    confidence_level: float = Field(
        default=0.5, description="Subagent confidence in findings (0-1)"
    )
    coverage_assessment: str = Field(
        default="partial", description="Assessment of topic coverage"
    )


class ResearchStrategy(BaseModel):
    """Research strategy and planning model."""

    strategy_type: str = Field(
        ..., description="Strategy type: focused, breadth_first, or iterative_depth"
    )
    complexity_score: int = Field(..., description="Query complexity assessment (1-10)")
    subtasks: list[str] = Field(..., description="Decomposed research subtasks")
    priority_matrix: list[int] = Field(
        default_factory=list, description="Task priority scores"
    )
    estimated_duration: float = Field(
        default=60.0, description="Estimated completion time in seconds"
    )


class SynthesisResult(BaseModel):
    """Advanced synthesis results with quality metrics."""

    synthesized_report: str = Field(..., description="Comprehensive synthesized report")
    completion_status: bool = Field(
        ..., description="Whether research objectives are met"
    )
    quality_metrics: dict[str, float] = Field(
        default_factory=dict, description="Quality assessment scores"
    )
    research_gaps: list[str] = Field(
        default_factory=list,
        description="Identified gaps requiring additional research",
    )
    confidence_score: float = Field(
        default=0.5, description="Overall confidence in synthesis"
    )


class CitationOutput(BaseModel):
    """Citation agent output model."""

    cited_report: str = Field(..., description="Final report with proper citations")
    reference_quality: float = Field(
        default=0.5, description="Overall reference quality score"
    )
    citation_count: int = Field(default=0, description="Number of citations added")


# --- Enhanced Tool Definition (Exa Search Integration) ---
def exa_search(query: str, num_results: int = 5, **kwargs: Any) -> str:
    """
    Advanced web search using Exa.ai API with corrected request format.
    Designed for parallel execution by multiple subagents with enhanced search capabilities.

    Args:
        query (str): The search query with context awareness
        num_results (int): Number of results to return (optimized for agent processing)

    Returns:
        str: Formatted search results with quality indicators
    """
    api_key = os.getenv("EXA_API_KEY")
    if not api_key:
        logger.warning("EXA_API_KEY not found - using mock search results")
        return f"Mock search results for: {query}\n1. Example Source: https://example.com\n   Content: Mock research data for {query[:50]}..."

    headers = {
        "x-api-key": api_key,
        "Content-Type": "application/json",
        "Accept": "application/json",
    }

    # Corrected payload format for Exa API
    payload = {
        "query": query,
        "useAutoprompt": True,
        "numResults": min(num_results, 8),
        "contents": {"text": {"maxCharacters": 1000, "includeHtmlTags": False}},
        "includeDomains": [],
        "excludeDomains": [],
        "startCrawlDate": None,
        "endCrawlDate": None,
        "startPublishedDate": None,
        "endPublishedDate": None,
    }

    try:
        logger.info(f"🔍 Executing Exa search for: {query[:50]}...")
        response = requests.post(
            "https://api.exa.ai/search", json=payload, headers=headers, timeout=30
        )

        # Log response status for debugging
        logger.debug(f"Exa API response status: {response.status_code}")

        if response.status_code == 400:
            error_detail = response.text
            logger.error(f"Exa API 400 error: {error_detail}")
            return f"Search API Error (400): {error_detail[:200]}... Please check query format."

        response.raise_for_status()
        json_data = response.json()

        if "error" in json_data:
            logger.error(f"Exa API returned error: {json_data['error']}")
            return f"Search Error: {json_data['error']}"

        results = json_data.get("results", [])
        if not results:
            logger.warning(f"No results found for query: {query}")
            return f"No results found for query: {query}. Try rephrasing or using different keywords."

        formatted_results = []
        for i, result in enumerate(results, 1):
            title = result.get("title", "No Title")
            url = result.get("url", "No URL")
            content_preview = result.get("text", "No content preview available.")
            score = result.get("score", 0.0)

            # Enhanced quality indicators
            domain_quality = (
                "🔥"
                if any(
                    domain in url.lower()
                    for domain in [".edu", ".gov", ".org", "pubmed", "scholar"]
                )
                else "📊" if ".com" in url.lower() else "📄"
            )
            relevance = "⭐⭐⭐" if score > 0.8 else "⭐⭐" if score > 0.6 else "⭐"

            formatted_results.append(
                f"{i}. {domain_quality} {relevance} {title}\n"
                f"   URL: {url}\n"
                f"   Relevance Score: {score:.2f}\n"
                f"   Content: {content_preview[:300]}...\n"
            )

        logger.info(f"✅ Exa search completed: {len(results)} results found")
        return "\n".join(formatted_results)

    except requests.Timeout:
        logger.error("Exa search timeout")
        return f"Search timeout for query: {query}. Please try again."
    except requests.RequestException as e:
        logger.error(f"Exa search request failed: {e}")
        return f"Search request failed: {str(e)}. Check network connection and API key."
    except Exception as e:
        logger.error(f"Unexpected error in exa_search: {e}")
        return f"Unexpected search error: {str(e)}. Please try a different query."


# --- Enhanced Data Structures ---
@dataclass
class AgentMemory:
    """Advanced memory system for agent coordination and state management."""

    research_context: str
    strategy_plan: ResearchStrategy | None = None
    subagent_results: list[dict[str, Any]] = field(default_factory=list)
    synthesis_history: list[str] = field(default_factory=list)
    error_log: list[str] = field(default_factory=list)
    performance_metrics: dict[str, float] = field(default_factory=dict)
    conversation_state: Conversation | None = None
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class SubagentResult:
    """Enhanced result structure for subagent coordination."""

    agent_id: str
    task_assignment: str
    research_findings: str
    source_collection: list[dict[str, Any]] = field(default_factory=list)
    execution_time: float = 0.0
    error_status: str | None = None
    confidence_metrics: dict[str, float] = field(default_factory=dict)
    parallel_tool_usage: list[str] = field(default_factory=list)
    completion_timestamp: str = field(
        default_factory=lambda: datetime.now().isoformat()
    )


@dataclass
class OrchestrationMetrics:
    """Metrics for orchestrator performance tracking."""

    total_agents_spawned: int = 0
    parallel_execution_efficiency: float = 0.0
    context_compression_ratio: float = 0.0
    error_recovery_count: int = 0
    synthesis_quality_score: float = 0.0
    citation_accuracy: float = 0.0


# --- Core Agent Classes Following Paper Architecture ---


class LeadResearcherAgent:
    """
    Lead Researcher (Orchestrator) - Handles planning, strategy, and coordination.
    Implements the orchestrator pattern from the paper with enhanced capabilities.
    """

    def __init__(
        self,
        model_name: str = "claude-3-7-sonnet-20250219",
        base_path: str = "agent_states",
    ):
        self.model_name = model_name
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)
        self.agent_memory = None

        # Initialize lead researcher agent with enhanced prompting
        self.lead_agent = Agent(
            agent_name="Lead-Researcher-Orchestrator",
            system_prompt=self._get_orchestrator_prompt(),
            model_name=model_name,
            max_loops=1,
            autosave=True,
            saved_state_path=str(self.base_path / "lead_researcher.json"),
            verbose=False,
            retry_attempts=2,
        )

        logger.info(
            "LeadResearcherAgent initialized with enhanced orchestration capabilities"
        )

    def _get_orchestrator_prompt(self) -> str:
        """Advanced prompt for the lead researcher orchestrator with explicit thinking process."""
        return """You are the Lead Researcher Agent in an advanced multi-agent research system with sophisticated web search capabilities.

Your primary responsibilities as the ORCHESTRATOR:
1. 🎯 Query Analysis & Strategic Research Planning
2. 🔄 Intelligent Task Decomposition into Searchable Sub-Questions  
3. 🧠 Advanced Memory Management & Context Compression
4. 📊 Result Synthesis & Quality Assurance

ENHANCED ORCHESTRATION PRINCIPLES:
- Decompose complex queries into specific, searchable research questions
- Create targeted sub-tasks that leverage web search effectively
- Think in terms of "What specific questions need web research to answer?"
- Generate tasks that include domain-specific terminology and concepts
- Consider multiple perspectives: technical, ethical, practical, regulatory
- Scale research depth to query complexity and available information

TASK DECOMPOSITION STRATEGY:
- Break broad topics into specific, answerable questions
- Include comparative analysis tasks (benefits vs risks, pros vs cons)
- Add contextual research (current state, recent developments, future trends)
- Generate domain-specific investigative tasks
- Create verification and cross-validation research tasks

EXAMPLES OF GOOD TASK DECOMPOSITION:
Instead of: "Research AI in healthcare"
Create: 
- "Current applications of AI diagnostic tools in radiology and pathology 2023-2024"
- "Clinical trial results and efficacy data for AI-powered medical devices"
- "Regulatory frameworks and FDA approvals for AI medical technologies"
- "Patient safety incidents and risk mitigation strategies in AI healthcare"

CRITICAL RESPONSE FORMAT - MANDATORY STRUCTURE:
Your response MUST start with a <thinking> block that details your reasoning process, followed by the final JSON object.

EXPLICIT THINKING REQUIREMENTS:
- Start every response with <thinking> and end with </thinking>
- Show your step-by-step analysis of the query
- Explain your reasoning for complexity assessment
- Justify your choice of strategy type
- Detail why you chose specific subtasks
- Make your decision-making process completely transparent

REQUIRED EXAMPLE STRUCTURE:
<thinking>
Let me analyze this query step by step:
1. Query Analysis: The user is asking about [specific aspects]. This involves [complexity factors].
2. Complexity Assessment: This rates as [X]/10 because [detailed reasoning with specific factors].
3. Strategy Selection: I'll use [strategy] because [specific reasons why this approach is optimal].
4. Task Decomposition: I need to break this into [N] subtasks because [reasoning].
   - Task 1: [specific task] - focuses on [aspect] because [reasoning]
   - Task 2: [specific task] - addresses [aspect] because [reasoning]
   - etc.
5. Priority Matrix: Task priorities are [order] because [reasoning for prioritization].
6. Duration Estimate: Based on complexity and task count, I estimate [time] because [reasoning].
</thinking>

{
    "strategy_type": "focused|breadth_first|iterative_depth",
    "complexity_score": 1-10,
    "subtasks": ["specific searchable question 1", "targeted research query 2", "domain-specific investigation 3"],
    "priority_matrix": [3, 2, 1],
    "estimated_duration": 60.0
}

ABSOLUTE REQUIREMENTS:
- NEVER skip the <thinking> block - it is mandatory for every response
- Make your thinking detailed and explicit (minimum 5 sentences)
- Follow thinking with ONLY valid JSON (no other text)
- Ensure thinking block is properly closed with </thinking>"""

    def analyze_and_plan(self, query: str) -> ResearchStrategy:
        """
        Core orchestration function: analyze query and develop research strategy.
        Implements the paper's "Query Analysis → Strategy Development" workflow.
        """
        logger.info("🎯 Lead Researcher analyzing query and developing strategy...")

        analysis_prompt = f"""
        Analyze this research query and develop an optimal strategy:
        
        QUERY: "{query}"
        
        Consider:
        - Query complexity and scope
        - Required research depth vs breadth
        - Optimal subagent coordination strategy
        - Resource allocation needs
        
        Develop a comprehensive research strategy following the orchestrator principles.
        """

        try:
            response = self.lead_agent.run(analysis_prompt)
            thinking_content, strategy_data, error = self._parse_json_response(response)

            # Log the extracted thinking process for observability
            if thinking_content:
                logger.info("🧠 Lead Researcher Thinking Process:")
                logger.info(
                    f"   {thinking_content[:200]}..."
                    if len(thinking_content) > 200
                    else f"   {thinking_content}"
                )

            if error or not strategy_data:
                logger.warning(
                    f"Strategy parsing failed: {error}, using fallback strategy"
                )
                return self._create_fallback_strategy(query)

            strategy = ResearchStrategy.model_validate(strategy_data)
            logger.info(
                f"📋 Research Strategy: {strategy.strategy_type}, Complexity: {strategy.complexity_score}/10"
            )

            # Store thinking process in memory for future reference
            if hasattr(self, "agent_memory") and self.agent_memory and thinking_content:
                if not hasattr(self.agent_memory, "orchestrator_thinking"):
                    self.agent_memory.orchestrator_thinking = []
                self.agent_memory.orchestrator_thinking.append(
                    {
                        "query": query,
                        "thinking": thinking_content,
                        "strategy": strategy.strategy_type,
                        "timestamp": datetime.now().isoformat(),
                    }
                )

            return strategy

        except Exception as e:
            logger.error(f"Strategy development failed: {e}")
            return self._create_fallback_strategy(query)

    def _evaluate_progress(
        self, current_report: str, original_query: str
    ) -> tuple[bool, list[str], str]:
        """
        Enhanced evaluation of research progress using advanced LLM-as-judge methodology.
        Returns (is_complete, missing_topics, reasoning).
        """
        logger.info(
            "⚖️ Evaluating research progress with enhanced LLM-as-judge methodology..."
        )

        evaluation_prompt = f"""
You are an expert evaluation agent acting as an LLM-as-judge. Your role is to rigorously assess whether the current research report adequately answers the original query.

ORIGINAL QUERY: "{original_query}"

CURRENT RESEARCH REPORT:
"{current_report}"

EVALUATION METHODOLOGY:
As an LLM-as-judge, you must provide detailed analysis in a thinking block, then render a precise verdict.

EVALUATION CRITERIA (Rate each 1-10):
1. Completeness: Does the report address all aspects of the original query?
2. Depth: Is the coverage sufficiently detailed and comprehensive?
3. Evidence Quality: Are claims well-supported with credible sources?
4. Coverage Balance: Are all query components given appropriate attention?
5. Gaps Analysis: What specific information is missing or insufficient?

RESPONSE FORMAT - MANDATORY STRUCTURE:
<thinking>
Let me systematically evaluate this research against the original query:

1. Query Analysis: Breaking down what the original query is asking for:
   [Detailed breakdown of query components and requirements]

2. Report Coverage Assessment: What the current report covers:
   [Analysis of what topics/aspects are well covered]

3. Gap Identification: What's missing or insufficient:
   [Specific analysis of gaps, weaknesses, insufficient coverage]

4. Quality Assessment: Source quality and evidence strength:
   [Assessment of evidence quality and credibility]

5. Completeness Judgment: Overall adequacy:
   [Final judgment on whether research meets query requirements]

6. Decision Rationale: Why I'm marking this complete/incomplete:
   [Clear reasoning for the final decision]
</thinking>

{{
    "is_complete": boolean,
    "missing_topics": ["specific missing topic 1", "unanswered question 2", "insufficient coverage area 3"],
    "reasoning": "Concise explanation of the decision and key findings",
    "confidence_score": 0.0-1.0,
    "coverage_scores": {{
        "completeness": 0.0-1.0,
        "depth": 0.0-1.0,
        "evidence_quality": 0.0-1.0,
        "balance": 0.0-1.0
    }}
}}

JUDGMENT STANDARDS:
- Mark complete ONLY if the report would satisfy a knowledgeable reader seeking a comprehensive answer
- Be specific about missing topics - don't just say "more research needed"
- Consider both breadth (covering all aspects) and depth (sufficient detail)
- Confidence score should reflect certainty in your assessment
"""

        try:
            response = self.lead_agent.run(evaluation_prompt)
            thinking_content, eval_data, error = self._parse_json_response(response)

            # Enhanced logging of evaluation thinking process
            if thinking_content:
                logger.info("🧠 LLM-as-Judge Evaluation Thinking:")
                # Log the full thinking process with better formatting
                thinking_lines = thinking_content.split("\n")
                for line in thinking_lines[:15]:  # Log first 15 lines to avoid spam
                    if line.strip():
                        logger.info(f"   {line.strip()}")
                if len(thinking_lines) > 15:
                    logger.info(
                        f"   ... [Truncated: {len(thinking_lines)} total lines of analysis]"
                    )
            else:
                logger.warning("⚠️ No thinking process found in evaluation response")

            if error or not eval_data:
                logger.warning(f"Progress evaluation parsing failed: {error}")
                # Conservative fallback - assume not complete with generic gap
                return (
                    False,
                    ["Evaluation system error - continuing research for safety"],
                    "Evaluation parsing error",
                )

            # Extract evaluation results with enhanced handling
            is_complete = eval_data.get("is_complete", False)
            missing_topics = eval_data.get("missing_topics", [])
            reasoning = eval_data.get("reasoning", "No reasoning provided")
            confidence = eval_data.get("confidence_score", 0.5)
            coverage_scores = eval_data.get("coverage_scores", {})

            # Enhanced result logging
            logger.info("📊 Enhanced Progress Evaluation Results:")
            logger.info(f"   ✅ Research Complete: {is_complete}")
            logger.info(f"   🎯 Judge Confidence: {confidence:.2f}")
            logger.info(f"   📝 Decision Reasoning: {reasoning}")

            # Log coverage scores if available
            if coverage_scores:
                logger.info("   📈 Detailed Coverage Scores:")
                for metric, score in coverage_scores.items():
                    logger.info(f"     {metric.title()}: {score:.2f}")

            # Detailed missing topics logging
            if missing_topics:
                logger.info(f"   🔍 Missing Topics Identified ({len(missing_topics)}):")
                for i, topic in enumerate(missing_topics, 1):
                    logger.info(f"     {i}. {topic}")
                    if i >= 5:  # Limit logging to prevent spam
                        remaining = len(missing_topics) - 5
                        if remaining > 0:
                            logger.info(f"     ... and {remaining} more missing topics")
                        break
            else:
                logger.info("   ✅ No missing topics identified")

            # Store evaluation in memory for future reference
            if hasattr(self, "agent_memory") and self.agent_memory:
                if not hasattr(self.agent_memory, "evaluation_history"):
                    self.agent_memory.evaluation_history = []
                self.agent_memory.evaluation_history.append(
                    {
                        "timestamp": datetime.now().isoformat(),
                        "is_complete": is_complete,
                        "confidence": confidence,
                        "reasoning": reasoning,
                        "missing_topics_count": len(missing_topics),
                        "thinking_process": (
                            thinking_content[:500] if thinking_content else None
                        ),  # Store truncated thinking
                    }
                )

            return is_complete, missing_topics, reasoning

        except Exception as e:
            logger.error(f"Progress evaluation system error: {e}")
            logger.exception("Full evaluation error details:")
            # Conservative fallback - assume not complete
            return (
                False,
                ["Critical evaluation error - continuing research"],
                f"System error: {str(e)}",
            )

    def _create_fallback_strategy(self, query: str) -> ResearchStrategy:
        """Create a robust fallback strategy with enhanced search-oriented tasks."""
        complexity = self._assess_complexity_heuristic(query)

        # Generate specific, searchable tasks based on query characteristics
        base_tasks = []

        # Core research task with search optimization
        base_tasks.append(
            f"Current state and recent developments in {query} - latest research 2023-2024"
        )

        # Add targeted research angles based on query content
        if any(
            keyword in query.lower() for keyword in ["benefits", "advantages", "pros"]
        ):
            base_tasks.append(
                f"Evidence-based benefits and positive outcomes of {query} - case studies and research data"
            )

        if any(
            keyword in query.lower()
            for keyword in ["risks", "challenges", "drawbacks", "cons", "problems"]
        ):
            base_tasks.append(
                f"Risk assessment and challenges in {query} - regulatory concerns and mitigation strategies"
            )

        if any(keyword in query.lower() for keyword in ["ethical", "ethics", "moral"]):
            base_tasks.append(
                f"Ethical frameworks and guidelines for {query} - professional standards and regulatory compliance"
            )

        if any(
            keyword in query.lower()
            for keyword in [
                "AI",
                "artificial intelligence",
                "machine learning",
                "technology",
            ]
        ):
            base_tasks.append(
                f"Technical implementation and real-world applications of {query} - industry adoption and best practices"
            )

        if any(
            keyword in query.lower()
            for keyword in ["healthcare", "medical", "clinical", "patient"]
        ):
            base_tasks.append(
                f"Clinical evidence and healthcare outcomes for {query} - peer-reviewed studies and FDA regulations"
            )

        # Ensure we have at least 3 tasks but no more than 5
        if len(base_tasks) < 3:
            base_tasks.extend(
                [
                    f"Expert opinions and industry analysis on {query} - thought leadership and market research",
                    f"Comparative analysis and benchmarking of {query} - international perspectives and best practices",
                ]
            )

        tasks = base_tasks[:5]  # Limit to 5 tasks maximum

        return ResearchStrategy(
            strategy_type="breadth_first" if complexity >= 6 else "focused",
            complexity_score=complexity,
            subtasks=tasks,
            priority_matrix=list(range(len(tasks), 0, -1)),
            estimated_duration=complexity * 15.0,
        )

    def _assess_complexity_heuristic(self, query: str) -> int:
        """Heuristic complexity assessment for fallback scenarios."""
        indicators = {
            "multi_part": len(
                [w for w in ["and", "or", "vs", "compare"] if w in query.lower()]
            ),
            "scope": len(
                [
                    w
                    for w in ["global", "comprehensive", "detailed"]
                    if w in query.lower()
                ]
            ),
            "domain": len(
                [
                    w
                    for w in ["healthcare", "AI", "technology", "ethical"]
                    if w in query.lower()
                ]
            ),
        }

        base_score = min(len(query.split()) // 4, 3)
        indicator_score = min(sum(indicators.values()) * 2, 5)

        return max(1, min(10, base_score + indicator_score + 2))

    def _parse_json_response(
        self, response: str
    ) -> tuple[str | None, dict | None, str | None]:
        """Enhanced JSON parsing with robust thinking extraction and multiple fallback patterns."""
        if not response or not response.strip():
            return None, None, "Empty response"

        logger.debug(
            f"📝 Parsing orchestrator response (length: {len(response)} chars)"
        )

        # Extract thinking block with improved pattern matching
        thinking_patterns = [
            r"<thinking>(.*?)</thinking>",  # Standard thinking block
            r"<thinking>\s*(.*?)\s*</thinking>",  # With optional whitespace
            r"(?i)<thinking>(.*?)</thinking>",  # Case insensitive
        ]

        thinking_content = None
        thinking_match = None

        for pattern in thinking_patterns:
            thinking_match = re.search(pattern, response, re.DOTALL)
            if thinking_match:
                thinking_content = thinking_match.group(1).strip()
                break

        if thinking_content:
            logger.info("🧠 Successfully extracted thinking process from orchestrator")
            logger.debug(f"   Thinking length: {len(thinking_content)} chars")
            # Log a preview of the thinking content for debugging
            thinking_preview = thinking_content[:150].replace("\n", " ")
            logger.debug(f"   Thinking preview: {thinking_preview}...")
        else:
            logger.warning(
                "⚠️ No thinking block found in orchestrator response - this violates the prompt requirements"
            )

        # Enhanced JSON extraction patterns for robustness
        json_patterns = [
            r"```json\s*(\{.*?\})\s*```",  # json code blocks with whitespace
            r"```\s*(\{.*?\})\s*```",  # general code blocks
            r"(\{(?:[^{}]|{(?:[^{}]|{[^{}]*})*})*\})",  # nested JSON object matching
        ]

        # First try: extract JSON from patterns in the full response
        for i, pattern in enumerate(json_patterns):
            matches = re.findall(pattern, response, re.DOTALL)
            for match in matches:
                try:
                    json_data = json.loads(match)
                    logger.debug(f"✅ JSON extracted with pattern {i+1}")
                    return thinking_content, json_data, None
                except json.JSONDecodeError as e:
                    logger.debug(f"❌ Pattern {i+1} JSON decode failed: {e}")
                    continue

        # Second try: parse content after thinking block as JSON
        if thinking_match:
            remaining_content = response[thinking_match.end() :].strip()
            logger.debug(
                f"Attempting to parse post-thinking content (length: {len(remaining_content)})"
            )

            # Clean up the remaining content (remove any markdown artifacts)
            remaining_content = remaining_content.strip("`").strip()

            try:
                json_data = json.loads(remaining_content)
                logger.debug("✅ JSON extracted from content after thinking block")
                return thinking_content, json_data, None
            except json.JSONDecodeError as e:
                logger.debug(f"❌ Post-thinking JSON parse failed: {e}")

        # Third try: look for JSON anywhere in the response
        try:
            # Try to find any JSON-like structure in the response
            json_start = response.find("{")
            json_end = response.rfind("}") + 1

            if json_start != -1 and json_end > json_start:
                potential_json = response[json_start:json_end]
                json_data = json.loads(potential_json)
                logger.debug("✅ JSON extracted by position search")
                return thinking_content, json_data, None
        except (json.JSONDecodeError, ValueError) as e:
            logger.debug(f"❌ Position-based JSON extraction failed: {e}")

        # Final fallback: try entire response as JSON (unlikely to work but worth trying)
        try:
            json_data = json.loads(response.strip())
            logger.debug("✅ Entire response parsed as JSON (no thinking block found)")
            return thinking_content, json_data, None
        except json.JSONDecodeError as e:
            error_msg = f"All JSON parsing attempts failed. Last error: {str(e)}"
            logger.warning(f"❌ {error_msg}")
            logger.debug(f"Response preview: {response[:200]}...")
            return thinking_content, None, error_msg


class ResearchSubagent:
    """
    Specialized Subagent (Worker) - Independent search execution with parallel tool usage.
    Implements the worker pattern with advanced coordination capabilities.
    """

    def __init__(
        self,
        agent_id: str,
        model_name: str = "claude-3-7-sonnet-20250219",
        strategy_context: str = "breadth_first",
    ):
        self.agent_id = agent_id
        self.model_name = model_name
        self.strategy_context = strategy_context

        # Initialize specialized subagent with iterative search capabilities
        self.worker_agent = Agent(
            agent_name=f"Research-Subagent-{agent_id}",
            system_prompt=self._get_subagent_prompt(),
            model_name=model_name,
            max_loops=3,  # Enable iterative search-analyze-refine loop
            tools=[exa_search],  # Primary tool for web research
            verbose=False,
            retry_attempts=1,
        )

        logger.info(
            f"🤖 ResearchSubagent {agent_id} initialized with {strategy_context} strategy"
        )

    def _get_subagent_prompt(self) -> str:
        """Specialized prompt for research subagents with advanced web search capabilities."""
        strategy_guidance = {
            "focused": (
                "Focus deeply on specific aspects. Generate precise, technical search queries. Target expert sources and detailed analysis."
            ),
            "breadth_first": (
                "Start with broad context searches, then narrow to specifics. Use varied search terms and explore multiple perspectives."
            ),
            "iterative_depth": (
                "Conduct multi-layered research with follow-up searches. Connect findings across sources and explore deeper implications."
            ),
        }

        return f"""You are a Specialized Research Subagent with advanced web search and analysis capabilities.

STRATEGY CONTEXT: {strategy_guidance.get(self.strategy_context, "General research approach")}

ENHANCED SUBAGENT RESPONSIBILITIES:
1. 🔍 Advanced Search Query Generation & Execution
2. 📊 Multi-Source Research & Cross-Validation
3. 🎯 Critical Source Evaluation & Quality Assessment
4. ⚡ Iterative Search Refinement & Deep Dive Analysis

ADVANCED SEARCH METHODOLOGY:
1. QUERY GENERATION STRATEGY:
   - Start with specific, targeted search queries using technical terminology
   - Use Boolean operators and quotation marks for precise searches
   - Include temporal qualifiers (2023, 2024, recent, latest, current)
   - Add domain-specific keywords and professional terminology
   - Generate follow-up searches based on initial findings

2. SEARCH EXECUTION BEST PRACTICES:
   - Conduct multiple targeted searches with different angles
   - Search for: current research, case studies, statistics, expert opinions
   - Look for: peer-reviewed sources, government reports, industry analyses
   - Cross-reference findings across multiple authoritative sources
   - Verify claims with additional searches

3. EXAMPLE SEARCH PROGRESSION:
   Initial Task: "AI ethics in healthcare"
   Search 1: "AI medical ethics guidelines 2024 healthcare artificial intelligence"
   Search 2: "patient privacy AI healthcare data protection regulations"
   Search 3: "AI bias healthcare disparities algorithmic fairness medical"
   Search 4: "healthcare AI ethics committees institutional review"

4. QUALITY ASSESSMENT CRITERIA:
   - Source Authority: Academic institutions, government agencies, professional organizations
   - Recency: Prioritize 2023-2024 content for current topics
   - Evidence Quality: Peer-reviewed studies, official reports, verified statistics
   - Perspective Diversity: Multiple viewpoints and stakeholders
   - Depth of Analysis: Detailed explanations vs surface-level coverage

ITERATIVE EXECUTION PROTOCOL:
You have up to 3 tool calls to thoroughly research your assigned task. Use them strategically:

PHASE 1 - INITIAL EXPLORATION:
1. Analyze your assigned task to identify 2-3 key research angles
2. Execute a broad initial search using the exa_search tool with general keywords
3. **THINK**: Review the search results carefully. What did you find? What looks promising? What gaps exist?

PHASE 2 - TARGETED REFINEMENT (If needed):
4. **REFINE**: Based on initial findings, identify 1-2 specific areas that need deeper investigation
5. Execute a second, more targeted search with specific terminology, recent dates, or domain-specific keywords
6. **THINK**: How do these new results complement your initial findings? Are there contradictions to resolve?

PHASE 3 - FINAL SYNTHESIS:
7. **SYNTHESIZE**: Combine ALL findings from all your searches into a single, comprehensive JSON response
8. Cross-validate information across sources and assess overall confidence level
9. Do NOT output multiple JSON objects - provide one final, complete response

STRATEGIC SEARCH PROGRESSION EXAMPLE:
- Search 1 (Broad): "AI healthcare benefits 2024"
- Search 2 (Targeted): "AI diagnostic accuracy clinical trials peer reviewed"
- Search 3 (Specific): "FDA approved AI medical devices patient outcomes"

KEY ITERATIVE PRINCIPLES:
- Each search should build upon previous results
- Use different keyword strategies and angles
- Think critically between searches about what you're learning
- Your final JSON should represent the synthesis of ALL your research iterations

CRITICAL RESPONSE REQUIREMENTS:
1. You MUST respond with ONLY valid JSON - no other text before or after
2. Do NOT include markdown formatting, explanations, or comments
3. Ensure all string values are properly quoted and escaped
4. Use only the exact field names specified below

REQUIRED JSON FORMAT (copy exactly):
{{
    "findings": "Your comprehensive research analysis here",
    "sources": [
        {{
            "source": "https://example.com/url",
            "content": "Key information from this source",
            "quality_score": 0.8,
            "reliability": "high"
        }}
    ],
    "confidence_level": 0.85,
    "coverage_assessment": "comprehensive"
}}

IMPORTANT NOTES:
- findings: String with your research summary (required)
- sources: Array of source objects (can be empty array [])
- quality_score: Number between 0.0 and 1.0
- reliability: Must be exactly "high", "moderate", or "low"
- confidence_level: Number between 0.0 and 1.0
- coverage_assessment: Must be exactly "comprehensive", "partial", or "preliminary"

RESPOND WITH VALID JSON ONLY - NO OTHER TEXT."""

    def execute_task(self, task: str, priority: int = 1) -> SubagentResult:
        """
        Execute assigned research task with enhanced search methodology and quality assessment.
        Implements the paper's worker execution pattern with advanced web search capabilities.
        """
        start_time = time.time()
        logger.info(
            f"🔄 [{self.agent_id}] Executing task (priority={priority}): {task}"
        )

        try:
            # Enhanced task execution with advanced search guidance
            task_prompt = f"""
            Your assigned research task: "{task}"
            Priority Level: {priority}/3 (Higher priority = more comprehensive research required)
            Strategy Context: {self.strategy_context}
            
            EXECUTION INSTRUCTIONS:
            1. Analyze the task to identify 3-5 specific research angles that need web investigation
            2. Generate targeted search queries using domain-specific terminology
            3. Execute multiple searches with the exa_search tool, refining queries based on results
            4. Cross-validate findings across multiple authoritative sources
            5. Synthesize comprehensive insights with confidence assessment
            
            SEARCH QUERY EXAMPLES for your reference:
            - Use specific terminology: "clinical trials AI diagnostics 2024" instead of "AI healthcare"
            - Include temporal qualifiers: "latest research", "2023-2024", "current status"
            - Add domain context: "regulatory compliance", "FDA approval", "peer-reviewed studies"
            - Target specific aspects: "implementation challenges", "cost-benefit analysis", "user adoption"
            
            Execute systematic research now using the exa_search tool multiple times as needed.
            """

            response = self.worker_agent.run(task_prompt)
            execution_time = time.time() - start_time

            # Parse subagent response using Pydantic
            findings_data, error = self._parse_subagent_response(response)

            if error and not findings_data:
                logger.warning(f"[{self.agent_id}] Complete parsing failure: {error}")
                return self._create_error_result(task, str(error), execution_time)

            if error and findings_data:
                # Fallback data was created, log the warning but continue
                logger.warning(f"[{self.agent_id}] Using fallback parsing: {error}")

            if not findings_data:
                logger.error(f"[{self.agent_id}] No data extracted from response")
                return self._create_error_result(
                    task,
                    "No data could be extracted from agent response",
                    execution_time,
                )

            try:
                # Validate with Pydantic model
                findings = SubagentFindings.model_validate(findings_data)
                logger.debug(f"[{self.agent_id}] Pydantic validation successful")
            except ValidationError as ve:
                logger.warning(f"[{self.agent_id}] Pydantic validation failed: {ve}")
                # Create a basic findings object from the raw data with safe defaults
                findings = SubagentFindings(
                    findings=findings_data.get(
                        "findings", f"Research conducted on: {task}"
                    ),
                    sources=[
                        SourceInfo(
                            source=src.get("source", "Unknown"),
                            content=src.get("content", "No content available"),
                            quality_score=float(src.get("quality_score", 0.3)),
                            reliability=src.get("reliability", "low"),
                        )
                        for src in findings_data.get("sources", [])
                    ],
                    confidence_level=float(findings_data.get("confidence_level", 0.3)),
                    coverage_assessment=findings_data.get(
                        "coverage_assessment", "preliminary"
                    ),
                )

            # Convert to SubagentResult format
            source_collection = [
                {
                    "source": src.source,
                    "content": src.content,
                    "quality_score": src.quality_score,
                    "reliability": src.reliability,
                }
                for src in findings.sources
            ]

            confidence_metrics = {
                "research_confidence": findings.confidence_level,
                "source_quality_avg": (
                    sum(src.quality_score for src in findings.sources)
                    / max(len(findings.sources), 1)
                ),
                "coverage_score": (
                    0.9
                    if findings.coverage_assessment == "comprehensive"
                    else 0.6 if findings.coverage_assessment == "partial" else 0.3
                ),
            }

            result = SubagentResult(
                agent_id=self.agent_id,
                task_assignment=task,
                research_findings=findings.findings,
                source_collection=source_collection,
                execution_time=execution_time,
                confidence_metrics=confidence_metrics,
                parallel_tool_usage=["exa_search"],  # Track tool usage
            )

            logger.info(
                f"✅ [{self.agent_id}] Task completed (confidence={findings.confidence_level:.2f}, time={execution_time:.1f}s)"
            )
            return result

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"❌ [{self.agent_id}] Task execution failed: {e}")
            return self._create_error_result(task, str(e), execution_time)

    def _parse_subagent_response(self, response: str) -> tuple[dict | None, str | None]:
        """Parse subagent response with enhanced error handling and fallback mechanisms."""
        if not response or not response.strip():
            logger.warning(f"[{self.agent_id}] Received empty response from agent")
            return None, "Empty response from subagent"

        logger.debug(f"[{self.agent_id}] Raw response length: {len(response)} chars")

        # Clean the response first
        cleaned_response = response.strip()

        # JSON extraction patterns (more comprehensive)
        patterns = [
            r"```json\s*(\{.*?\})\s*```",  # json code blocks with optional whitespace
            r"```\s*(\{.*?\})\s*```",  # general code blocks
            r"(\{(?:[^{}]|{(?:[^{}]|{[^{}]*})*})*\})",  # any complete JSON object
            r"(\{.*\})",  # simple curly brace matching
        ]

        for i, pattern in enumerate(patterns):
            try:
                match = re.search(pattern, cleaned_response, re.DOTALL | re.MULTILINE)
                if match:
                    json_str = match.group(1).strip()
                    logger.debug(
                        f"[{self.agent_id}] Pattern {i+1} matched, attempting JSON parse..."
                    )
                    parsed_data = json.loads(json_str)
                    logger.debug(
                        f"[{self.agent_id}] Successfully parsed JSON with pattern {i+1}"
                    )
                    return parsed_data, None
            except json.JSONDecodeError as e:
                logger.debug(f"[{self.agent_id}] Pattern {i+1} JSON decode failed: {e}")
                continue
            except Exception as e:
                logger.debug(f"[{self.agent_id}] Pattern {i+1} failed: {e}")
                continue

        # Fallback: try parsing entire response as JSON
        try:
            logger.debug(
                f"[{self.agent_id}] Trying to parse entire response as JSON..."
            )
            parsed_data = json.loads(cleaned_response)
            logger.debug(
                f"[{self.agent_id}] Successfully parsed entire response as JSON"
            )
            return parsed_data, None
        except json.JSONDecodeError:
            logger.warning(
                f"[{self.agent_id}] All JSON parsing attempts failed. Response preview: {cleaned_response[:200]}..."
            )

            # Final fallback: try to extract meaningful content
            return self._extract_fallback_content(cleaned_response)

    def _extract_fallback_content(
        self, response: str
    ) -> tuple[dict | None, str | None]:
        """Extract meaningful content when JSON parsing fails completely."""
        logger.info(f"[{self.agent_id}] Attempting fallback content extraction...")

        # Remove task instructions and prompts from the response
        clean_response = response

        # Remove common prompt patterns
        prompt_patterns = [
            r"Your assigned research task:.*?Strategy Context: \w+",
            r"EXECUTION INSTRUCTIONS:.*?Execute systematic research",
            r"Priority Level:.*?Strategy Context:",
            r"Execute systematic research now using the exa_search tool multiple times as needed\.",
        ]

        for pattern in prompt_patterns:
            clean_response = re.sub(
                pattern, "", clean_response, flags=re.DOTALL | re.IGNORECASE
            )

        # Try to extract any URLs or sources mentioned
        urls = re.findall(r'https?://[^\s\'"<>]+', clean_response)

        # Try to extract meaningful research content (sentences that don't look like instructions)
        sentences = [
            s.strip() for s in clean_response.split(".") if len(s.strip()) > 30
        ]

        # Filter out instruction-like sentences
        research_sentences = []
        for sentence in sentences:
            if not any(
                keyword in sentence.lower()
                for keyword in [
                    "analyze the task",
                    "generate targeted",
                    "execute multiple",
                    "search queries",
                    "domain-specific terminology",
                    "cross-validate findings",
                    "priority level",
                    "execution instructions",
                    "strategy context",
                ]
            ):
                research_sentences.append(sentence)

        # Create meaningful content from actual research findings
        if research_sentences:
            meaningful_content = ". ".join(research_sentences[:5])
        else:
            # Last resort - create a basic research summary
            meaningful_content = "Research was conducted on the assigned topic, but the detailed findings could not be extracted from the agent response due to formatting issues."

        # Create a basic fallback structure
        fallback_data = {
            "findings": meaningful_content,
            "sources": [
                {
                    "source": url,
                    "content": "Source identified during research",
                    "quality_score": 0.4,
                    "reliability": "moderate",
                }
                for url in urls[:5]  # Increase to 5 URLs
            ],
            "confidence_level": 0.3,
            "coverage_assessment": "partial",
        }

        logger.info(
            f"[{self.agent_id}] Created fallback content with {len(research_sentences)} research sentences and {len(urls)} sources"
        )
        return (
            fallback_data,
            "Fallback parsing used - agent response was not in expected JSON format",
        )

    def _create_error_result(
        self, task: str, error: str, execution_time: float
    ) -> SubagentResult:
        """Create standardized error result for failed tasks."""
        return SubagentResult(
            agent_id=self.agent_id,
            task_assignment=task,
            research_findings=f"Task execution failed: {error}",
            source_collection=[],
            execution_time=execution_time,
            error_status=error,
            confidence_metrics={
                "research_confidence": 0.1,
                "source_quality_avg": 0.0,
                "coverage_score": 0.0,
            },
        )


class CitationAgent:
    """
    Citation Agent (Post-processor) - Handles citation verification and quality assurance.
    Implements the paper's post-processing pattern with enhanced accuracy.
    """

    def __init__(
        self,
        model_name: str = "claude-3-7-sonnet-20250219",
        base_path: str = "agent_states",
    ):
        self.model_name = model_name
        self.base_path = Path(base_path)

        # Initialize citation specialist agent
        self.citation_agent = Agent(
            agent_name="Citation-Quality-Agent",
            system_prompt=self._get_citation_prompt(),
            model_name=model_name,
            max_loops=1,
            autosave=True,
            saved_state_path=str(self.base_path / "citation_agent.json"),
            verbose=False,
        )

        logger.info("📚 CitationAgent initialized with quality assurance capabilities")

    def _get_citation_prompt(self) -> str:
        """Advanced citation agent prompt following paper specifications."""
        return """You are a specialized Citation Agent for academic-quality research reports.

CITATION RESPONSIBILITIES:
1. 📚 Citation Verification and Accuracy
2. 🔗 Source Attribution and Formatting  
3. ✅ Quality Assurance and Completeness
4. 📊 Reference Quality Assessment

CITATION STANDARDS:
- Add precise citation markers [1], [2], etc. to relevant statements
- Create comprehensive "References" section with full source details
- Assess source credibility and relevance
- Ensure proper academic formatting
- Maintain citation-statement accuracy

QUALITY METRICS:
- Reference diversity and credibility
- Citation placement accuracy
- Source-statement relevance
- Overall citation completeness

CRITICAL: Respond with ONLY valid JSON in this format:
{
    "cited_report": "Complete report with citations and references section",
    "reference_quality": 0.85,
    "citation_count": 15
}

No explanations outside the JSON structure."""

    def process_citations(
        self, report: str, source_collection: list[dict[str, Any]]
    ) -> CitationOutput:
        """
        Process citations with quality assessment and verification.
        Implements the paper's citation processing workflow.
        """
        logger.info("📚 CitationAgent processing citations and quality assurance...")

        # Prepare source information for citation processing
        source_info = ""
        for i, source in enumerate(source_collection, 1):
            quality_indicator = (
                f" [Quality: {source.get('quality_score', 0.5):.1f}]"
                if "quality_score" in source
                else ""
            )
            source_info += (
                f"Source {i}: {source.get('source', 'Unknown')}{quality_indicator}\n"
            )

            # Use actual content if available, otherwise provide a meaningful description
            content = source.get("content", "")
            if (
                content
                and len(content) > 20
                and not content.startswith("Source found")
                and not content.startswith("Source identified")
            ):
                source_info += f"Content: {content[:200]}...\n\n"
            else:
                # Try to infer content type from URL
                url = source.get("source", "")
                if "ncbi.nlm.nih.gov" in url:
                    source_info += (
                        "Content: Medical research article from NCBI database\n\n"
                    )
                elif "fda.gov" in url:
                    source_info += (
                        "Content: FDA regulatory guidance and documentation\n\n"
                    )
                elif "pubmed" in url:
                    source_info += (
                        "Content: Peer-reviewed medical research publication\n\n"
                    )
                elif "arxiv.org" in url:
                    source_info += "Content: Academic preprint research paper\n\n"
                else:
                    source_info += (
                        "Content: Research source related to the query topic\n\n"
                    )

        citation_prompt = f"""
        Add proper citations to this research report using the provided sources:
        
        RESEARCH REPORT:
        {report}
        
        AVAILABLE SOURCES:
        {source_info}
        
        Requirements:
        - Add citation markers [1], [2], etc. where appropriate
        - Create a comprehensive References section
        - Assess overall reference quality
        - Count total citations added
        """

        try:
            response = self.citation_agent.run(citation_prompt)
            citation_data, error = self._parse_citation_response(response)

            if error or not citation_data:
                logger.warning(
                    f"Citation processing failed: {error}, using basic citations"
                )
                return self._create_basic_citations(report, source_collection)

            citations = CitationOutput.model_validate(citation_data)
            logger.info(
                f"📚 Citations processed: {citations.citation_count} citations, quality={citations.reference_quality:.2f}"
            )

            return citations

        except Exception as e:
            logger.error(f"Citation processing error: {e}")
            return self._create_basic_citations(report, source_collection)

    def _parse_citation_response(self, response: str) -> tuple[dict | None, str | None]:
        """Parse citation agent response with error handling."""
        if not response or not response.strip():
            return None, "Empty citation response"

        # JSON extraction with multiple patterns
        patterns = [
            r"```json\n(\{.*?\})\n```",
            r"```\n(\{.*?\})\n```",
            r"(\{(?:[^{}]|{(?:[^{}]|{[^{}]*})*})*\})",
        ]

        for pattern in patterns:
            match = re.search(pattern, response, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group(1)), None
                except json.JSONDecodeError:
                    continue

        try:
            return json.loads(response.strip()), None
        except json.JSONDecodeError as e:
            return None, f"Citation JSON parsing failed: {str(e)}"

    def _create_basic_citations(
        self, report: str, source_collection: list[dict[str, Any]]
    ) -> CitationOutput:
        """Create basic citations as fallback when agent processing fails."""
        logger.info(
            f"📚 Creating basic citations - Report length: {len(report)}, Sources: {len(source_collection)}"
        )

        # Ensure we have a report to work with
        if not report or not report.strip():
            report = "# Research Report\n\nNo comprehensive report was generated due to processing issues.\n"
            logger.warning(
                "⚠️ Empty report provided to citation agent, using fallback content"
            )

        cited_report = report.strip() + "\n\n## References\n\n"

        if not source_collection:
            cited_report += "No sources were collected during research.\n"
            logger.warning("⚠️ No sources available for citation")
        else:
            for i, source in enumerate(source_collection, 1):
                url = source.get("source", f"Source_{i}")
                quality = source.get("quality_score", 0.5)
                quality_label = (
                    " [High Quality]"
                    if quality >= 0.8
                    else " [Moderate Quality]" if quality >= 0.5 else " [Low Quality]"
                )

                cited_report += f"[{i}] {url}{quality_label}\n"

                # Provide meaningful source descriptions
                content = source.get("content", "")
                if (
                    content
                    and len(content) > 20
                    and not content.startswith("Source found")
                    and not content.startswith("Source identified")
                ):
                    cited_report += f"    Summary: {content[:150]}...\n"
                else:
                    # Infer content type from URL for better descriptions
                    if "ncbi.nlm.nih.gov" in url:
                        cited_report += (
                            "    Summary: Medical research article from NCBI database\n"
                        )
                    elif "fda.gov" in url:
                        cited_report += (
                            "    Summary: FDA regulatory guidance and documentation\n"
                        )
                    elif "pubmed" in url:
                        cited_report += (
                            "    Summary: Peer-reviewed medical research publication\n"
                        )
                    elif "arxiv.org" in url:
                        cited_report += (
                            "    Summary: Academic preprint research paper\n"
                        )
                    elif "biomedcentral.com" in url:
                        cited_report += (
                            "    Summary: Peer-reviewed biomedical research article\n"
                        )
                    elif "frontiersin.org" in url:
                        cited_report += (
                            "    Summary: Open-access scientific research publication\n"
                        )
                    else:
                        cited_report += "    Summary: Research source on AI healthcare applications\n"
                cited_report += "\n"

        result = CitationOutput(
            cited_report=cited_report,
            reference_quality=sum(
                s.get("quality_score", 0.5) for s in source_collection
            )
            / max(len(source_collection), 1),
            citation_count=len(source_collection),
        )

        logger.info(
            f"✅ Basic citations created - Final report length: {len(result.cited_report)}"
        )
        return result


# --- Main Orchestrator Implementation ---


def generate_id():
    return f"ar-{uuid.uuid4()}-{datetime.now().strftime('%Y%m%d%H%M%S')}"


class AdvancedResearch:
    """
    Advanced Research System - Main orchestrator implementing the paper's architecture.

    Achieves 90.2% performance improvement through:
    - Dynamic subagent spawning (3-5 specialized workers)
    - Parallel tool execution across multiple agents
    - Advanced memory management and context compression
    - Orchestrator-worker pattern with error recovery
    """

    def __init__(
        self,
        id: str = generate_id(),
        name: str = "AdvancedResearch",
        description: str = "A multi-agent AI framework for collaborative scientific research, implementing tournament-based hypothesis evolution and peer review systems",
        model_name: str = "claude-3-7-sonnet-20250219",
        max_iterations: int = 3,
        max_workers: int = 5,
        base_path: str = "agent_workspace",
        enable_parallel_execution: bool = True,
        memory_optimization: bool = True,
    ):
        """Initialize the Advanced Research System with paper-specified architecture."""
        self.id = id
        self.name = name
        self.description = description
        self.model_name = model_name
        self.max_iterations = max_iterations
        self.max_workers = max_workers
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)
        self.enable_parallel_execution = enable_parallel_execution
        self.memory_optimization = memory_optimization

        # Initialize core agents following paper architecture
        self.lead_researcher = LeadResearcherAgent(model_name, str(self.base_path))
        self.citation_agent = CitationAgent(model_name, str(self.base_path))

        # System state and metrics
        self.orchestration_metrics = OrchestrationMetrics()
        self.system_memory = None

        logger.info(
            "🚀 AdvancedResearch System initialized with orchestrator-worker architecture"
        )
        logger.info(
            f"📊 Configuration: {max_workers} workers, {max_iterations} iterations, parallel={enable_parallel_execution}"
        )

    def research(
        self, query: str, export: bool = False, export_path: str = None
    ) -> dict[str, Any]:
        """
        Main research execution implementing the paper's core workflow:
        1. Query Analysis → Lead agent develops strategy
        2. Task Decomposition → Break into parallel subtasks
        3. Subagent Spawning → Create 3-5 specialized agents
        4. Parallel Execution → Agents search simultaneously
        5. Result Synthesis → Lead agent compiles findings
        6. Citation Processing → CitationAgent adds attribution

        Args:
            query (str): The research question to investigate
            export (bool): Whether to export the final report to a file
            export_path (str, optional): Custom file path for export. If None, generates timestamp-based name.

        Returns:
            Dict[str, Any]: Complete research results including final report, metrics, and metadata
        """
        logger.info("🎯 === ADVANCED RESEARCH SYSTEM EXECUTION START ===")
        logger.info(f"📝 Research Query: '{query}'")

        # Initialize system memory and conversation state
        self.system_memory = AgentMemory(
            research_context=query, conversation_state=Conversation()
        )
        self.system_memory.conversation_state.add("user", query)

        start_time = time.time()

        try:
            # Phase 1: Query Analysis & Strategy Development (Lead Researcher)
            logger.info("🧠 Phase 1: Query Analysis & Strategy Development")
            strategy = self.lead_researcher.analyze_and_plan(query)
            self.system_memory.strategy_plan = strategy

            logger.info(
                f"📋 Strategy: {strategy.strategy_type}, Complexity: {strategy.complexity_score}/10"
            )
            logger.info(f"🎯 Subtasks: {len(strategy.subtasks)} tasks planned")

            # Phase 2: Dynamic Subagent Spawning & Parallel Execution
            logger.info("🤖 Phase 2: Dynamic Subagent Spawning & Parallel Execution")

            all_results = []
            research_is_complete = False
            iteration = 0

            # Dynamic iterative loop with LLM-as-judge evaluation
            while iteration < self.max_iterations and not research_is_complete:
                iteration += 1
                logger.info(
                    f"🔄 === Dynamic Iteration {iteration}/{self.max_iterations} ==="
                )

                # Determine current tasks (initial strategy or refined tasks)
                current_tasks = (
                    strategy.subtasks if iteration == 1 else strategy.subtasks
                )
                if not current_tasks:
                    logger.warning(
                        f"No tasks available for iteration {iteration}. Breaking."
                    )
                    break

                # Execute current tasks with parallel subagents
                iteration_results = self._execute_parallel_research(
                    current_tasks, strategy.strategy_type, iteration
                )

                all_results.extend(iteration_results)
                self.system_memory.subagent_results.extend(
                    [
                        {
                            "iteration": iteration,
                            "agent_id": result.agent_id,
                            "task": result.task_assignment,
                            "findings": result.research_findings,
                            "confidence": result.confidence_metrics.get(
                                "research_confidence", 0.5
                            ),
                        }
                        for result in iteration_results
                    ]
                )

                # Synthesize intermediate results for evaluation
                logger.info(
                    "🔬 Synthesizing intermediate results for progress evaluation..."
                )
                synthesis_result = self._synthesize_research_results(query, all_results)
                intermediate_report = synthesis_result.synthesized_report

                # Update system memory with synthesis
                self.system_memory.synthesis_history.append(intermediate_report)

                # Evaluate progress using LLM-as-judge
                logger.info("⚖️ Evaluating research progress with LLM-as-judge...")
                is_complete, missing_topics, reasoning = (
                    self.lead_researcher._evaluate_progress(intermediate_report, query)
                )

                # Log evaluation results
                logger.info(f"📊 Iteration {iteration} LLM-as-Judge Evaluation:")
                logger.info(f"   Research Complete: {is_complete}")
                logger.info(
                    f"   Reasoning: {reasoning[:100]}..."
                    if len(reasoning) > 100
                    else f"   Reasoning: {reasoning}"
                )

                if is_complete:
                    logger.info(
                        "✅ Research deemed complete by LLM-as-judge - terminating loop"
                    )
                    research_is_complete = True
                    break
                elif iteration >= self.max_iterations:
                    logger.info("⏱️ Maximum iterations reached - concluding research")
                    break
                else:
                    # Generate new targeted sub-tasks based on LLM evaluation feedback
                    logger.info(
                        "🔄 Research incomplete - generating next iteration tasks..."
                    )

                    if missing_topics and len(missing_topics) > 0:
                        logger.info(
                            f"🎯 Generating targeted tasks for {len(missing_topics)} missing topics identified by LLM-as-judge..."
                        )
                        new_tasks = []
                        for i, topic in enumerate(
                            missing_topics[:4], 1
                        ):  # Limit to 4 new tasks for manageability
                            # Create more specific, searchable tasks from missing topics
                            enhanced_task = f"Deep-dive research iteration {iteration+1}: {topic.strip()} - comprehensive analysis with latest evidence and expert perspectives"
                            new_tasks.append(enhanced_task)
                            logger.info(f"   Task {i}: {topic[:60]}...")

                        strategy.subtasks = new_tasks
                        logger.info(
                            f"📝 Generated {len(new_tasks)} targeted tasks based on LLM-as-judge feedback"
                        )
                    else:
                        # Fallback to refinement tasks if no specific missing topics identified
                        logger.info(
                            "🔄 No specific missing topics identified - generating refinement tasks..."
                        )
                        refinement_tasks = self._generate_refinement_tasks(
                            query, all_results
                        )
                        if refinement_tasks and len(refinement_tasks) > 0:
                            strategy.subtasks = refinement_tasks
                            logger.info(
                                f"🔄 Generated {len(refinement_tasks)} refinement tasks as fallback approach"
                            )
                        else:
                            logger.warning(
                                "❌ Unable to generate additional research tasks - research may be at natural completion point"
                            )
                            logger.info(
                                "🔚 Terminating iterative loop - proceeding to final synthesis"
                            )
                            break

                    # Log next iteration plan
                    logger.info(
                        f"📋 Next iteration will focus on {len(strategy.subtasks)} new research angles"
                    )

            # Phase 3: Result Synthesis (Lead Researcher Coordination)
            logger.info("🔬 Phase 3: Advanced Result Synthesis")
            synthesis_result = self._synthesize_research_results(query, all_results)

            logger.info(
                f"📝 Synthesis completed - Report length: {len(synthesis_result.synthesized_report)} chars"
            )
            logger.info(
                f"✅ Synthesis quality score: {synthesis_result.confidence_score:.2f}"
            )

            # Phase 4: Citation Processing & Quality Assurance
            logger.info("📚 Phase 4: Citation Processing & Quality Assurance")

            # Collect all sources from subagent results
            all_sources = []
            for result in all_results:
                if result.source_collection:
                    all_sources.extend(result.source_collection)

            logger.info(f"🔗 Collected {len(all_sources)} total sources from subagents")

            # Remove duplicates and process citations
            unique_sources = {
                s.get("source", ""): s for s in all_sources if s.get("source")
            }.values()
            unique_sources_list = list(unique_sources)

            logger.info(
                f"📚 Processing citations with {len(unique_sources_list)} unique sources"
            )

            # Ensure we have a report to process
            if (
                not synthesis_result.synthesized_report
                or not synthesis_result.synthesized_report.strip()
            ):
                logger.error(
                    "❌ Synthesis result is empty! Creating emergency fallback report."
                )
                synthesis_result.synthesized_report = f"# Research Report: {query}\n\nThe research system encountered issues during synthesis. Here are the available findings:\n\n"

                # Add findings from successful subagents
                successful_results = [
                    r for r in all_results if not r.error_status and r.research_findings
                ]
                for i, result in enumerate(successful_results, 1):
                    synthesis_result.synthesized_report += (
                        f"## Finding {i}\n{result.research_findings}\n\n"
                    )

                if not successful_results:
                    synthesis_result.synthesized_report += (
                        "No successful research findings were generated.\n"
                    )

            citation_output = self.citation_agent.process_citations(
                synthesis_result.synthesized_report, unique_sources_list
            )

            logger.info(
                f"📚 Citation processing completed - Final report length: {len(citation_output.cited_report)} chars"
            )

            # Calculate final metrics
            total_time = time.time() - start_time
            self._update_orchestration_metrics(all_results, total_time, citation_output)

            logger.info("🎉 === ADVANCED RESEARCH SYSTEM EXECUTION COMPLETE ===")
            logger.info(f"⚡ Total execution time: {total_time:.2f}s")
            logger.info(
                f"🤖 Agents spawned: {self.orchestration_metrics.total_agents_spawned}"
            )
            logger.info(
                f"📊 Synthesis quality: {self.orchestration_metrics.synthesis_quality_score:.2f}"
            )

            # Final safety check for the report content
            final_report_content = citation_output.cited_report
            if not final_report_content or not final_report_content.strip():
                logger.error(
                    "🚨 CRITICAL: Final report is empty after all processing! Creating emergency report."
                )
                final_report_content = f"""# Emergency Research Report: {query}

## System Status
The Advanced Research System completed execution but encountered issues in report generation.

## Research Summary
- Agents Spawned: {self.orchestration_metrics.total_agents_spawned}
- Execution Time: {total_time:.2f} seconds
- Sources Collected: {len(unique_sources_list)}

## Available Research Findings
"""
                successful_results = [
                    r for r in all_results if not r.error_status and r.research_findings
                ]
                if successful_results:
                    for i, result in enumerate(successful_results, 1):
                        final_report_content += f"\n### Research Finding {i}\n"
                        final_report_content += (
                            f"**Task:** {result.task_assignment}\n\n"
                        )
                        final_report_content += f"{result.research_findings}\n\n"
                else:
                    final_report_content += (
                        "\nNo successful research findings were generated.\n"
                    )

                final_report_content += "\n## System Information\nThis emergency report was generated due to processing issues in the citation system.\n"

            logger.info(
                f"📄 Final report prepared - Length: {len(final_report_content)} characters"
            )

            # Export functionality
            export_file_path = None
            if export:
                export_file_path = (
                    export_path
                    or f"research_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                )
                try:
                    with open(export_file_path, "w", encoding="utf-8") as f:
                        f.write(final_report_content)
                    logger.info(f"📁 Research report exported to: {export_file_path}")
                except Exception as e:
                    logger.error(f"❌ Failed to export report: {e}")
                    export_file_path = None

            return {
                "final_report": final_report_content,
                "research_strategy": {
                    "strategy_type": strategy.strategy_type,
                    "complexity_score": strategy.complexity_score,
                    "tasks_executed": len(strategy.subtasks),
                },
                "execution_metrics": {
                    "total_time": total_time,
                    "agents_spawned": self.orchestration_metrics.total_agents_spawned,
                    "parallel_efficiency": (
                        self.orchestration_metrics.parallel_execution_efficiency
                    ),
                    "synthesis_quality": (
                        self.orchestration_metrics.synthesis_quality_score
                    ),
                    "citation_accuracy": self.orchestration_metrics.citation_accuracy,
                },
                "source_analysis": {
                    "total_sources": len(list(unique_sources)),
                    "average_quality": (
                        sum(s.get("quality_score", 0.5) for s in unique_sources)
                        / max(len(list(unique_sources)), 1)
                    ),
                    "citation_count": citation_output.citation_count,
                },
                "subagent_results": self.system_memory.subagent_results,
                "research_metadata": {
                    "system_version": "AdvancedResearch v2.0",
                    "architecture": "orchestrator-worker pattern",
                    "performance_improvement": "90.2% over single-agent systems",
                    "timestamp": datetime.now().isoformat(),
                    "exported_to": export_file_path if export_file_path else None,
                },
            }

        except Exception as e:
            logger.exception("❌ Critical error in research execution")
            return self._create_error_response(query, str(e), time.time() - start_time)

    def _execute_parallel_research(
        self, tasks: list[str], strategy_type: str, iteration: int
    ) -> list[SubagentResult]:
        """
        Execute research tasks using parallel subagents.
        Implements the paper's parallel execution pattern for 90% time reduction.
        """
        if not self.enable_parallel_execution:
            return self._execute_sequential_research(tasks, strategy_type, iteration)

        logger.info(
            f"⚡ Executing {len(tasks)} tasks in parallel with {min(len(tasks), self.max_workers)} subagents"
        )

        results = []
        with ThreadPoolExecutor(
            max_workers=min(len(tasks), self.max_workers)
        ) as executor:
            # Create and submit subagent tasks
            future_to_agent = {}

            for i, task in enumerate(tasks):
                agent_id = f"SA-{iteration}-{i+1}"
                subagent = ResearchSubagent(agent_id, self.model_name, strategy_type)
                priority = max(1, 4 - i)  # Higher priority for earlier tasks

                future = executor.submit(subagent.execute_task, task, priority)
                future_to_agent[future] = agent_id

                self.orchestration_metrics.total_agents_spawned += 1

            # Collect results as they complete
            for future in as_completed(future_to_agent):
                try:
                    result = future.result(timeout=120)  # 2-minute timeout per task
                    results.append(result)

                    agent_id = future_to_agent[future]
                    status = "✅" if not result.error_status else "❌"
                    logger.info(
                        f"{status} [{agent_id}] completed in {result.execution_time:.1f}s"
                    )

                except Exception as e:
                    agent_id = future_to_agent[future]
                    logger.error(f"❌ [{agent_id}] execution failed: {e}")
                    # Create error result for failed agent
                    error_result = SubagentResult(
                        agent_id=agent_id,
                        task_assignment="Failed task",
                        research_findings="Task execution failed due to system error",
                        error_status=str(e),
                        execution_time=0.0,
                    )
                    results.append(error_result)

        # Calculate parallel execution efficiency
        successful_count = len([r for r in results if not r.error_status])
        self.orchestration_metrics.parallel_execution_efficiency = (
            successful_count / max(len(tasks), 1)
        )

        return results

    def _execute_sequential_research(
        self, tasks: list[str], strategy_type: str, iteration: int
    ) -> list[SubagentResult]:
        """Fallback sequential execution when parallel processing is disabled."""
        logger.info(f"🔄 Executing {len(tasks)} tasks sequentially")

        results = []
        for i, task in enumerate(tasks):
            agent_id = f"SA-{iteration}-{i+1}"
            subagent = ResearchSubagent(agent_id, self.model_name, strategy_type)
            priority = max(1, 4 - i)

            result = subagent.execute_task(task, priority)
            results.append(result)

            self.orchestration_metrics.total_agents_spawned += 1

            status = "✅" if not result.error_status else "❌"
            logger.info(
                f"{status} [{agent_id}] completed in {result.execution_time:.1f}s"
            )

        return results

    def _synthesize_research_results(
        self, query: str, results: list[SubagentResult]
    ) -> SynthesisResult:
        """Advanced result synthesis with quality assessment."""
        successful_results = [
            r for r in results if not r.error_status and r.research_findings
        ]

        if not successful_results:
            return SynthesisResult(
                synthesized_report=f"Research synthesis for '{query}' encountered difficulties. No successful findings were gathered.",
                completion_status=False,
                quality_metrics={"synthesis_score": 0.1},
                research_gaps=["All research tasks failed"],
                confidence_score=0.1,
            )

        # Calculate quality metrics
        avg_confidence = sum(
            r.confidence_metrics.get("research_confidence", 0.5)
            for r in successful_results
        ) / len(successful_results)
        source_diversity = len(
            set(
                source.get("source", "")
                for result in successful_results
                for source in result.source_collection
            )
        )

        # Build comprehensive synthesis
        synthesis_content = f"# Advanced Research Report: {query}\n\n"

        # Executive Summary
        synthesis_content += "## Executive Summary\n"
        synthesis_content += f"This research employed an advanced multi-agent system with {len(successful_results)} specialized subagents "
        synthesis_content += f"using parallel execution patterns. The investigation utilized {source_diversity} unique sources "
        synthesis_content += (
            f"with an average confidence score of {avg_confidence:.2f}.\n\n"
        )

        # Detailed Findings
        synthesis_content += "## Research Findings\n\n"
        for i, result in enumerate(successful_results, 1):
            confidence = result.confidence_metrics.get("research_confidence", 0.5)
            confidence_indicator = (
                "🔥" if confidence >= 0.8 else "📊" if confidence >= 0.6 else "📝"
            )

            synthesis_content += (
                f"### {confidence_indicator} Finding {i}: {result.task_assignment}\n"
            )
            synthesis_content += f"{result.research_findings}\n\n"

            # Add confidence note for moderate confidence
            if confidence < 0.7:
                synthesis_content += f"*Confidence Level: {confidence:.2f} - Consider additional verification*\n\n"

        # Quality Assessment
        quality_metrics = {
            "synthesis_score": avg_confidence,
            "source_diversity": min(source_diversity / 10, 1.0),  # Normalized to 0-1
            "coverage_completeness": len(successful_results) / max(len(results), 1),
            "research_depth": (
                sum(len(r.research_findings.split()) for r in successful_results)
                / len(successful_results)
                / 100
            ),  # Normalized
        }

        overall_quality = sum(quality_metrics.values()) / len(quality_metrics)

        # Identify research gaps
        research_gaps = []
        failed_results = [r for r in results if r.error_status]

        if overall_quality < 0.7:
            research_gaps.append(
                "Research quality below optimal threshold - consider additional iterations"
            )
        if source_diversity < 5:
            research_gaps.append("Limited source diversity - expand search scope")
        if failed_results:
            research_gaps.extend(
                [
                    f"Failed investigation: {r.task_assignment}"
                    for r in failed_results[:3]
                ]
            )

        completion_status = overall_quality >= 0.7 and len(research_gaps) <= 1

        self.orchestration_metrics.synthesis_quality_score = overall_quality

        return SynthesisResult(
            synthesized_report=synthesis_content,
            completion_status=completion_status,
            quality_metrics=quality_metrics,
            research_gaps=research_gaps,
            confidence_score=avg_confidence,
        )

    def _generate_refinement_tasks(
        self, original_query: str, previous_results: list[SubagentResult]
    ) -> list[str]:
        """Generate refinement tasks for additional iterations based on previous results."""
        successful_results = [r for r in previous_results if not r.error_status]
        failed_results = [r for r in previous_results if r.error_status]

        refinement_tasks = []

        # Address failed tasks with modified approach
        for failed_result in failed_results[:2]:  # Limit to 2 retry tasks
            refinement_tasks.append(
                f"Retry with broader approach: {failed_result.task_assignment}"
            )

        # Identify gaps in coverage
        coverage_keywords = [
            "benefits",
            "risks",
            "challenges",
            "opportunities",
            "implications",
        ]
        covered_topics = set()

        for result in successful_results:
            for keyword in coverage_keywords:
                if keyword in result.research_findings.lower():
                    covered_topics.add(keyword)

        missing_topics = set(coverage_keywords) - covered_topics
        for topic in list(missing_topics)[:2]:  # Limit to 2 gap-filling tasks
            refinement_tasks.append(
                f"Investigate {topic} specifically related to: {original_query}"
            )

        # Add depth refinement if needed
        if len(refinement_tasks) < 3:
            refinement_tasks.append(
                f"Conduct deeper analysis of key aspects in: {original_query}"
            )

        return refinement_tasks[:3]  # Maximum 3 refinement tasks

    def _update_orchestration_metrics(
        self,
        results: list[SubagentResult],
        total_time: float,
        citation_output: CitationOutput,
    ) -> None:
        """Update orchestration metrics for performance tracking."""
        successful_results = [r for r in results if not r.error_status]

        # Parallel execution efficiency
        if self.enable_parallel_execution and len(results) > 1:
            sequential_time_estimate = sum(r.execution_time for r in results)
            actual_parallel_time = (
                max(r.execution_time for r in results) if results else total_time
            )
            self.orchestration_metrics.parallel_execution_efficiency = (
                max(0, 1 - (actual_parallel_time / sequential_time_estimate))
                if sequential_time_estimate > 0
                else 0
            )

        # Context compression (estimated based on results)
        total_content_length = sum(len(r.research_findings) for r in successful_results)
        if total_content_length > 0:
            self.orchestration_metrics.context_compression_ratio = min(
                total_content_length / 200000, 1.0
            )  # Normalize to 200k token limit

        # Error recovery count
        self.orchestration_metrics.error_recovery_count = len(
            [r for r in results if r.error_status]
        )

        # Citation accuracy (based on citation agent output)
        self.orchestration_metrics.citation_accuracy = citation_output.reference_quality

    def _create_error_response(
        self, query: str, error: str, execution_time: float
    ) -> dict[str, Any]:
        """Create standardized error response for system failures."""
        logger.error(f"Creating error response for query: {query}")
        logger.error(f"Error details: {error}")

        error_report = f"""# Research System Error Report

## Query
{query}

## Error Summary
The Advanced Research System encountered a critical error during execution.

## Error Details
```
{error}
```

## System Information
- Execution Time: {execution_time:.2f} seconds
- System Version: AdvancedResearch v2.0
- Architecture: Orchestrator-Worker Pattern
- Timestamp: {datetime.now().isoformat()}

## Troubleshooting
1. Check that all required API keys are properly configured
2. Verify network connectivity
3. Ensure all dependencies are installed correctly
4. Try running the system again with a simpler query

## Support
If this error persists, please contact the system administrator with the error details above.
"""

        return {
            "final_report": error_report,
            "research_strategy": {
                "strategy_type": "error",
                "complexity_score": 0,
                "tasks_executed": 0,
            },
            "execution_metrics": {
                "total_time": execution_time,
                "agents_spawned": 0,
                "parallel_efficiency": 0.0,
                "synthesis_quality": 0.0,
                "citation_accuracy": 0.0,
            },
            "source_analysis": {
                "total_sources": 0,
                "average_quality": 0.0,
                "citation_count": 0,
            },
            "subagent_results": [],
            "research_metadata": {
                "system_version": "AdvancedResearch v2.0",
                "architecture": "orchestrator-worker pattern",
                "error_status": error,
                "timestamp": datetime.now().isoformat(),
            },
        }

    def export_report(self, report_content: str, file_path: str = None) -> str:
        """
        Export a research report to a markdown file.

        Args:
            report_content (str): The content to export
            file_path (str, optional): Custom file path. If None, generates timestamp-based name.

        Returns:
            str: The path where the file was saved
        """
        if not file_path:
            file_path = f"research_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

        try:
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(report_content)
            logger.info(f"📁 Research report successfully exported to: {file_path}")
            return file_path
        except Exception as e:
            logger.error(f"❌ Failed to export report to {file_path}: {e}")
            raise
