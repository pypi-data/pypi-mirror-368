#!/usr/bin/env bash

#
# DATABRICKS CONFIDENTIAL & PROPRIETARY
# __________________
#
# Copyright 2022-present Databricks, Inc.
# All Rights Reserved.
#
# NOTICE:  All information contained herein is, and remains the property of Databricks, Inc.
# and its suppliers, if any.  The intellectual and technical concepts contained herein are
# proprietary to Databricks, Inc. and its suppliers and may be covered by U.S. and foreign Patents,
# patents in process, and are protected by trade secret and/or copyright law. Dissemination, use,
# or reproduction of this information is strictly forbidden unless prior written permission is
# obtained from Databricks, Inc.
#
# If you view or obtain a copy of this information and believe Databricks, Inc. may not have
# intended it to be made available, please promptly report it to Databricks Legal Department
# @ legal@databricks.com.
#

# Shell script for starting the Spark Shell REPL using Bazel with Safe Spark
# functionality available for prototyping

set -euo pipefail

GIT_ROOT="$(git rev-parse --show-toplevel)"

function output_jar_cleanup {
  rm -rf $GIT_ROOT/outpath
}

REPL_TARGET=//repl:repl-hive-2.3__hadoop-3.2_2.12
HIVE_UNSHADED_TARGET=//sql/hive:hive-unshaded-hive-2.3__hadoop-3.2_2.12
SAFESPARK_TARGET=//safespark/udf/common:client-hive-2.3__hadoop-3.2_2.12
SAFESPARK_PY_TARGET=//safespark/udf/py:py-hive-2.3__hadoop-3.2_2.12

BAZEL_CMD=(bazel build --aspects=@universe//bazel/rules/scala/scala:classpath.bzl%classpath --output_groups=+runtimeclasspath)
# Uncomment as appropriate to run debug/release/asan
# For asan you also need to modify the env in conf/spark-env.sh. Instructions are in that file.
#
"${BAZEL_CMD[@]}" "${REPL_TARGET}" "${HIVE_UNSHADED_TARGET}" "${SAFESPARK_TARGET}" "${SAFESPARK_PY_TARGET}" --config debug
# "${BAZEL_CMD[@]}" "${REPL_TARGET}" "${HIVE_UNSHADED_TARGET}" --config release
# "${BAZEL_CMD[@]}" "${REPL_TARGET}" "${HIVE_UNSHADED_TARGET}" --config asan
RUNTIME_CLASSPATH_FILES=(bazel-bin/sql/hive/hive-unshaded-hive-2.3__hadoop-3.2_2.12.runtime.classpath.txt)
RUNTIME_CLASSPATH_FILES+=(bazel-bin/repl/repl-hive-2.3__hadoop-3.2_2.12.runtime.classpath.txt)
RUNTIME_CLASSPATH_FILES+=(bazel-bin/safespark/udf/common/client-hive-2.3__hadoop-3.2_2.12.runtime.classpath.txt)
RUNTIME_CLASSPATH_FILES+=(bazel-bin/safespark/udf/py/py-hive-2.3__hadoop-3.2_2.12.runtime.classpath.txt)

# Load image before we invoke repl, so image discovery mechanism in DockeredUpPyServer does not fail.
bazel run //safespark/udf/py:safespark-sandbox-python_binary_loader
echo
echo "Image loaded."
# By setting spark testing, we switch Dispatcher backing API to local docker
export SPARK_TESTING=true
# Copy all jars into outpath so we don't run into classpath sizing problems
# - NOTE: manifest version was tried and failed because it seems not all spark jars like that.
rm -rf $GIT_ROOT/outpath
trap output_jar_cleanup EXIT
mkdir outpath
cat ${RUNTIME_CLASSPATH_FILES[@]} | sort | uniq | python3 -c "
import sys, shutil
for i, line in enumerate(sys.stdin.readlines()):
   shutil.copy(line.strip(), 'outpath/%d.jar' % i)
"

export BAZEL_SPARK_SUBMIT="$(find outpath -type f | sort | tr '\n' ':')"

GIT_ROOT="$(git rev-parse --show-toplevel)"

export spark_databricks_sparkisolation_enabled=true
"$GIT_ROOT/bin/spark-shell" \
   --conf 'spark.master=local[*]' \
   --conf 'spark.databricks.sparkisolation.enabled=true' \
   --conf 'spark.databricks.sql.externalUDF.isolation.enabled=true' \
   --conf 'spark.databricks.sql.pythonUDF.enabled=true' \
    $@
