# appserver_sdk_python_ai/webscraping/__init__.py
"""
M√≥dulo de Web Scraping para appserver_sdk_python_ai
============================================

Este m√≥dulo fornece funcionalidades avan√ßadas de web scraping com convers√£o
para markdown usando Docling e outras ferramentas.

Componentes principais:
- DoclingWebScraper: Scraper principal com Docling
- Utilit√°rios de limpeza e valida√ß√£o
- Sistema de cache robusto
- Processamento em lote
- Tratamento avan√ßado de erros

Exemplo de uso b√°sico:
    from appserver_sdk_python_ai.webscraping import DoclingWebScraper, ScrapingConfig

    config = ScrapingConfig(clean_html=True, enable_cache=True)
    scraper = DoclingWebScraper(config)
    result = scraper.scrape_to_markdown("https://example.com")

Exemplo de uso simplificado:
    from appserver_sdk_python_ai.webscraping import quick_scrape
    markdown = quick_scrape("https://example.com")
"""

__version__ = "1.0.0"
__author__ = "appserver_sdk_python_ai"

# Importa√ß√µes padr√£o
import logging

# Importa√ß√µes principais
from .core.config import (
    DoclingConfig,
    GlobalWebScrapingConfig,
    ScrapingConfig,
    global_config,
)
from .core.exceptions import (
    AuthenticationError,
    CacheError,
    ContentTooLargeError,
    ConversionError,
    NetworkError,
    RateLimitError,
    UnsupportedFormatError,
    ValidationError,
    WebScrapingError,
)
from .core.models import (
    BatchScrapingResult,
    CacheEntry,
    ScrapingResult,
    ScrapingStatus,
    WebPageMetadata,
)

# Importa√ß√µes condicionais para evitar erros quando depend√™ncias n√£o est√£o instaladas
try:
    from .docling.scraper import DoclingWebScraper

    SCRAPER_AVAILABLE = True
except ImportError as e:
    DoclingWebScraper = None  # type: ignore
    SCRAPER_AVAILABLE = False
    import warnings

    warnings.warn(f"DoclingWebScraper n√£o p√¥de ser importado: {e}", stacklevel=2)

# OCR agora √© um m√≥dulo independente
# Para usar OCR, importe diretamente: from appserver_sdk_python_ai.ocr import ...
OCR_AVAILABLE = False  # Mantido para compatibilidade, mas OCR n√£o est√° mais aqui

try:
    from .utils.cache import CacheManager, MemoryCache
    from .utils.cleaner import ContentCleaner
    from .utils.validators import ContentValidator, RobotsTxtChecker, URLValidator

    UTILS_AVAILABLE = True
except ImportError as e:
    ContentCleaner = None  # type: ignore
    CacheManager = None  # type: ignore
    MemoryCache = None  # type: ignore
    URLValidator = None  # type: ignore
    ContentValidator = None  # type: ignore
    RobotsTxtChecker = None  # type: ignore
    UTILS_AVAILABLE = False
    import warnings

    warnings.warn(f"Utilit√°rios n√£o puderam ser importados: {e}", stacklevel=2)

# Verificar disponibilidade do Docling
try:
    import docling

    DOCLING_AVAILABLE = True
    DOCLING_VERSION = getattr(docling, "__version__", "unknown")
except ImportError:
    DOCLING_AVAILABLE = False
    DOCLING_VERSION = None


# Fun√ß√µes de conveni√™ncia
def quick_scrape(
    url: str,
    clean_html: bool = True,
    include_images: bool = True,
    enable_cache: bool = False,
) -> str:
    """
    Fun√ß√£o de conveni√™ncia para scraping r√°pido.

    Args:
        url: URL para fazer scraping
        clean_html: Se deve limpar HTML
        include_images: Se deve incluir imagens
        enable_cache: Se deve habilitar cache

    Returns:
        str: Conte√∫do em markdown

    Raises:
        WebScrapingError: Em caso de erro no scraping
    """
    if not SCRAPER_AVAILABLE or DoclingWebScraper is None:
        raise WebScrapingError(
            "DoclingWebScraper n√£o est√° dispon√≠vel. Verifique as depend√™ncias."
        )

    config = ScrapingConfig(
        clean_html=clean_html, include_images=include_images, enable_cache=enable_cache
    )

    scraper = DoclingWebScraper(config)
    result = scraper.scrape_to_markdown(url)

    if not result.success:
        raise WebScrapingError(f"Falha no scraping: {result.error}", url)

    return result.content


def batch_scrape_simple(
    urls: list, output_dir: str = "scraped_content", max_workers: int = 5
) -> dict:
    """
    Fun√ß√£o de conveni√™ncia para scraping em lote.

    Args:
        urls: Lista de URLs
        output_dir: Diret√≥rio de sa√≠da
        max_workers: N√∫mero m√°ximo de workers

    Returns:
        dict: Dicion√°rio com status de sucesso para cada URL
    """
    if not SCRAPER_AVAILABLE or DoclingWebScraper is None:
        raise WebScrapingError(
            "DoclingWebScraper n√£o est√° dispon√≠vel. Verifique as depend√™ncias."
        )

    scraper = DoclingWebScraper()
    results = scraper.batch_scrape(urls, output_dir, max_workers)

    return {result.url: result.success for result in results}


def create_custom_scraper(
    timeout: int = 30,
    user_agent: str | None = None,
    clean_html: bool = True,
    include_images: bool = True,
    enable_cache: bool = False,
    **kwargs,
):
    """
    Cria um scraper customizado com configura√ß√µes espec√≠ficas.

    Args:
        timeout: Timeout para requisi√ß√µes
        user_agent: User agent customizado
        clean_html: Se deve limpar HTML
        include_images: Se deve incluir imagens
        enable_cache: Se deve habilitar cache
        **kwargs: Outros par√¢metros de configura√ß√£o

    Returns:
        DoclingWebScraper: Inst√¢ncia configurada
    """
    if not SCRAPER_AVAILABLE or DoclingWebScraper is None:
        raise WebScrapingError(
            "DoclingWebScraper n√£o est√° dispon√≠vel. Verifique as depend√™ncias."
        )

    config = ScrapingConfig(
        timeout=timeout,
        user_agent=user_agent or global_config.default_user_agent,
        clean_html=clean_html,
        include_images=include_images,
        enable_cache=enable_cache,
        **kwargs,
    )

    return DoclingWebScraper(config)


# Fun√ß√µes de OCR foram movidas para o m√≥dulo independente appserver_sdk_python_ai.ocr
# Para usar OCR, importe diretamente:
# from appserver_sdk_python_ai.ocr import quick_ocr, batch_ocr, create_custom_ocr_processor


def _ocr_deprecated_warning():
    """Aviso sobre a mudan√ßa do OCR para m√≥dulo independente."""
    import warnings

    warnings.warn(
        "As fun√ß√µes de OCR foram movidas para o m√≥dulo independente 'appserver_sdk_python_ai.ocr'. "
        "Use: from appserver_sdk_python_ai.ocr import quick_ocr, batch_ocr, create_custom_ocr_processor",
        DeprecationWarning,
        stacklevel=3,
    )


def process_pdf_with_ocr(
    pdf_path: str,
    output_file: str | None = None,
    extract_images: bool = True,
    extract_tables: bool = True,
    **kwargs,
) -> "ScrapingResult":
    """
    Processa um PDF com OCR e extra√ß√£o de imagens/tabelas usando Docling.

    Args:
        pdf_path: Caminho para o arquivo PDF
        output_file: Caminho opcional para salvar o markdown
        extract_images: Se deve extrair imagens
        extract_tables: Se deve extrair tabelas
        **kwargs: Argumentos adicionais para o scraper

    Returns:
        ScrapingResult: Resultado do processamento

    Raises:
        ConversionError: Se o Docling n√£o estiver dispon√≠vel
        ValidationError: Se o arquivo n√£o for encontrado
    """
    if not SCRAPER_AVAILABLE or DoclingWebScraper is None:
        raise WebScrapingError(
            "DoclingWebScraper n√£o est√° dispon√≠vel. Verifique as depend√™ncias."
        )

    scraper = DoclingWebScraper(**kwargs)
    return scraper.process_pdf_with_ocr(
        pdf_path=pdf_path,
        output_file=output_file,
        extract_images=extract_images,
        extract_tables=extract_tables,
    )


def batch_process_pdfs(
    pdf_paths: list[str],
    output_dir: str | None = None,
    max_workers: int = 3,
    extract_images: bool = True,
    extract_tables: bool = True,
    progress_callback=None,
    **kwargs,
) -> list["ScrapingResult"]:
    """
    Processa m√∫ltiplos PDFs em paralelo com OCR.

    Args:
        pdf_paths: Lista de caminhos para PDFs
        output_dir: Diret√≥rio de sa√≠da (opcional)
        max_workers: N√∫mero m√°ximo de threads
        extract_images: Se deve extrair imagens
        extract_tables: Se deve extrair tabelas
        progress_callback: Callback para acompanhar progresso
        **kwargs: Argumentos adicionais para o scraper

    Returns:
        List[ScrapingResult]: Lista de resultados
    """
    if not SCRAPER_AVAILABLE or DoclingWebScraper is None:
        raise WebScrapingError(
            "DoclingWebScraper n√£o est√° dispon√≠vel. Verifique as depend√™ncias."
        )

    scraper = DoclingWebScraper(**kwargs)
    return scraper.batch_process_pdfs(
        pdf_paths=pdf_paths,
        output_dir=output_dir,
        max_workers=max_workers,
        extract_images=extract_images,
        extract_tables=extract_tables,
        progress_callback=progress_callback,
    )


# Fun√ß√µes de informa√ß√£o
def get_version_info():
    """Retorna informa√ß√µes sobre a vers√£o e depend√™ncias."""
    import sys

    return {
        "webscraping_version": __version__,
        "docling_available": DOCLING_AVAILABLE,
        "docling_version": DOCLING_VERSION,
        "python_version": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
    }


def check_dependencies():
    """Verifica se todas as depend√™ncias est√£o instaladas."""
    dependencies = {
        "requests": None,
        "beautifulsoup4": None,
        "lxml": None,
        "docling": None,
    }

    for dep in dependencies:
        try:
            if dep == "beautifulsoup4":
                import bs4

                dependencies[dep] = getattr(bs4, "__version__", "installed")
            elif dep == "docling":
                import docling

                dependencies[dep] = getattr(docling, "__version__", "installed")
            else:
                module = __import__(dep)
                dependencies[dep] = getattr(module, "__version__", "installed")
        except ImportError:
            dependencies[dep] = "NOT_INSTALLED"

    return dependencies


def health_check():
    """Verifica a sa√∫de do m√≥dulo e suas depend√™ncias."""
    health = {
        "status": "OK",
        "version": __version__,
        "dependencies": check_dependencies(),
        "features": {
            "docling_conversion": DOCLING_AVAILABLE,
            "cache_system": True,
            "batch_processing": True,
        },
        "issues": [],
    }

    deps = health["dependencies"]

    # Verificar depend√™ncias cr√≠ticas
    if deps["requests"] == "NOT_INSTALLED":
        health["status"] = "ERROR"
        health["issues"].append("requests n√£o est√° instalado")

    if deps["beautifulsoup4"] == "NOT_INSTALLED":
        health["status"] = "ERROR"
        health["issues"].append("beautifulsoup4 n√£o est√° instalado")

    # Verificar depend√™ncias opcionais
    if deps["lxml"] == "NOT_INSTALLED":
        if health["status"] != "ERROR":
            health["status"] = "WARNING"
        health["issues"].append(
            "lxml n√£o est√° instalado (parser HTML pode ser mais lento)"
        )

    if deps["docling"] == "NOT_INSTALLED":
        if health["status"] != "ERROR":
            health["status"] = "WARNING"
        health["issues"].append(
            "docling n√£o est√° instalado (convers√£o b√°sica ser√° usada)"
        )

    return health


def print_status():
    """Imprime status do m√≥dulo."""
    health = health_check()

    print("=" * 60)
    print("M√ìDULO WEB SCRAPING - appserver_sdk_python_ai")
    print("=" * 60)
    print(f"Vers√£o: {__version__}")
    print(f"Status: {health['status']}")
    print(f"Docling: {'‚úÖ Dispon√≠vel' if DOCLING_AVAILABLE else '‚ùå N√£o dispon√≠vel'}")
    print(f"OCR: {'‚úÖ Dispon√≠vel' if OCR_AVAILABLE else '‚ùå N√£o dispon√≠vel'}")

    print("\nüì¶ Depend√™ncias:")
    deps = health["dependencies"]

    # Depend√™ncias principais
    print("  üìã Principais:")
    main_deps = ["requests", "beautifulsoup4", "lxml", "docling"]
    for dep in main_deps:
        if dep in deps:
            status = "‚úÖ" if deps[dep] != "NOT_INSTALLED" else "‚ùå"
            version_str = deps[dep] if deps[dep] != "NOT_INSTALLED" else "N√ÉO INSTALADO"
            print(f"    {status} {dep}: {version_str}")

    # OCR foi movido para m√≥dulo independente
    print("  üîç OCR: Movido para m√≥dulo independente 'appserver_sdk_python_ai.ocr'")

    if health["issues"]:
        print("\n‚ö†Ô∏è Problemas encontrados:")
        for issue in health["issues"]:
            print(f"  ‚Ä¢ {issue}")

    print("\nüöÄ Recursos dispon√≠veis:")
    for feature, available in health["features"].items():
        status = "‚úÖ" if available else "‚ùå"
        print(f"  {status} {feature.replace('_', ' ').title()}")

    print("=" * 60)


# Configura√ß√£o de logging


def setup_logging(level=logging.INFO, format_string=None):
    """
    Configura logging para o m√≥dulo.

    Args:
        level: N√≠vel de logging
        format_string: Formato customizado para logs
    """
    if format_string is None:
        format_string = global_config.log_format

    logging.basicConfig(
        level=level, format=format_string, handlers=[logging.StreamHandler()]
    )


# Exportar tudo necess√°rio
__all__ = [
    # Classes principais
    "DoclingWebScraper",
    "ScrapingConfig",
    "DoclingConfig",
    "GlobalWebScrapingConfig",
    "global_config",
    # Modelos
    "ScrapingResult",
    "BatchScrapingResult",
    "WebPageMetadata",
    "CacheEntry",
    "ScrapingStatus",
    # Exce√ß√µes
    "WebScrapingError",
    "ConversionError",
    "NetworkError",
    "ValidationError",
    "CacheError",
    "RateLimitError",
    "AuthenticationError",
    "ContentTooLargeError",
    "UnsupportedFormatError",
    # Utilit√°rios
    "ContentCleaner",
    "CacheManager",
    "MemoryCache",
    "URLValidator",
    "ContentValidator",
    "RobotsTxtChecker",
    # Fun√ß√µes de conveni√™ncia
    "quick_scrape",
    "batch_scrape_simple",
    "create_custom_scraper",
    "process_pdf_with_ocr",
    "batch_process_pdfs",
    # Informa√ß√µes e configura√ß√£o
    "get_version_info",
    "check_dependencies",
    "health_check",
    "print_status",
    "setup_logging",
    # Constantes
    "__version__",
    "DOCLING_AVAILABLE",
    "DOCLING_VERSION",
    "OCR_AVAILABLE",  # Mantido para compatibilidade
]

# Inicializa√ß√£o autom√°tica
logger = logging.getLogger(__name__)
logger.info(f"M√≥dulo webscraping v{__version__} carregado")

if not DOCLING_AVAILABLE:
    logger.warning("Docling n√£o dispon√≠vel - usando convers√£o b√°sica")
