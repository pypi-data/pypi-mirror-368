import time
from typing import Any, Dict, List, Literal, Optional, Union
from pydantic import BaseModel, ConfigDict, Field
import shortuuid
from openai.types.moderation import Moderation, Categories, CategoryScores
from openai.types.moderation_create_response import ModerationCreateResponse


class Categories(Categories):
    model_config = ConfigDict(
        populate_by_name=True,
    )


class CategoryScores(CategoryScores):
    model_config = ConfigDict(
        populate_by_name=True,
    )


class ModelPermission(BaseModel):
    id: str = Field(default_factory=lambda: f"modelperm-{shortuuid.random()}")
    object: str = "model_permission"
    created: int = Field(default_factory=lambda: int(time.time()))
    allow_create_engine: bool = False
    allow_sampling: bool = True
    allow_logprobs: bool = True
    allow_search_indices: bool = True
    allow_view: bool = True
    allow_fine_tuning: bool = False
    organization: str = "*"
    group: Optional[str] = None
    is_blocking: str = False


class ModelCard(BaseModel):
    id: str
    object: str = "model"
    created: int = Field(default_factory=lambda: int(time.time()))
    owned_by: str = "fastchat"
    root: Optional[str] = None
    parent: Optional[str] = None
    img_url: Optional[str] = None
    is_vip: bool = False
    is_outside: bool = False
    is_online: bool = True
    is_free: bool = False
    permission: List[ModelPermission] = []


class ModelList(BaseModel):
    object: str = "list"
    data: List[ModelCard] = []


class OpenAIModel(BaseModel):
    id: str = None
    name: str = None
    maxLength: int = None
    tokenLimit: int = None


class ChatMessageContentImageUrlModel(BaseModel):
    url: Optional[str] = None
    detail: Optional[Literal["high", "low"]] = "low"


class ChatMessageContentItemModel(BaseModel):
    type: Literal["text", "image_url"]
    text: Optional[str] = None
    image_url: Optional[ChatMessageContentImageUrlModel] = None


class FunctionCall(BaseModel):
    arguments: str
    """
    The arguments to call the function with, as generated by the model in JSON
    format. Note that the model does not always generate valid JSON, and may
    hallucinate parameters not defined by your function schema. Validate the
    arguments in your code before calling your function.
    """

    name: str
    """The name of the function to call."""


class ChatMessage(BaseModel):
    role: Literal["user", "assistant", "system", "tool", "function"]
    content: Union[str, List[ChatMessageContentItemModel], None]
    function_call: Optional[FunctionCall] = None
    name: str = None
    # maybe tool_call


class DeltaMessage(BaseModel):
    role: Optional[Literal["user", "assistant", "system"]] = None
    content: Optional[str] = None


class ChatCompletionRequest(BaseModel):
    # model: OpenAIModel
    model: str
    messages: List[ChatMessage]
    functions: Optional[List[Dict]] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    top_k: Optional[int] = None
    max_new_tokens: Optional[int] = None
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False
    plugin: Optional[str] = None
    user: Optional[str] = None
    meta_data: Optional[str] = None  # stores kb_name etc.
    stop: Optional[List[str]] = None


class FeedbackRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    score: Optional[float] = None
    max_new_tokens: Optional[int] = None
    stream: Optional[bool] = False
    plugin: Optional[str] = None
    user: Optional[str] = None
    client: Optional[str] = None


class ChatCompletionResponseChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: Literal["stop", "length", "function_call"]


class ChatCompletionResponseStreamChoice(BaseModel):
    index: int
    delta: DeltaMessage
    finish_reason: Optional[Literal["stop", "length"]]


class ChatCompletionResponse(BaseModel):
    id: str
    model: str
    object: Literal["chat.completion", "chat.completion.chunk"]
    choices: List[
        Union[ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice]
    ]
    created: Optional[int] = Field(default_factory=lambda: int(time.time()))


class EmbeddingRequest(BaseModel):
    model: str
    input: Union[str, List[str]]


class Embedding(BaseModel):
    embedding: List[float]
    """The embedding vector, which is a list of floats.

    The length of vector depends on the model as listed in the
    [embedding guide](https://platform.openai.com/docs/guides/embeddings).
    """

    index: int
    """The index of the embedding in the list of embeddings."""

    object: Literal["embedding"]


class Usage(BaseModel):
    prompt_tokens: int
    """The number of tokens used by the prompt."""

    total_tokens: int
    """The total number of tokens used by the request."""


class CreateEmbeddingResponse(BaseModel):
    data: List[Embedding]
    """The list of embeddings generated by the model."""

    model: str
    """The name of the model used to generate the embedding."""

    object: Literal["list"]
    """The object type, which is always "list"."""

    usage: Usage
    """The usage information for the request."""


class CompletionUsage(BaseModel):
    completion_tokens: int
    """Number of tokens in the generated completion."""
    prompt_tokens: int
    """Number of tokens in the prompt."""
    total_tokens: int
    """Total number of tokens used in the request (prompt + completion)."""
