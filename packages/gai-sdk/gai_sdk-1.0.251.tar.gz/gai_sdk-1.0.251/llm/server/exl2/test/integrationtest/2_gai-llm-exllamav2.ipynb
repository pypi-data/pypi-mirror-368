{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gai Chat Server (ExLlama2)\n",
                "\n",
                "**important: Select venv Python Interpreter before you start**\n",
                "\n",
                "This repository is designed to be used with Visual Studio Code and Docker DevContainer.\n",
                "\n",
                "![dev-container](../../img/dev-container.png)\n",
                "\n",
                "## 1. Setup\n",
                "\n",
                "**Instructions:**\n",
                "\n",
                "a) Download model\n",
                "\n",
                "```bash\n",
                "huggingface-cli download bartowski/Llama-3.2-3B-Instruct-exl2 \\\n",
                "    --revision c08d657b27cf0450deaddc3e582be20beec3e62d \\\n",
                "    --local-dir ~/.gai/models/Llama-3.2-3B-Instruct-exl2 \\\n",
                "    --local-dir-use-symlinks False\n",
                "```\n",
                "\n",
                "b) Create gai.yml in ~/.gai\n",
                "\n",
                "```yaml\n",
                "generators:\n",
                "    ttt:\n",
                "        type: ttt\n",
                "        engine: exllamav2\n",
                "        model: llama3.2:3b\n",
                "        name: ttt\n",
                "        hyperparameters:\n",
                "            temperature: 0.85\n",
                "            top_p: 0.8\n",
                "            top_k: 50\n",
                "            max_tokens: 1000\n",
                "            tool_choice: auto\n",
                "            max_retries: 5\n",
                "            stop:\n",
                "            - <|im_end|>\n",
                "            - </s>\n",
                "            - '[/INST]'\n",
                "        extra:\n",
                "            model_path: models/Llama-3.2-3B-Instruct-exl2\n",
                "            max_seq_len: 8192\n",
                "            prompt_format: mistral\n",
                "            no_flash_attn: true\n",
                "            seed: null\n",
                "            decode_special_tokens: false\n",
                "        module:\n",
                "            name: gai.llm.server.gai_exllamav2\n",
                "            class: GaiExLlamav2\n",
                "        source:\n",
                "            type: huggingface\n",
                "            repo_id: bartowski/Llama-3.2-3B-Instruct-exl2\n",
                "            local_dir: Llama-3.2-3B-Instruct-exl2\n",
                "            revision: c08d657b27cf0450deaddc3e582be20beec3e62d\n",
                "            file: null\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Pull Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/workspaces/gai-llm-svr-exl2/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "/workspaces/gai-llm-svr-exl2/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
                        "  warnings.warn(\n",
                        "Fetching 12 files:   8%|‚ñä         | 1/12 [00:00<00:08,  1.33it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Download status: {'progress': 8.333333333333332, 'current': 1, 'total': 12, 'message': 'Downloading'}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Fetching 12 files:  33%|‚ñà‚ñà‚ñà‚ñé      | 4/12 [00:01<00:02,  3.99it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Download status: {'progress': 33.33333333333333, 'current': 4, 'total': 12, 'message': 'Downloading'}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Fetching 12 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [04:15<00:00, 21.27s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Download status: {'progress': 75.0, 'current': 9, 'total': 12, 'message': 'Downloading'}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'/home/vscode/.gai/models/Llama-3.2-3B-Instruct-exl2'"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Takes around 7 minutes to run under normal conditions\n",
                "from gai.llm.lib.generators_utils import download, text_progress_callback\n",
                "download(name_or_config=\"llama3.2:8bpw:exl2\", status_callback=text_progress_callback)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Smoke Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        ".gairc =  {\"app_dir\":\"/home/vscode/.gai\"}\n",
                        "\n",
                        "‚úÖ  Before loading\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">0.76</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> GB</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "Free memory: \u001b[1;91m0.76\u001b[0m\u001b[91m GB\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Model loaded in subprocess\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">1.05</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> GB</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "Free memory: \u001b[1;91m1.05\u001b[0m\u001b[91m GB\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ñ∂Ô∏è  Response: ChatCompletion(id='chatcmpl-b5d1a348-fa25-4847-8511-c1a858353a26', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\\nA long time ago, in a land far away, there was a fearsome dragon. This dragon was not like any other dragon that had come before it. It was the most feared creature in all the land, but for a very good reason. The dragon was incredibly smart, and had an insatiable thirst for knowledge. It spent all of its days studying the ancient texts and devouring books on all manner of subjects. This dragon was unlike any other, as it sought to use its power for good.\\n', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1748236281, model='exllamav2-mistral7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=104, prompt_tokens=14, total_tokens=118, completion_tokens_details=None, prompt_tokens_details=None))\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">6.74</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> GB</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "Free memory: \u001b[1;92m6.74\u001b[0m\u001b[92m GB\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üóëÔ∏è  Model unloaded, GPU memory freed\n"
                    ]
                }
            ],
            "source": [
                "# 1) Confirm gai initialized\n",
                "\n",
                "import os\n",
                "gairc=None\n",
                "with open(os.path.expanduser(\"~/.gairc\"),\"r\") as f:\n",
                "    gairc = f.read()\n",
                "print(\".gairc = \",gairc)\n",
                "assert os.path.exists(os.path.expanduser(\"~/.gai\"))\n",
                "\n",
                "# 2) Build generator configuration\n",
                "\n",
                "from gai.lib.config import config_helper\n",
                "yaml_config = \"\"\"\n",
                "# bpw: 8 bits, size: 3.98 GB\n",
                "type: \"ttt\"\n",
                "engine: \"exllamav2\"\n",
                "model: \"dolphin3.0_llama3.1:4.25bpw\"\n",
                "name: \"dolphin3.0_llama3.1:4.25bpw:exl2\"\n",
                "extra:\n",
                "    model_path: \"models/Dolphin3.0-Llama3.1-8B-4_25bpw-exl2\"\n",
                "    max_seq_len: 8192\n",
                "    prompt_format: \"llama\"\n",
                "    no_flash_attn: true\n",
                "    seed: null\n",
                "    decode_special_tokens: false\n",
                "hyperparameters:\n",
                "    temperature: 0.85\n",
                "    top_p: 0.8\n",
                "    top_k: 50\n",
                "    max_tokens: 1000\n",
                "    tool_choice: \"auto\"\n",
                "    max_retries: 5\n",
                "    stop: [\"<|im_end|>\", \"</s>\", \"[/INST]\"]\n",
                "module:\n",
                "    name: \"gai.llm.server.gai_exllamav2\"\n",
                "    class: \"GaiExLlamav2\"\n",
                "source:\n",
                "    type: \"huggingface\"\n",
                "    repo_id: \"bartowski/Dolphin3.0-Llama3.1-8B-exl2\"\n",
                "    local_dir: \"Dolphin3.0-Llama3.1-8B-4_25bpw-exl2\"\n",
                "    revision: \"896301e945342d032ef0b3a81b57f0d5a8bac6fe\"\n",
                "\"\"\"\n",
                "\n",
                "import yaml\n",
                "generator_config = yaml.safe_load(yaml_config)\n",
                "generator_config = config_helper.get_generator_config(generator_config)\n",
                "\n",
                "print(\"‚úÖ  Before loading\")\n",
                "from gai.lib.diagnostics import free_mem\n",
                "free_mem()\n",
                "\n",
                "from gai.lib.api import SingletonHost\n",
                "from rich.console import Console\n",
                "console=Console()\n",
                "\n",
                "# 3) Acquire (or create) the singleton\n",
                "host = SingletonHost.GetInstanceFromConfig(generator_config)\n",
                "\n",
                "# 4) Load the generator into child process\n",
                "host.load()\n",
                "print(\"‚úÖ Model loaded in subprocess\")\n",
                "free_mem()\n",
                "\n",
                "# 5) Generate a response\n",
                "response = host.create(\n",
                "    model=\"ttt\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"Tell me a one paragraph story about a dragon\"}],\n",
                "    stream=False\n",
                ")\n",
                "print(\"‚ñ∂Ô∏è  Response:\", response)\n",
                "\n",
                "# 6) Unload to tear down the child and free GPU memory\n",
                "host.unload()\n",
                "free_mem()\n",
                "print(\"üóëÔ∏è  Model unloaded, GPU memory freed\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Integration Test"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### a) Startup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ  Before loading\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">6.71</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> GB</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "Free memory: \u001b[1;92m6.71\u001b[0m\u001b[92m GB\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Model loaded in subprocess\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">1.11</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> GB</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "Free memory: \u001b[1;91m1.11\u001b[0m\u001b[91m GB\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "1.1096267700195312"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import os\n",
                "os.environ[\"LOG_LEVEL\"] = \"INFO\"\n",
                "\n",
                "# convert yaml to json\n",
                "import yaml\n",
                "yaml_config = \"\"\"\n",
                "# bpw: 8 bits, size: 3.98 GB\n",
                "type: ttt\n",
                "engine: exllamav2\n",
                "model: dolphin3.0_llama3.1:4.25bpw\n",
                "name: dolphin3.0_llama3.1:4.25bpw:exl2\n",
                "hyperparameters:\n",
                "    temperature: 0.85\n",
                "    top_p: 0.8\n",
                "    top_k: 50\n",
                "    max_tokens: 1000\n",
                "    tool_choice: auto\n",
                "    max_retries: 5\n",
                "    stop:\n",
                "        - <|im_end|>\n",
                "        - </s>\n",
                "        - \"[/INST]\"\n",
                "extra:\n",
                "    model_path: models/Dolphin3.0-Llama3.1-8B-4_25bpw-exl2\n",
                "    max_seq_len: 8192\n",
                "    prompt_format: llama\n",
                "    no_flash_attn: true\n",
                "    seed: null\n",
                "    decode_special_tokens: false\n",
                "module:\n",
                "    name: gai.llm.server.gai_exllamav2\n",
                "    class: GaiExLlamav2\n",
                "source:\n",
                "    type: huggingface\n",
                "    repo_id: bartowski/Dolphin3.0-Llama3.1-8B-exl2\n",
                "    local_dir: Dolphin3.0-Llama3.1-8B-4_25bpw-exl2\n",
                "    revision: 896301e945342d032ef0b3a81b57f0d5a8bac6fe\n",
                "\"\"\"\n",
                "\n",
                "import yaml\n",
                "generator_config = yaml.safe_load(yaml_config)\n",
                "from gai.lib.config import config_helper\n",
                "generator_config = config_helper.get_generator_config(generator_config)\n",
                "\n",
                "print(\"‚úÖ  Before loading\")\n",
                "from gai.lib.diagnostics import free_mem\n",
                "free_mem()\n",
                "\n",
                "from gai.lib.api import SingletonHost\n",
                "host = SingletonHost.GetInstanceFromConfig(generator_config, verbose=False)\n",
                "host.load()\n",
                "print(\"‚úÖ Model loaded in subprocess\")\n",
                "free_mem()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### a) Test streaming"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Once upon a time, in a small village nestled between two great mountains, there lived a young girl named Lila. She was known throughout the village for her kindness and generosity, always ready to lend a helping hand or offer a warm smile to those in need. One day, while out gathering herbs in the nearby forest, Lila stumbled upon a wounded bird, its wing badly injured and unable to fly. Moved by compassion, Lila carefully picked up the bird and took it back to her home, where she tended to its wounds with care and love. As the days passed, the bird slowly healed under Lila's gentle care, until one day it was able to fly again. Overjoyed, Lila released the bird back into the wild, watching as it soared high into the sky. From that day on, the villagers spoke of Lila's kindness and how it had brought joy and healing to all who knew her."
                    ]
                }
            ],
            "source": [
                "response = host.create(\n",
                "    model=\"ttt\",\n",
                "    messages=[{\"role\":\"user\",\"content\":\"Tell me a one paragraph story\"},\n",
                "                {\"role\":\"assistant\",\"content\":\"\"}],\n",
                "    stream=True)\n",
                "for message in response:\n",
                "    if message.choices[0].delta.content:\n",
                "        print(message.choices[0].delta.content, end=\"\", flush=True)\n",
                "   "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### b) Test generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Once upon a time, in a land far away, there was a young girl named Lily. She lived in a small village surrounded by a dense forest. One day, she decided to explore the forest and stumbled upon a magical tree. The tree had shimmering leaves, and when Lily touched it, she was transported to a magical world. In this world, animals could talk, and the sky was filled with colorful birds. Lily spent her days in this magical world, learning about the animals and their ways\n",
                        "finish reason: length\n"
                    ]
                }
            ],
            "source": [
                "response = host.create(\n",
                "    model=\"ttt\",\n",
                "    messages=[{\"role\":\"user\",\"content\":\"Tell me a one paragraph story\"},\n",
                "                {\"role\":\"assistant\",\"content\":\"\"}],\n",
                "    stream=False,\n",
                "    max_tokens=100)\n",
                "print(response.choices[0].message.content)\n",
                "print(\"finish reason:\", response.choices[0].finish_reason)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### c) Test Tool Calling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ChatCompletion(id='chatcmpl-d3baabf6-af5c-414a-b3f3-351273f04b22', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_08e932bd-276f-4d4a-9d2f-aa4365b09ef9', function=Function(arguments='{\"search_query\": \"current time in Singapore\"}', name='google'), type='function')]))], created=1748236397, model='exllamav2-mistral7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=23, prompt_tokens=367, total_tokens=390, completion_tokens_details=None, prompt_tokens_details=None))\n"
                    ]
                }
            ],
            "source": [
                "response = host.create(\n",
                "    model=\"ttt\",\n",
                "    messages=[\n",
                "        {\"role\":\"user\",\"content\":\"What is the current time in Singapore?\"},\n",
                "        {\"role\":\"assistant\",\"content\":\"\"}\n",
                "    ],\n",
                "    tool_choice=\"required\",\n",
                "    tools=[\n",
                "        {\n",
                "            \"type\": \"function\",\n",
                "            \"function\": {\n",
                "                \"name\": \"google\",\n",
                "                \"description\": \"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
                "                \"parameters\": {\n",
                "                    \"type\": \"object\",\n",
                "                    \"properties\": {\n",
                "                        \"search_query\": {\n",
                "                            \"type\": \"string\",\n",
                "                            \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
                "                        }\n",
                "                    },\n",
                "                    \"required\": [\"search_query\"]\n",
                "                }\n",
                "            }\n",
                "        }\n",
                "    ],\n",
                "    stream=False)\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### d) Test Structured Output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_3396/4278824633.py:20: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
                        "  json_schema=Book.schema(),\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ChatCompletion(id='chatcmpl-6e8b8155-0683-4b7e-9164-678c88bddb15', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"title\": \"Foundation\",\\n  \"summary\": \"Foundation is a science fiction novel by American writer Isaac Asimov. It is the first published in his Foundation Trilogy (later expanded into the Foundation series). Foundation is a cycle of five interrelated short stories, first published as a single book by Gnome Press in 1951. Collectively they tell the early story of the Foundation, an institute founded by psychohistorian Hari Seldon to preserve the best of galactic civilization after the collapse of the Galactic Empire.\",\\n  \"author\": \"Isaac Asimov\",\\n  \"published_year\": 1951\\n}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1748236407, model='exllamav2-mistral7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=128, prompt_tokens=306, total_tokens=434, completion_tokens_details=None, prompt_tokens_details=None))\n"
                    ]
                }
            ],
            "source": [
                "# Define Schema\n",
                "from pydantic import BaseModel\n",
                "class Book(BaseModel):\n",
                "    title: str\n",
                "    summary: str\n",
                "    author: str\n",
                "    published_year: int\n",
                "\n",
                "text = \"\"\"Foundation is a science fiction novel by American writer\n",
                "Isaac Asimov. It is the first published in his Foundation Trilogy (later\n",
                "expanded into the Foundation series). Foundation is a cycle of five\n",
                "interrelated short stories, first published as a single book by Gnome Press\n",
                "in 1951. Collectively they tell the early story of the Foundation,\n",
                "an institute founded by psychohistorian Hari Seldon to preserve the best\n",
                "of galactic civilization after the collapse of the Galactic Empire.\n",
                "\"\"\"\n",
                "response = host.create(\n",
                "    model=\"ttt\",\n",
                "    messages=[{'role':'user','content':text},{'role':'assistant','content':''}], \n",
                "    json_schema=Book.schema(),\n",
                "    stream=False\n",
                "    )\n",
                "print(response)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Switch Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Once upon a time, in a small village, there lived a brave knight named Sir Thomas. He was known far and wide for his courage and kindness. One day, a fierce dragon attacked the village, burning everything in its path. The villagers were terrified and didn't know what to do. But Sir Thomas didn't hesitate. He picked up his sword and rode towards the dragon, vowing to protect his people.\n",
                        "\n",
                        " The battle was long and fierce, but Sir Thomas was determined. With every slash of his sword, the dragon grew weaker and weaker. Finally, after what felt like an eternity, the dragon fell to the ground, defeated. The villagers cheered in joy, and Sir Thomas was hailed as a hero. From that day on, peace and prosperity returned to the village, and Sir Thomas was remembered as their savior. "
                    ]
                }
            ],
            "source": [
                "# Just call using a different model name and the singleton host will switch to the new model\n",
                "\n",
                "response = host.create(\n",
                "    model=\"dolphin_mistral:exl2\",\n",
                "    messages=[{\"role\":\"user\",\"content\":\"Tell me a one paragraph story\"},\n",
                "                {\"role\":\"assistant\",\"content\":\"\"}],\n",
                "    stream=True)\n",
                "for message in response:\n",
                "    if message.choices[0].delta.content:\n",
                "        print(message.choices[0].delta.content, end=\"\", flush=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Teardown"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">4.30</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> GB</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "Free memory: \u001b[1;92m4.30\u001b[0m\u001b[92m GB\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "4.303363800048828"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "host.unload()\n",
                "free_mem()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. API Test\n",
                "\n",
                "**Instructions**:\n",
                "\n",
                "a) Open Debug Icon and select **Python Debugger: gai-ttt server (dolphin)**\n",
                "\n",
                "b) Press `F5` to start the API server.\n",
                "\n",
                "c) Wait for the server to start.\n",
                "\n",
                "\n",
                "**Tests**:\n",
                "\n",
                "Run the following cells to test the API."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### a) Test Pulling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "null"
                    ]
                }
            ],
            "source": [
                "%%bash\n",
                "curl -X POST \\\n",
                "    http://localhost:12031/gen/v1/chat/pull \\\n",
                "    -H 'Content-Type: application/json' \\\n",
                "    -s \\\n",
                "    -N \\\n",
                "    -d \"{\\\"model\\\":\\\"dolphin_mistral:exl2\\\"}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### a) Test Generating"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{\"id\":\"chatcmpl-9787d061-1553-4775-b6d5-b9edbcba10bd\",\"choices\":[{\"finish_reason\":\"length\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"Once upon a time, in a small village nestled between two great mountains, there lived a young girl named Aria. She was a curious and adventurous soul, always eager to explore the world around her. One day, while wandering through the forest, she stumbled upon a mysterious door hidden behind a waterfall.\\n\\nIntrigued, Aria pushed open the door and found herself in a magical realm filled with talking animals, enchanted trees, and sparkling fairy dust. As she ventured deeper into this strange new world\",\"refusal\":null,\"role\":\"assistant\",\"annotations\":null,\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1748236643,\"model\":\"exllamav2-mistral7b\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":100,\"prompt_tokens\":14,\"total_tokens\":114,\"completion_tokens_details\":null,\"prompt_tokens_details\":null}}"
                    ]
                }
            ],
            "source": [
                "%%bash\n",
                "curl -X POST \\\n",
                "    http://gai-llm-svr-exl2:12031/gen/v1/chat/completions \\\n",
                "    -H 'Content-Type: application/json' \\\n",
                "    -s \\\n",
                "    -N \\\n",
                "    -d \"{\\\"model\\\":\\\"ttt\\\", \\\n",
                "        \\\"messages\\\": [ \\\n",
                "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"Tell me a story.\\\"}, \\\n",
                "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
                "        ],\\\n",
                "        \\\"max_tokens\\\":100,\\\n",
                "        \\\"tool_choice\\\": \\\"none\\\"}\"\n",
                "        "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### b) Test Streaming"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Once upon a time, there was a young girl named Lily who loved to read. She spent most of her days curled up in her favorite chair with a book in her hands. One day, while she was reading, she heard a knock on the door"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "import httpx\n",
                "\n",
                "json_payload = {\n",
                "    \"model\":\"ttt\",\n",
                "    \"temperature\": 0.2,\n",
                "    \"max_tokens\": 50,\n",
                "    \"stream\": \"true\",  # This should probably be a boolean True, not \"true\"\n",
                "    \"messages\": [\n",
                "        {\n",
                "            \"role\": \"user\",\n",
                "            \"content\": \"Tell me a one paragraph story.\"\n",
                "        },\n",
                "        {\n",
                "            \"role\": \"assistant\",\n",
                "            \"content\": \"\"\n",
                "        }\n",
                "    ]\n",
                "}\n",
                "async def http_post_async(json_payload):\n",
                "\n",
                "    # Send the POST request using httpx with streaming\n",
                "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
                "        async with client.stream(\"POST\", \"http://gai-llm-svr-exl2:12031/gen/v1/chat/completions\", json=json_payload) as response:\n",
                "            response.raise_for_status()\n",
                "            async for chunk in response.aiter_text():  # Use aiter_text() to handle decoding\n",
                "                chunk=json.loads(chunk)\n",
                "                chunk=chunk[\"choices\"][0][\"delta\"][\"content\"]\n",
                "                if chunk:  # Check for non-empty chunks\n",
                "                    print(chunk, end=\"\", flush=True)\n",
                "\n",
                "try:\n",
                "    response=await http_post_async(json_payload)\n",
                "except httpx.HTTPStatusError as e:\n",
                "    print(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### c) Test Tool Calling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{\"id\":\"chatcmpl-ea3fa081-c13b-492f-82c6-7c1d43c895a3\",\"choices\":[{\"finish_reason\":\"tool_calls\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":null,\"refusal\":null,\"role\":\"assistant\",\"annotations\":null,\"audio\":null,\"function_call\":null,\"tool_calls\":[{\"id\":\"call_78c7e95a-3ebc-4021-b9a7-483df071ff45\",\"function\":{\"arguments\":\"{\\\"search_query\\\": \\\"current time in Singapore\\\"}\",\"name\":\"google\"},\"type\":\"function\"}]}}],\"created\":1748187141,\"model\":\"exllamav2-mistral7b\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":23,\"prompt_tokens\":366,\"total_tokens\":389,\"completion_tokens_details\":null,\"prompt_tokens_details\":null}}"
                    ]
                }
            ],
            "source": [
                "%%bash\n",
                "curl -X POST \\\n",
                "    http://gai-llm-svr-exl2:12031/gen/v1/chat/completions \\\n",
                "    -H 'Content-Type: application/json' \\\n",
                "    -s \\\n",
                "    -N \\\n",
                "    -d \"{\\\"model\\\":\\\"ttt\\\", \\\n",
                "        \\\"messages\\\": [ \\\n",
                "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"What is the current time in Singapore\\\"}, \\\n",
                "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
                "        ],\\\n",
                "        \\\"tools\\\": [\\\n",
                "            {\\\n",
                "                \\\"type\\\": \\\"function\\\",\\\n",
                "                \\\"function\\\": {\\\n",
                "                    \\\"name\\\": \\\"google\\\",\\\n",
                "                    \\\"description\\\": \\\"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\\\",\\\n",
                "                    \\\"parameters\\\": {\\\n",
                "                        \\\"type\\\": \\\"object\\\",\\\n",
                "                        \\\"properties\\\": {\\\n",
                "                            \\\"search_query\\\": {\\\n",
                "                                \\\"type\\\": \\\"string\\\",\\\n",
                "                                \\\"description\\\": \\\"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\\\"\\\n",
                "                            }\\\n",
                "                        },\\\n",
                "                        \\\"required\\\": [\\\"search_query\\\"]\\\n",
                "                    }\\\n",
                "                }\\\n",
                "            }\\\n",
                "        ],\\\n",
                "        \\\"tool_choice\\\": \\\"required\\\"}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### d) Test JSON Schema"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{\"id\":\"chatcmpl-d99d95ad-9418-4728-9e4d-25d2b40e0565\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"{\\n  \\\"title\\\": \\\"Foundation\\\",\\n  \\\"summary\\\": \\\"Foundation is a science fiction novel by American writer Isaac Asimov. It is the first published in his Foundation Trilogy (later expanded into the Foundation series). Foundation is a cycle of five interrelated short stories, first published as a single book by Gnome Press in 1951. Collectively they tell the early story of the Foundation, an institute founded by psychohistorian Hari Seldon to preserve the best of galactic civilization after the collapse of the Galactic Empire.\\\",\\n  \\\"author\\\": \\\"Isaac Asimov\\\",\\n  \\\"published_year\\\": 1951\\n}\",\"refusal\":null,\"role\":\"assistant\",\"annotations\":null,\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1748236684,\"model\":\"exllamav2-mistral7b\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":128,\"prompt_tokens\":305,\"total_tokens\":433,\"completion_tokens_details\":null,\"prompt_tokens_details\":null}}"
                    ]
                }
            ],
            "source": [
                "%%bash\n",
                "curl -X POST \\\n",
                "    http://gai-llm-svr-exl2:12031/gen/v1/chat/completions \\\n",
                "    -H 'Content-Type: application/json' \\\n",
                "    -s \\\n",
                "    -N \\\n",
                "    -d \"{\\\"model\\\":\\\"ttt\\\", \\\n",
                "        \\\"messages\\\": [ \\\n",
                "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"Foundation is a science fiction novel by American writer \\\n",
                "            Isaac Asimov. It is the first published in his Foundation Trilogy (later \\\n",
                "            expanded into the Foundation series). Foundation is a cycle of five \\\n",
                "            interrelated short stories, first published as a single book by Gnome Press \\\n",
                "            in 1951. Collectively they tell the early story of the Foundation, \\\n",
                "            an institute founded by psychohistorian Hari Seldon to preserve the best \\\n",
                "            of galactic civilization after the collapse of the Galactic Empire.\\\"}, \\\n",
                "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
                "        ],\\\n",
                "        \\\"json_schema\\\": {\\\"properties\\\": \\\n",
                "            {\\\"title\\\": \\\n",
                "                {\\\"title\\\": \\\"Title\\\", \\\"type\\\": \\\"string\\\"}, \\\n",
                "                    \\\"summary\\\": {\\\"title\\\": \\\"Summary\\\", \\\"type\\\": \\\"string\\\"}, \\\n",
                "                    \\\"author\\\": {\\\"title\\\": \\\"Author\\\", \\\n",
                "                    \\\"type\\\": \\\"string\\\"\\\n",
                "                }, \\\n",
                "                \\\"published_year\\\": {\\\n",
                "                    \\\"title\\\": \\\"Published Year\\\", \\\n",
                "                    \\\"type\\\": \\\"integer\\\"}}, \\\n",
                "                \\\"required\\\": [\\\n",
                "                    \\\"title\\\", \\\n",
                "                    \\\"summary\\\", \\\n",
                "                    \\\"author\\\", \\\n",
                "                    \\\"published_year\\\"\\\n",
                "                ], \\\n",
                "                \\\"title\\\": \\\"Book\\\", \\\n",
                "                \\\"type\\\": \\\"object\\\"\\\n",
                "            },\\\n",
                "        \\\"stream\\\": false }\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### e) Shut down the API Service"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
