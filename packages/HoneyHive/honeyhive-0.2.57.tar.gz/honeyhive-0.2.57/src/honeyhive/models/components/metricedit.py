"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from enum import Enum
from honeyhive.types import BaseModel
from typing import Any, Dict, List, Optional, TypedDict
from typing_extensions import NotRequired


class MetricEditType(str, Enum):
    r"""Type of the metric - \"custom\", \"model\", \"human\" or \"composite\" """

    CUSTOM = "custom"
    MODEL = "model"
    HUMAN = "human"
    COMPOSITE = "composite"


class MetricEditReturnType(str, Enum):
    r"""The data type of the metric value - \"boolean\", \"float\", \"string\" """

    BOOLEAN = "boolean"
    FLOAT = "float"
    STRING = "string"


class MetricEditThresholdTypedDict(TypedDict):
    r"""Threshold for numeric metrics to decide passing or failing in tests"""

    min: NotRequired[float]
    max: NotRequired[float]


class MetricEditThreshold(BaseModel):
    r"""Threshold for numeric metrics to decide passing or failing in tests"""

    min: Optional[float] = None

    max: Optional[float] = None


class MetricEditEventType(str, Enum):
    r"""Type of event that the metric is set to be computed on"""

    MODEL = "model"
    TOOL = "tool"
    CHAIN = "chain"
    SESSION = "session"


class MetricEditTypedDict(TypedDict):
    metric_id: str
    r"""Unique identifier of the metric"""
    criteria: NotRequired[str]
    r"""Criteria for human or composite metrics"""
    name: NotRequired[str]
    r"""Updated name of the metric"""
    description: NotRequired[str]
    r"""Short description of what the metric does"""
    code_snippet: NotRequired[str]
    r"""Updated code block for the metric"""
    prompt: NotRequired[str]
    r"""Updated Evaluator prompt for the metric"""
    type: NotRequired[MetricEditType]
    r"""Type of the metric - \"custom\", \"model\", \"human\" or \"composite\" """
    enabled_in_prod: NotRequired[bool]
    r"""Whether to compute on all production events automatically"""
    needs_ground_truth: NotRequired[bool]
    r"""Whether a ground truth (on metadata) is required to compute it"""
    return_type: NotRequired[MetricEditReturnType]
    r"""The data type of the metric value - \"boolean\", \"float\", \"string\" """
    threshold: NotRequired[MetricEditThresholdTypedDict]
    r"""Threshold for numeric metrics to decide passing or failing in tests"""
    pass_when: NotRequired[bool]
    r"""Threshold for boolean metrics to decide passing or failing in tests"""
    event_name: NotRequired[str]
    r"""Name of event that the metric is set to be computed on"""
    event_type: NotRequired[MetricEditEventType]
    r"""Type of event that the metric is set to be computed on"""
    model_provider: NotRequired[str]
    r"""Provider of the model, formatted as a LiteLLM provider prefix"""
    model_name: NotRequired[str]
    r"""Name of the model, formatted as a LiteLLM model name"""
    child_metrics: NotRequired[List[Dict[str, Any]]]
    r"""Child metrics added under composite events"""


class MetricEdit(BaseModel):
    metric_id: str
    r"""Unique identifier of the metric"""

    criteria: Optional[str] = None
    r"""Criteria for human or composite metrics"""

    name: Optional[str] = None
    r"""Updated name of the metric"""

    description: Optional[str] = None
    r"""Short description of what the metric does"""

    code_snippet: Optional[str] = None
    r"""Updated code block for the metric"""

    prompt: Optional[str] = None
    r"""Updated Evaluator prompt for the metric"""

    type: Optional[MetricEditType] = None
    r"""Type of the metric - \"custom\", \"model\", \"human\" or \"composite\" """

    enabled_in_prod: Optional[bool] = None
    r"""Whether to compute on all production events automatically"""

    needs_ground_truth: Optional[bool] = None
    r"""Whether a ground truth (on metadata) is required to compute it"""

    return_type: Optional[MetricEditReturnType] = None
    r"""The data type of the metric value - \"boolean\", \"float\", \"string\" """

    threshold: Optional[MetricEditThreshold] = None
    r"""Threshold for numeric metrics to decide passing or failing in tests"""

    pass_when: Optional[bool] = None
    r"""Threshold for boolean metrics to decide passing or failing in tests"""

    event_name: Optional[str] = None
    r"""Name of event that the metric is set to be computed on"""

    event_type: Optional[MetricEditEventType] = None
    r"""Type of event that the metric is set to be computed on"""

    model_provider: Optional[str] = None
    r"""Provider of the model, formatted as a LiteLLM provider prefix"""

    model_name: Optional[str] = None
    r"""Name of the model, formatted as a LiteLLM model name"""

    child_metrics: Optional[List[Dict[str, Any]]] = None
    r"""Child metrics added under composite events"""
