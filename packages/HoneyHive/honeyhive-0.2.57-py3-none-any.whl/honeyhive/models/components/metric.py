"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from enum import Enum
from honeyhive.types import BaseModel
import pydantic
from typing import Any, Dict, List, Optional, TypedDict
from typing_extensions import Annotated, NotRequired


class MetricType(str, Enum):
    r"""Type of the metric - \"custom\", \"model\", \"human\" or \"composite\" """

    CUSTOM = "custom"
    MODEL = "model"
    HUMAN = "human"
    COMPOSITE = "composite"


class ReturnType(str, Enum):
    r"""The data type of the metric value - \"boolean\", \"float\", \"string\" """

    BOOLEAN = "boolean"
    FLOAT = "float"
    STRING = "string"


class ThresholdTypedDict(TypedDict):
    r"""Threshold for numeric metrics to decide passing or failing in tests"""

    min: NotRequired[float]
    max: NotRequired[float]


class Threshold(BaseModel):
    r"""Threshold for numeric metrics to decide passing or failing in tests"""

    min: Optional[float] = None

    max: Optional[float] = None


class MetricTypedDict(TypedDict):
    name: str
    r"""Name of the metric"""
    task: str
    r"""Name of the project associated with metric"""
    type: MetricType
    r"""Type of the metric - \"custom\", \"model\", \"human\" or \"composite\" """
    description: str
    r"""Short description of what the metric does"""
    return_type: ReturnType
    r"""The data type of the metric value - \"boolean\", \"float\", \"string\" """
    criteria: NotRequired[str]
    r"""Criteria for human or composite metrics"""
    code_snippet: NotRequired[str]
    r"""Associated code block for the metric"""
    prompt: NotRequired[str]
    r"""Evaluator prompt for the metric"""
    enabled_in_prod: NotRequired[bool]
    r"""Whether to compute on all production events automatically"""
    needs_ground_truth: NotRequired[bool]
    r"""Whether a ground truth (on metadata) is required to compute it"""
    threshold: NotRequired[ThresholdTypedDict]
    r"""Threshold for numeric metrics to decide passing or failing in tests"""
    pass_when: NotRequired[bool]
    r"""Threshold for boolean metrics to decide passing or failing in tests"""
    id: NotRequired[str]
    r"""Unique idenitifier"""
    event_name: NotRequired[str]
    r"""Name of event that the metric is set to be computed on"""
    event_type: NotRequired[str]
    r"""Type of event that the metric is set to be computed on"""
    model_provider: NotRequired[str]
    r"""Provider of the model, formatted as a LiteLLM provider prefix"""
    model_name: NotRequired[str]
    r"""Name of the model, formatted as a LiteLLM model name"""
    child_metrics: NotRequired[List[Dict[str, Any]]]
    r"""Child metrics added under composite events"""


class Metric(BaseModel):
    name: str
    r"""Name of the metric"""

    task: str
    r"""Name of the project associated with metric"""

    type: MetricType
    r"""Type of the metric - \"custom\", \"model\", \"human\" or \"composite\" """

    description: str
    r"""Short description of what the metric does"""

    return_type: ReturnType
    r"""The data type of the metric value - \"boolean\", \"float\", \"string\" """

    criteria: Optional[str] = None
    r"""Criteria for human or composite metrics"""

    code_snippet: Optional[str] = None
    r"""Associated code block for the metric"""

    prompt: Optional[str] = None
    r"""Evaluator prompt for the metric"""

    enabled_in_prod: Optional[bool] = None
    r"""Whether to compute on all production events automatically"""

    needs_ground_truth: Optional[bool] = None
    r"""Whether a ground truth (on metadata) is required to compute it"""

    threshold: Optional[Threshold] = None
    r"""Threshold for numeric metrics to decide passing or failing in tests"""

    pass_when: Optional[bool] = None
    r"""Threshold for boolean metrics to decide passing or failing in tests"""

    id: Annotated[Optional[str], pydantic.Field(alias="_id")] = None
    r"""Unique idenitifier"""

    event_name: Optional[str] = None
    r"""Name of event that the metric is set to be computed on"""

    event_type: Optional[str] = None
    r"""Type of event that the metric is set to be computed on"""

    model_provider: Optional[str] = None
    r"""Provider of the model, formatted as a LiteLLM provider prefix"""

    model_name: Optional[str] = None
    r"""Name of the model, formatted as a LiteLLM model name"""

    child_metrics: Optional[List[Dict[str, Any]]] = None
    r"""Child metrics added under composite events"""
