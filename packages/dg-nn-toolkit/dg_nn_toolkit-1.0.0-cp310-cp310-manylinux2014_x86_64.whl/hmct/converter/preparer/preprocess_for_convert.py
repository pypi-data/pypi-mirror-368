import logging
from typing import Dict, Sequence

import numpy as np

from horizon_nn.common import InputDictParser, add_model_output
from horizon_nn.ir import DataType, OnnxModel, numpy_dtype_to_onnx_dtype
from horizon_nn.version import __version__

from .add_preprocess_node import add_preprocess_node


def preprocess_for_convert(
    onnx_model: OnnxModel,
    input_dict_parser: InputDictParser,
    output_nodes: Sequence[str],
) -> OnnxModel:
    """Apply patches to preprocess the ONNX model for validity."""
    # fix for invalid ONNX models generated by MXNet
    onnx_model = remove_spatial_attr_from_bn(onnx_model)
    onnx_model = modify_argmax_output_elemtype(onnx_model)
    # add horizon producer info to the model
    onnx_model = add_producer_info(onnx_model)
    # set shape info for PyOp node
    onnx_model = set_pyop_shape(onnx_model)
    # set model input shape to target input shape
    onnx_model = set_input_shape(
        onnx_model,
        input_shapes=input_dict_parser.get_input_shapes(),
    )
    # add preprocess node at model input
    onnx_model = add_preprocess_node(
        onnx_model=onnx_model,
        input_dict_parser=input_dict_parser,
    )
    # add node outputs to model outputs if needed
    if output_nodes:
        onnx_model = add_model_output(
            onnx_model=onnx_model,
            output_nodes=output_nodes,
            keep_original_output=False,
        )

    return onnx_model


def remove_spatial_attr_from_bn(onnx_model: OnnxModel) -> OnnxModel:
    """Remove spatial attribute from BatchNormalization node in high opset.

    If a onnx model is produced by old version MXNet, there is a bug in it.
    There will be a attr 'spatial' in BatchNormalization node in opset10/11.
    However, this attr has been deprecated by ONNX since opset9.

    In other words, this old-mxnet-generated onnx model is invalid and
    onnx.model_checker will return false. This func is used to remove this
    'spatial' attr so that this this onnx model will be valid.
    """
    flag = False

    for onnx_node in onnx_model.nodes:
        if (
            onnx_node.op_type == "BatchNormalization"
            and "spatial" in onnx_node.attributes
        ):
            onnx_node.remove_attribute(attr_name="spatial")
            flag = True

    if flag:
        logging.info(
            "Remove spatial attr from BN nodes to make sure "
            "the onnx model will be valid.",
        )

    return onnx_model


def modify_argmax_output_elemtype(onnx_model: OnnxModel) -> OnnxModel:
    """Set output data type of ArgMax node as int64.

    If a onnx model is produced by old version MXNet, there is a bug in it.
    The output element type of ArgMax is set as int32 but ArgMax in onnx
    only support int64 output element type.

    In other words, this old-mxnet-generated onnx model is invalid and
    onnx.model_checker will return false. This func is used to modify the
    output element type for ArgMax in onnx so that this this onnx model
    will be valid.
    """
    flag = False
    int64_dtype = numpy_dtype_to_onnx_dtype(np.dtype("int64"))

    for onnx_node in onnx_model.nodes:
        if onnx_node.op_type == "ArgMax" and onnx_node.outputs[0].dtype != int64_dtype:
            onnx_node.outputs[0].dtype = int64_dtype
            flag = True

    if flag:
        logging.info(
            "Modify argmax output element type from int32 to int64 "
            "to make sure this onnx model will be valid.",
        )

    return onnx_model


def add_producer_info(onnx_model: OnnxModel) -> OnnxModel:
    """Add producer horizon_nn information to the ONNX model."""
    # 设置模型生成者为horizon_nn, 相应的版本信息也进行记录
    onnx_model.producer_name = "horizon_nn"
    onnx_model.producer_version = __version__

    return onnx_model


def _str_to_int_list(int_str):
    return [int(i.strip()) for i in int_str.split(",")]


def set_pyop_shape(onnx_model: OnnxModel) -> OnnxModel:
    # collect PyOp nodes in the onnx model
    pyop_nodes = onnx_model.type2nodes["PyOp"]
    # set shape for each PyOp node
    for onnx_node in pyop_nodes:
        # if the PyOp output value info already exist in onnx model,
        # then continue and do nothing.
        if all(output_var.proto is not None for output_var in onnx_node.outputs):
            logging.info(f"PyOp node {onnx_node.name} already has output value info")
            continue

        # if the PyOp node output value info does not exist in onnx model,
        # then parse the info from 'output_{shape/types}' attribute
        output_shape_strs = None
        output_types_ints = None

        # try to get "output_shape" and "output_types" info from onnx model.
        # these 2 attrs are required when export onnx from pytorch for PyOp.
        output_shape_strs = onnx_node.attributes["output_shape"]
        output_types_ints = onnx_node.attributes["output_types"]

        if output_shape_strs is None:
            raise ValueError(
                f"PyOp node {onnx_node.name} does not have output_shape "
                f"attribute, its output shape can not be inferred.",
            )

        if output_types_ints is None:
            raise ValueError(
                f"PyOp node {onnx_node.name} does not have output_types "
                f"attribute, its output type can not be inferred.",
            )

        output_shape_list = [
            _str_to_int_list(i.decode("utf8")) for i in output_shape_strs
        ]

        assert len(output_shape_list) == len(onnx_node.outputs), (
            f"output shape length {len(output_shape_list)} "
            f"does not match node output length {len(onnx_node.outputs)}"
        )

        assert len(output_shape_list) == len(output_types_ints), (
            f"output shape length {len(output_shape_list)} "
            f"does not match output type length {len(output_types_ints)}"
        )

        # add PyOp node output value info
        for output_idx, output_var in enumerate(onnx_node.outputs):
            output_var.dtype = DataType(output_types_ints[output_idx])
            output_var.shape = output_shape_list[output_idx]

    return onnx_model


def set_input_shape(
    onnx_model: OnnxModel,
    input_shapes: Dict[str, Sequence[int]],
) -> OnnxModel:
    """Set ONNX model inputs to target input shapes and check the validity.

    Raises:
        ValueError: If any name of target input shape does not exist in model inputs
        ValueError: If any rank of target input shape does not match the
            corresponding model input
        ValueError: If any missing or dynamic dimension exists in
            target input shapes
    """
    # obtain all the model's non-initializer inputs
    input_mappings = onnx_model.graph.input_mappings

    for input_name, input_shape in input_shapes.items():
        if input_name not in input_mappings:
            raise ValueError(
                f"The input {input_name} is not found in model inputs, "
                f"existing model's input names: {list(input_mappings.keys())}",
            )
        input_var = input_mappings[input_name]

        if len(input_shape) != len(input_var.shape):
            raise ValueError(
                f"For input {input_name}, the rank of target shape "
                f"({len(input_shape)}) does not match the rank of "
                f"model shape ({len(input_var.shape)}).",
            )

        # check input_shape to make sure there exists
        # no dynamic or missing shape.
        for shape_item in input_shape:
            if not isinstance(shape_item, int) or shape_item == -1:
                raise ValueError(
                    f"For input {input_name}, the target input shape "
                    f"{input_shape} is invalid, as there exist missing "
                    f"or dynamic dimension.",
                )

        if input_var.shape == input_shape:
            continue

        # Check if target input shape is compatible with the model input shape.
        # If any dimension of model input shape is an integer and
        # its value != -1, this dim_value is unmodifiable generally.
        # For this case, we will attempt to forcibly modify the input shape,
        # which may cause errors.
        for x, y in zip(input_var.shape, input_shape):
            if isinstance(x, int) and x != -1 and x != y:
                logging.warning(
                    f"For input {input_name}, the target input shape "
                    f"{input_shape} does not match the model input "
                    f"shape {input_var.shape}. We will attempt to forcibly "
                    f"modify the input shape, which may cause errors.",
                )
                break

        logging.info(
            f"Replace the input {input_name}'s shape from "
            f"{input_var.shape} to {input_shape}.",
        )
        input_var.shape = input_shape

    try:
        onnx_model.infer_shapes().check_validity()
    except Exception as exc:
        raise ValueError(
            "The onnx model is invalid (shape_inference or model_check failed). "
            "Please check your input model again."
        ) from exc

    return onnx_model
