{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d1f5aa-3d85-4872-a900-6a74e7ca20de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/ubuntu/BigVGAN/')\n",
    "sys.path.append('/home/ubuntu/music-spectrogram-diffusion-pytorch/')\n",
    "\n",
    "import soundfile as sf\n",
    "import yaml\n",
    "from importlib import import_module\n",
    "import note_seq\n",
    "from preprocessor.event_codec import Codec\n",
    "from preprocessor.preprocessor import preprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "os.chdir('/home/ubuntu/BigVGAN/')\n",
    "\n",
    "import torch\n",
    "\n",
    "from bigvgan import BigVGAN\n",
    "\n",
    "os.chdir('/home/ubuntu/music-spectrogram-diffusion-pytorch/')\n",
    "@torch.inference_mode()\n",
    "def diff_main(model, tokens, segment_length, spec_frames, with_context, T=1000, verbose=True):\n",
    "    \"\"\"\n",
    "    model: your transformer/diffusion spec-predictor\n",
    "    tokens: iterable of token tensors\n",
    "    segment_length: length of zero context in samples\n",
    "    spec_frames: number of frames to predict per step\n",
    "    with_context: bool, whether to carry mel_context\n",
    "    \"\"\"\n",
    "    # --- Assume 'device' and 'vocoder' are defined in the outer scope ---\n",
    "    output_specs = []\n",
    "    zero_wav_context = torch.zeros(1, segment_length, device=device) if with_context else None\n",
    "    mel_context = None\n",
    "\n",
    "    model.scheduler.set_timesteps(T)\n",
    "\n",
    "    # 2. Generate token-conditioned mel-specs\n",
    "    for token in tqdm(tokens, disable=not verbose):\n",
    "        x = token.unsqueeze(0).to(device)\n",
    "\n",
    "        # pick which context arg to send\n",
    "        if len(output_specs) == 0 and with_context:\n",
    "            pred = model(\n",
    "                x,\n",
    "                seq_length=spec_frames,\n",
    "                wav_context=zero_wav_context,\n",
    "                rescale=False,\n",
    "                T=T,\n",
    "                verbose=verbose\n",
    "            )\n",
    "        else:\n",
    "            pred = model(\n",
    "                x,\n",
    "                seq_length=spec_frames,\n",
    "                mel_context=mel_context,\n",
    "                rescale=False,\n",
    "                T=T,\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "        output_specs.append(pred)\n",
    "        mel_context = pred if with_context else None\n",
    "\n",
    "    print(\"Pred shape:\", pred.shape)\n",
    "    print(\"Pred range:\", pred.min().item(), pred.max().item())\n",
    "\n",
    "    # stitch time-axis\n",
    "    output_tensor = torch.cat(output_specs, dim=1)              # [1, total_frames, n_mels]\n",
    "\n",
    "    # --- CRITICAL FIX HERE ---\n",
    "    # Transpose to match MelToDB.reverse expected input shape [B, n_mels, T]\n",
    "    output_tensor = output_tensor.transpose(1, 2)              # [1, n_mels, total_frames]\n",
    "\n",
    "    # --- CRITICAL CHANGE HERE ---\n",
    "    # Use the model's MelToDB `reverse` method to convert the diffusion model's [-1, 1] output\n",
    "    # back to the log-magnitude spectrogram format that the BigVGAN vocoder expects.\n",
    "    # The updated `reverse` function handles the denormalization from [-1,1] to the log scale.\n",
    "    log_mels_for_vocoder = model.mel.reverse(output_tensor)     # [1, n_mels, total_frames]\n",
    "    # This `log_mels_for_vocoder` is now in the format: log(clamp(magnitude, min=clip_val))\n",
    "    # which matches BigVGAN's `spectral_normalize_torch` output/input requirement.\n",
    "\n",
    "    # Ensure correct dtype for BigVGAN (shape is already [B, n_mels, T])\n",
    "    log_mels_for_vocoder = log_mels_for_vocoder.to(torch.float32) # Ensure float32\n",
    "\n",
    "    # Optional: Add explicit clamp for numerical stability based on BigVGAN training stats\n",
    "    # Typical log(magnitude) values are between -20 and 5. Adjust if needed based on observations.\n",
    "    # log_mels_for_vocoder = torch.clamp(log_mels_for_vocoder, min=-30.0, max=10.0)\n",
    "\n",
    "    # --- Generate waveform with BigVGAN ---\n",
    "    with torch.inference_mode():\n",
    "        wav_gen = vocoder(log_mels_for_vocoder)                 # [1, 1, T_audio]\n",
    "\n",
    "    # --- Process output ---\n",
    "    wav_gen_float = wav_gen.squeeze(0).squeeze(0).cpu()         # [T_audio]\n",
    "\n",
    "    # BigVGAN typically outputs in [-1, 1] range, but let's be safe and clamp\n",
    "    # Clamp to [-1, 1] range to prevent any potential clipping artifacts\n",
    "    wav_gen_float = torch.clamp(wav_gen_float, -1.0, 1.0)\n",
    "\n",
    "    # Convert to 16-bit PCM\n",
    "    wav_int16 = (wav_gen_float * 32767.0).clamp(-32768, 32767).cpu().numpy().astype('int16')\n",
    "\n",
    "    # Return the final audio and optionally the log-mel spectrogram used for vocoding\n",
    "    # wav_int16 is np.ndarray with shape [T_audio] and int16 dtype\n",
    "    # log_mels_for_vocoder.cpu().numpy()[0] is the log-mel spectrogram fed to the vocoder\n",
    "    return wav_int16, log_mels_for_vocoder.cpu().numpy()[0], output_specs\n",
    "\n",
    "    # If you want to return the intermediate linear magnitude spectrogram for analysis:\n",
    "    # linear_mels = torch.exp(log_mels_for_vocoder)\n",
    "    # return wav_int16, linear_mels.cpu().numpy()[0]\n",
    "\n",
    "#============================================================================\n",
    "\n",
    "%cd /home/ubuntu/music-spectrogram-diffusion-pytorch\n",
    "\n",
    "config = '/home/ubuntu/music-spectrogram-diffusion-pytorch/cfg/diff_base_44k.yaml'\n",
    "midi = '/home/ubuntu/music-spectrogram-diffusion-pytorch/test.mid'\n",
    "ckpt = '/home/ubuntu/last.ckpt'\n",
    "\n",
    "output = '/home/ubuntu/output.wav'\n",
    "\n",
    "with open(config) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model_configs = config['model']\n",
    "\n",
    "module_path, class_name = model_configs['class_path'].rsplit('.', 1)\n",
    "module = import_module(module_path)\n",
    "model = getattr(module, class_name).load_from_checkpoint(\n",
    "    ckpt, **model_configs['init_args'])\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "hop_length = model_configs['init_args']['hop_length']\n",
    "n_mels = model_configs['init_args']['n_mels']\n",
    "data_configs = config['data']\n",
    "sr = data_configs['init_args']['sample_rate']\n",
    "segment_length = data_configs['init_args']['segment_length']\n",
    "spec_frames = segment_length // hop_length\n",
    "resolution = 100\n",
    "segment_length_in_time = segment_length / sr\n",
    "codec = Codec(int(segment_length_in_time * resolution + 1))\n",
    "\n",
    "with_context = data_configs['init_args']['with_context'] and model_configs['init_args']['with_context']\n",
    "\n",
    "ns = note_seq.midi_file_to_note_sequence(midi)\n",
    "ns = note_seq.apply_sustain_control_changes(ns)\n",
    "tokens, _ = preprocess(ns, codec=codec)\n",
    "\n",
    "repo_id = \"nvidia/bigvgan_v2_44khz_128band_512x\"\n",
    "device = 'cuda'\n",
    "\n",
    "vocoder = BigVGAN.from_pretrained(\n",
    "        repo_id,\n",
    "        use_cuda_kernel=False,\n",
    ")\n",
    "\n",
    "vocoder.h['fmax'] = 22050\n",
    "vocoder.h[\"num_freq\"]   = 1025\n",
    "\n",
    "vocoder.cuda()\n",
    "vocoder.eval()\n",
    "vocoder.remove_weight_norm()\n",
    "\n",
    "pred, db_mels, output_specs = diff_main(model, tokens, segment_length,\n",
    "                 spec_frames, with_context)\n",
    "\n",
    "sf.write(output, pred, sr)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(db_mels,\n",
    "           aspect='auto',\n",
    "           origin='lower',\n",
    "           interpolation='nearest',\n",
    "           cmap='magma')\n",
    "plt.colorbar(label='Amplitude')\n",
    "plt.xlabel('Frame Index')\n",
    "plt.ylabel('Mel Bin Index')\n",
    "plt.title('Predicted Mel-Spectrogram')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/home/ubuntu/mel_plot.png', dpi=150)\n",
    "plt.close()  # free memory / avoid overplotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a5bb1-fa25-4074-a316-fcab0274657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed1bd15-64ad-46cd-b4cd-4a937493985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e477f8-27f8-47d7-ab19-9bd5c9abf4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitch time-axis\n",
    "output_tensor = torch.cat(output_specs, dim=1)              # [1, total_frames, n_mels]\n",
    "\n",
    "# --- CRITICAL FIX HERE ---\n",
    "# Transpose to match MelToDB.reverse expected input shape [B, n_mels, T]\n",
    "output_tensor = output_tensor.transpose(1, 2)              # [1, n_mels, total_frames]\n",
    "\n",
    "#log_mels_for_vocoder = model.mel.reverse(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b582d21-972f-4c54-a6ae-3b3df04b70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ef696-1b58-4be4-8919-5d2065085939",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = -11.512925 #output_tensor.min()\n",
    "max_val = 2.5241776 #output_tensor.max()\n",
    "normalized_0_to_1 = (output_tensor - min_val) / (max_val - min_val)\n",
    "output_tensor_normalized = normalized_0_to_1 * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0981b-1f53-4040-af6f-8d34b79dd271",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor_normalized = (output_tensor + 1.0) / 2.0 * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff67d8-af99-41a3-bcd6-a419fecf5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(output_tensor_normalized[0].cpu(),\n",
    "           aspect='auto',\n",
    "           origin='lower',\n",
    "           interpolation='nearest',\n",
    "           cmap='magma')\n",
    "plt.colorbar(label='Amplitude')\n",
    "plt.xlabel('Frame Index')\n",
    "plt.ylabel('Mel Bin Index')\n",
    "plt.title('Predicted Mel-Spectrogram')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/home/ubuntu/mel_plot.png', dpi=150)\n",
    "plt.close()  # free memory / avoid overplotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd11bb-f493-43b4-a383-3f355d19c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor[0][64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd2962-a068-4689-ba0e-6c3449e2a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After concatenation\n",
    "print(\"Final output_tensor range (before reverse):\", output_tensor.min().item(), \"to\", output_tensor.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0020b7c4-751c-46ef-b67f-df165e7cc0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pred shape:\", pred.shape)\n",
    "print(\"Pred range:\", pred.min().item(), pred.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c94059-c50f-413b-9c23-cdba49aff85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocoder.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6eb9d-141c-4215-a1db-3a68acbeddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with known inputs to see expected range\n",
    "import torch\n",
    "\n",
    "# Create test mels in different ranges\n",
    "test_mel_normalized = torch.randn(1, 128, 100).clamp(-1, 1)  # [-1, 1] range\n",
    "test_mel_db = torch.randn(1, 128, 100) * 20 - 40  # [-60, 20] dB range  \n",
    "test_mel_linear = torch.rand(1, 128, 100) * 10  # Linear magnitude\n",
    "\n",
    "try:\n",
    "    with torch.inference_mode():\n",
    "        out1 = vocoder(test_mel_normalized.cuda())\n",
    "        out2 = vocoder(test_mel_db.cuda())  \n",
    "        out3 = vocoder(test_mel_linear.cuda())\n",
    "    print(\"All inputs work - check output quality\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with certain input types: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4123c3-d880-450e-b038-23e888d30542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = torch.linspace(0,1,1000)\n",
    "log_snr = model.get_log_snr(t)\n",
    "plt.plot(t.cpu(), log_snr.cpu())\n",
    "plt.title(\"Cosine log-SNR schedule\")\n",
    "plt.xlabel(\"t\"); plt.ylabel(\"log SNR\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923d81f-0d0d-4275-a8c2-2b763e5e709c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
