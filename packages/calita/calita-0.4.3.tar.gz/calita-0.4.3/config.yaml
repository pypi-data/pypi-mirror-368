
logging:
  level: "INFO"                   ## Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_file: "logs/alita.log"      ## Path to log file (directory will be created automatically)


agent:
  primary_llm: "openai/qwen3-235b-a22b"                  ## Primary high-capability model (e.g., gpt-4o or Claude-3.7-Sonnet)
  coder_llm:  "openai/qwen3-coder-480b-a35b-instruct"    ## use for generate script and coder
  reason_llm: "openai/qwq-32b"                           ##"openai/qwq-32b"

  script_gen_prompt_template: "templates/script_template_e2b.txt"  ## Code generation prompt template


api:
  litellm_api_key:                 ## Replace with your actual Litellm Opensource model key, if empty use $LITELLM_API_KEY in os env
  litellm_api_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"     ## LiteLLM Opensource API endpoint
  openai_api_key: "YOUR_OPENAI_API_KEY_HERE"            ## Replace with your actual OpenAI API or Litellm key, if empty use $OPENAI_API_KEY in os env
  openai_api_url: "https://api.openai.com/v1/"          ## OpenAI API  endpoint
  anthropic_api_key: "YOUR_ANTHROPIC_API_KEY_HERE"      ## Replace with your actual Anthropic API key
  anthropic_base_url: "https://api.anthropic.com/v1"    ## Anthropic API endpoint (optional, defaults to official API)

  temperature: 0.7                ## Temperature for LLM API calls (0.0-2.0)
  max_tokens: 16384               ## Maximum tokens for API responses

## Optional: MCP Registry configuration
mcp_registry:
  registry_url: "http://localhost:47071"  ## Path to store MCP registry service


## Optional: Code execution configuration
code_exec_timeout: 600           ## Timeout for script execution in seconds
code_max_iterations: 3           ## Maximum iterations for McpCreationAgent

graph_max_iterations: 3          ## Maximum iterations for ManagerAgent
