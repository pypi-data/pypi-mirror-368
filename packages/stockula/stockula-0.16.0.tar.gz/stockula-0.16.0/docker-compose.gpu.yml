---
# Docker Compose configuration for GPU-accelerated Stockula
#
# Requirements:
#   - NVIDIA Container Runtime (nvidia-docker2)
#   - NVIDIA GPU drivers >= 450.80.02
#   - Docker Compose with GPU support (>= 1.28.0)
#
# Usage:
#   # Build and start GPU services
#   docker compose -f docker-compose.gpu.yml up --build
#
#   # Run specific GPU service
#   docker compose -f docker-compose.gpu.yml run stockula-gpu-cli
#
#   # Run with specific configuration
#   docker compose -f docker-compose.gpu.yml run stockula-gpu-cli python -m stockula --config .stockula.yaml --mode forecast

services:
  # GPU-accelerated production service
  stockula-gpu:
    build:
      context: .
      dockerfile: Dockerfile.nvidia
      target: gpu-production
      args:
        VERSION: ${STOCKULA_VERSION:-dev}
        BUILD_DATE: ${BUILD_DATE:-$(date -u +"%Y-%m-%dT%H:%M:%SZ")}
        GIT_COMMIT: ${GIT_COMMIT:-unknown}
        GIT_URL: ${GIT_URL:-https://github.com/mkm29/stockula}
    image: stockula:gpu-production
    container_name: stockula-gpu-prod
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - AUTOTS_GPU_ENABLED=true
      - AUTOTS_N_JOBS=1
      - TF_FORCE_GPU_ALLOW_GROWTH=true
      - STOCKULA_LOG_LEVEL=${STOCKULA_LOG_LEVEL:-INFO}
      - PYTHONWARNINGS=ignore
    volumes:
      # Mount configuration and data
      - ./.stockula.yaml:/app/.stockula.yaml:ro
      - ./data:/app/data:rw
      - ./results:/app/results:rw
      - ./models:/app/models:rw
      # Cache directories for better performance
      - stockula-gpu-cache:/app/.cache
      - stockula-gpu-models:/home/stockula/.cache
    working_dir: /app
    user: stockula
    command: ["stockula", "--help"]
    profiles:
      - gpu
      - production

  # GPU-accelerated CLI service for development
  stockula-gpu-cli:
    build:
      context: .
      dockerfile: Dockerfile.nvidia
      target: gpu-cli
      args:
        VERSION: ${STOCKULA_VERSION:-dev}
        BUILD_DATE: ${BUILD_DATE:-$(date -u +"%Y-%m-%dT%H:%M:%SZ")}
        GIT_COMMIT: ${GIT_COMMIT:-unknown}
        GIT_URL: ${GIT_URL:-https://github.com/mkm29/stockula}
    image: stockula:gpu-cli
    container_name: stockula-gpu-cli
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - AUTOTS_GPU_ENABLED=true
      - AUTOTS_N_JOBS=1
      - TF_FORCE_GPU_ALLOW_GROWTH=true
      - STOCKULA_LOG_LEVEL=${STOCKULA_LOG_LEVEL:-INFO}
      - PYTHONWARNINGS=ignore
      # Jupyter configuration
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=${JUPYTER_TOKEN:-stockula}
    volumes:
      # Mount configuration and data
      - ./.stockula.yaml:/app/.stockula.yaml:rw
      - ./data:/app/data:rw
      - ./results:/app/results:rw
      - ./models:/app/models:rw
      - ./examples:/app/examples:ro
      # Development directories
      - ./src:/app/src:ro
      - ./tests:/app/tests:ro
      # Cache and temporary directories
      - stockula-gpu-cache:/app/.cache
      - stockula-gpu-models:/home/stockula/.cache
      - stockula-gpu-jupyter:/home/stockula/.jupyter
    working_dir: /app
    user: stockula
    ports:
      - "${JUPYTER_PORT:-8888}:8888"
    stdin_open: true
    tty: true
    command: ["/bin/bash"]
    profiles:
      - gpu
      - development

  # Jupyter Lab service for GPU-accelerated notebook development
  stockula-jupyter-gpu:
    build:
      context: .
      dockerfile: Dockerfile.nvidia
      target: gpu-cli
      args:
        VERSION: ${STOCKULA_VERSION:-dev}
        BUILD_DATE: ${BUILD_DATE:-$(date -u +"%Y-%m-%dT%H:%M:%SZ")}
        GIT_COMMIT: ${GIT_COMMIT:-unknown}
        GIT_URL: ${GIT_URL:-https://github.com/mkm29/stockula}
    image: stockula:gpu-cli
    container_name: stockula-jupyter-gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - AUTOTS_GPU_ENABLED=true
      - AUTOTS_N_JOBS=1
      - TF_FORCE_GPU_ALLOW_GROWTH=true
      - STOCKULA_LOG_LEVEL=${STOCKULA_LOG_LEVEL:-DEBUG}
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=${JUPYTER_TOKEN:-stockula}
    volumes:
      - ./.stockula.yaml:/app/.stockula.yaml:rw
      - ./data:/app/data:rw
      - ./results:/app/results:rw
      - ./models:/app/models:rw
      - ./examples:/app/examples:ro
      - ./notebooks:/app/notebooks:rw
      - stockula-gpu-cache:/app/.cache
      - stockula-gpu-models:/home/stockula/.cache
      - stockula-gpu-jupyter:/home/stockula/.jupyter
    working_dir: /app
    user: stockula
    ports:
      - "${JUPYTER_PORT:-8888}:8888"
    command: ["jupyter", "lab", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root"]
    profiles:
      - gpu
      - jupyter

# Named volumes for persistent GPU-optimized caching
volumes:
  stockula-gpu-cache:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size=2g,uid=1000,gid=1000
  stockula-gpu-models:
    driver: local
  stockula-gpu-jupyter:
    driver: local

# Networks
networks:
  default:
    name: stockula-gpu
    driver: bridge
