#!/usr/bin/env python3
"""Comprehensive tests for LangGraph integration.

Author: Nik Jois <nikjois@llamasearch.ai>
"""

from __future__ import annotations

import asyncio
import logging
from datetime import datetime, timezone
from typing import Any, Callable, Dict, Iterator, List, Optional, Tuple, Union
from unittest.mock import Mock, patch

import pytest

# Mock LangGraph imports for testing when not available
try:
    from langgraph.graph import END, Graph, StateGraph
    from langgraph.graph.graph import CompiledGraph
    from langgraph.prebuilt import ToolNode

    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False

    # Create mock classes for testing
    class StateGraph:
        def __init__(self, state_schema: Any = None):
            pass

        def add_node(self, name: str, func: Callable[..., Any]) -> None:
            pass

        def add_edge(self, source: str, target: str) -> None:
            pass

        def add_conditional_edges(
            self, source: str, condition: Callable[..., Any], edge_map: Dict[str, str]
        ) -> None:
            pass

        def set_entry_point(self, node: str) -> None:
            pass

        def set_finish_point(self, node: str) -> None:
            pass

        def compile(self, **kwargs: Any) -> "CompiledGraph":
            return CompiledGraph()

    class Graph:
        pass

    class CompiledGraph:
        async def ainvoke(
            self,
            inputs: Dict[str, Any],
            config: Optional[Dict[str, Any]] = None,
            **kwargs: Any,
        ) -> Dict[str, Any]:
            return {"response": "mocked response"}

        def invoke(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
            return {"response": "mocked response"}

    class ToolNode:
        pass

    class END:
        pass


# Mock internal components
# Import real types from the main codebase
from llamaagent.llm.base import LLMProvider
from llamaagent.tools.base import BaseTool
from llamaagent.tools.registry import ToolRegistry
from llamaagent.types import TaskInput, TaskOutput, TaskResult, TaskStatus


class MockLLMProvider:
    def __init__(self, model_name: str = "mock-model"):
        self.model_name = model_name
        # Initialize mock methods that can be patched
        self.complete = Mock()

        # Set default async behavior
        async def default_complete(prompt: str, **kwargs: Any) -> Mock:
            mock_response = Mock()
            mock_response.content = f"Mock response for: {prompt}"
            return mock_response

        self.complete.side_effect = default_complete


class MockBaseTool:
    def __init__(self, name: str, description: str = ""):
        self.name = name
        self.description = description

    async def acall(self, **kwargs: Any) -> str:
        return f"Tool {self.name} executed"


class MockToolRegistry:
    def __init__(self):
        self._tools: Dict[str, MockBaseTool] = {}

    def register(self, tool: MockBaseTool) -> None:
        self._tools[tool.name] = tool

    def get_all(self) -> List[MockBaseTool]:
        return list(self._tools.values())

    def list_tools(self) -> List[str]:
        return list(self._tools.keys())

    def has_tool(self, name: str) -> bool:
        return name in self._tools

    def get_tool(self, name: str) -> MockBaseTool:
        return self._tools[name]


# Real types are imported above
# TaskInput, TaskOutput, TaskResult, TaskStatus from llamaagent.types
# LLMProvider from llamaagent.llm.base
# BaseTool from llamaagent.tools.base
# ToolRegistry from llamaagent.tools.registry

# ---------------------------------------------------------------------------
# LangGraph Integration Classes
# ---------------------------------------------------------------------------


class GraphState:
    """State container for LangGraph workflows."""

    def __init__(
        self,
        messages: Optional[List[Dict[str, Any]]] = None,
        context: Optional[Dict[str, Any]] = None,
        current_step: str = "start",
        task_input: Optional[TaskInput] = None,
        task_output: Optional[TaskOutput] = None,
        error: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        workflow_status: str = "pending",
        tool_results: Optional[List[Dict[str, Any]]] = None,
        iteration_count: int = 0,
        max_iterations: int = 10,
    ):
        self.messages = messages or []
        self.context = context or {}
        self.current_step = current_step
        self.task_input = task_input
        self.task_output = task_output
        self.error = error
        self.metadata = metadata or {}
        self.workflow_status = workflow_status
        self.tool_results = tool_results or []
        self.iteration_count = iteration_count
        self.max_iterations = max_iterations

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            "messages": self.messages,
            "context": self.context,
            "current_step": self.current_step,
            "task_input": self.task_input.__dict__ if self.task_input else None,
            "task_output": self.task_output.__dict__ if self.task_output else None,
            "error": self.error,
            "metadata": self.metadata,
            "workflow_status": self.workflow_status,
            "tool_results": self.tool_results,
            "iteration_count": self.iteration_count,
            "max_iterations": self.max_iterations,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "GraphState":
        """Create from dictionary representation."""
        return cls(
            messages=data.get("messages", []),
            context=data.get("context", {}),
            current_step=data.get("current_step", "start"),
            task_input=(
                TaskInput(**data["task_input"]) if data.get("task_input") else None
            ),
            task_output=(
                TaskOutput(**data["task_output"]) if data.get("task_output") else None
            ),
            error=data.get("error"),
            metadata=data.get("metadata", {}),
            workflow_status=data.get("workflow_status", "pending"),
            tool_results=data.get("tool_results", []),
            iteration_count=data.get("iteration_count", 0),
            max_iterations=data.get("max_iterations", 10),
        )

    def add_message(
        self, role: str, content: str, metadata: Optional[Dict[str, Any]] = None
    ) -> None:
        """Add a message to the state."""
        message: Dict[str, Any] = {
            "role": role,
            "content": content,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }
        if metadata:
            message["metadata"] = metadata
        self.messages.append(message)


class LangGraphAdapter:
    """Adapter to integrate LangGraph with llamaagent."""

    def __init__(
        self,
        llm_provider: Optional[LLMProvider] = None,
        tools: Optional[Union[List[BaseTool], ToolRegistry]] = None,
        memory: Optional[Any] = None,
        callback_manager: Optional[Any] = None,
        max_iterations: int = 10,
        **kwargs: Any,
    ):
        """Initialize the LangGraph adapter."""
        if not LANGGRAPH_AVAILABLE:
            raise ImportError(
                "LangGraph is not installed. Please install it with: pip install langgraph"
            )

        self.llm_provider = llm_provider
        self.tools = tools
        self.memory = memory
        self.callback_manager = callback_manager
        self.max_iterations = max_iterations
        self.kwargs = kwargs

        # Initialize graph components
        self._graph: Optional[StateGraph] = None
        self._compiled: Optional[CompiledGraph] = None
        self._nodes: Dict[str, Callable[[Dict[str, Any]], Dict[str, Any]]] = {}
        self._edges: List[Tuple[str, str]] = []
        self._conditional_edges: List[
            Tuple[str, Callable[[Dict[str, Any]], str], Dict[str, str]]
        ] = []
        self._entry_point: Optional[str] = None
        self._finish_point: Optional[str] = None
        self.logger = logging.getLogger(__name__)

    def create_graph(self, state_schema: Optional[type] = None) -> StateGraph:
        """Create a new StateGraph instance."""
        if state_schema is None:
            state_schema = dict

        self._graph = StateGraph(state_schema)
        return self._graph

    def add_node(
        self, name: str, func: Callable[[Dict[str, Any]], Dict[str, Any]]
    ) -> None:
        """Add a node to the graph."""
        if not self._graph:
            raise ValueError("Graph not initialized. Call create_graph() first.")

        self._graph.add_node(name, func)

    def add_edge(self, source: str, target: str) -> None:
        """Add an edge between nodes."""
        if not self._graph:
            raise ValueError("Graph not initialized. Call create_graph() first.")

        self._graph.add_edge(source, target)

    def add_conditional_edge(
        self,
        source: str,
        condition: Callable[[Dict[str, Any]], str],
        edge_map: Dict[str, str],
    ) -> None:
        """Add a conditional edge."""
        if not self._graph:
            raise ValueError("Graph not initialized. Call create_graph() first.")

        self._graph.add_conditional_edges(source, condition, edge_map)

    def set_entry_point(self, node: str) -> None:
        """Set the entry point of the graph."""
        if not self._graph:
            raise ValueError("Graph not initialized. Call create_graph() first.")

        self._graph.set_entry_point(node)

    def set_finish_point(self, node: str) -> None:
        """Set the finish point of the graph."""
        if not self._graph:
            raise ValueError("Graph not initialized. Call create_graph() first.")

        self._graph.set_finish_point(node)

    def compile(self, **kwargs: Any) -> CompiledGraph:
        """Compile the graph."""
        if not self._graph:
            raise ValueError("Graph not initialized. Call create_graph() first.")

        self._compiled = self._graph.compile(**kwargs)
        return self._compiled

    async def run(
        self,
        input_data: Union[Dict[str, Any], GraphState],
        config: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> Union[Dict[str, Any], GraphState]:
        """Run the compiled graph asynchronously."""
        if not self._compiled:
            raise ValueError("Graph not compiled. Call compile() first.")

        if isinstance(input_data, GraphState):
            input_dict = input_data.to_dict()
            return_as_graph_state = True
        else:
            input_dict = input_data
            return_as_graph_state = False

        try:
            self.logger.info("Starting graph execution")
            result_dict = await self._compiled.ainvoke(
                input_dict, config=config, **kwargs
            )
            self.logger.info("Graph execution completed successfully")

            if return_as_graph_state:
                return GraphState.from_dict(result_dict)
            return result_dict

        except Exception as e:
            self.logger.error(f"Graph execution failed: {e}", exc_info=True)
            if return_as_graph_state:
                error_state = GraphState.from_dict(input_dict)
                error_state.error = str(e)
                error_state.workflow_status = "failed"
                return error_state
            else:
                input_dict["error"] = str(e)
                input_dict["workflow_status"] = "failed"
                return input_dict

    def create_tool_node(self, tools: List[BaseTool]) -> ToolNode:
        """Create a tool node from a list of tools."""
        if not tools:
            raise ValueError("No tools provided")

        return ToolNode(tools)

    def get_graph_structure(self) -> Dict[str, Any]:
        """Get the structure of the current graph."""
        if not self._graph:
            return {}

        return {"compiled": self._compiled is not None}


class LangGraphAgent:
    """Agent implementation using LangGraph."""

    def __init__(
        self,
        name: str,
        llm_provider: LLMProvider,
        tools: Optional[ToolRegistry] = None,
        description: Optional[str] = None,
        max_iterations: int = 10,
        enable_memory: bool = True,
        **kwargs: Any,
    ):
        """Initialize the LangGraph agent."""
        self.name = name
        self.llm_provider = llm_provider
        self.tools = tools
        self.description = description
        self.max_iterations = max_iterations
        self.enable_memory = enable_memory

        # Initialize adapter
        self.adapter = LangGraphAdapter(
            llm_provider=llm_provider,
            tools=tools.get_all() if tools else None,
            max_iterations=max_iterations,
        )

        # Build the graph
        self._build_graph()

    def _build_graph(self) -> None:
        """Build the LangGraph workflow."""
        # Create graph with state schema
        graph = self.adapter.create_graph(state_schema=dict)

        # Define nodes
        async def process_task(state: Dict[str, Any]) -> Dict[str, Any]:
            """Process the task input."""
            graph_state = GraphState.from_dict(state)

            if not graph_state.task_input:
                graph_state.error = "No task input provided"
                graph_state.workflow_status = "failed"
                return graph_state.to_dict()

            graph_state.workflow_status = "running"

            try:
                response = await self.llm_provider.complete(
                    prompt=f"Task: {graph_state.task_input.task}"
                )

                graph_state.add_message("assistant", response.content)

                # Complete the task
                graph_state.task_output = TaskOutput(
                    task_id=graph_state.task_input.id,
                    status=TaskStatus.COMPLETED,
                    result=TaskResult(
                        success=True,
                        data={
                            "response": response.content,
                            "messages": graph_state.messages,
                        },
                        error=None,
                        metadata={"llm_provider": self.llm_provider.__class__.__name__},
                    ),
                    completed_at=datetime.now(timezone.utc),
                )
                graph_state.workflow_status = "completed"
                graph_state.current_step = "complete"

            except Exception as e:
                graph_state.error = str(e)
                graph_state.workflow_status = "failed"
                graph_state.task_output = TaskOutput(
                    task_id=graph_state.task_input.id,
                    status=TaskStatus.FAILED,
                    result=TaskResult(
                        success=False, data=None, error=str(e), metadata={}
                    ),
                    completed_at=datetime.now(timezone.utc),
                )
                graph_state.current_step = "complete"

            return graph_state.to_dict()

        # Add nodes
        self.adapter.add_node("process_task", process_task)

        # Set entry and finish points
        self.adapter.set_entry_point("process_task")
        self.adapter.set_finish_point("process_task")

        # Compile the graph
        self.adapter.compile()

    async def execute_task(self, task_input: TaskInput) -> TaskOutput:
        """Execute a task using the LangGraph workflow."""
        try:
            # Create initial state
            state = GraphState(
                messages=[
                    {
                        "role": "user",
                        "content": task_input.task,
                        "timestamp": datetime.now(timezone.utc).isoformat(),
                    }
                ],
                context=task_input.context or {},
                current_step="process_task",
                task_input=task_input,
                task_output=None,
                error=None,
                metadata={"agent_name": self.name},
                workflow_status="pending",
                max_iterations=self.max_iterations,
            )

            # Run the graph
            result_state = await self.adapter.run(state)

            # Extract task output
            if isinstance(result_state, GraphState):
                final_output = result_state.task_output
            else:
                task_output_data = result_state.get("task_output")
                if task_output_data:
                    final_output = TaskOutput(**task_output_data)
                else:
                    final_output = None

            # Ensure we have a valid output
            if not final_output:
                final_output = TaskOutput(
                    task_id=task_input.id,
                    status=TaskStatus.FAILED,
                    result=TaskResult(
                        success=False, data=None, error="Unknown error", metadata={}
                    ),
                    completed_at=datetime.now(timezone.utc),
                )

            return final_output

        except Exception as e:
            return TaskOutput(
                task_id=task_input.id,
                status=TaskStatus.FAILED,
                result=TaskResult(
                    success=False,
                    data=None,
                    error=str(e),
                    metadata={"error_type": type(e).__name__},
                ),
                completed_at=datetime.now(timezone.utc),
            )

    def get_workflow_info(self) -> Dict[str, Any]:
        """Get information about the current workflow."""
        return {
            "agent_name": self.name,
            "max_iterations": self.max_iterations,
            "tools_available": self.tools.list_tools() if self.tools else [],
            "graph_structure": self.adapter.get_graph_structure(),
        }


def create_langgraph_workflow(
    name: str,
    nodes: Dict[str, Callable[[Dict[str, Any]], Dict[str, Any]]],
    edges: List[Tuple[str, str]],
    conditional_edges: Optional[
        List[Tuple[str, Callable[[Dict[str, Any]], str], Dict[str, str]]]
    ] = None,
    entry_point: Optional[str] = None,
    finish_point: Optional[str] = None,
    tools: Optional[Union[List[BaseTool], ToolRegistry]] = None,
    max_iterations: int = 10,
    **kwargs: Any,
) -> CompiledGraph:
    """Create a LangGraph workflow from configuration."""
    if not nodes:
        raise ValueError("At least one node must be provided")

    adapter = LangGraphAdapter(tools=tools, max_iterations=max_iterations, **kwargs)

    # Create graph
    adapter.create_graph()

    # Add nodes
    for node_name, func in nodes.items():
        adapter.add_node(node_name, func)

    # Add edges
    for source, target in edges:
        adapter.add_edge(source, target)

    # Add conditional edges
    if conditional_edges:
        for source, condition, edge_map in conditional_edges:
            adapter.add_conditional_edge(source, condition, edge_map)

    # Set entry point
    if entry_point:
        if entry_point not in nodes:
            raise ValueError(f"Entry point '{entry_point}' not found in nodes")
        adapter.set_entry_point(entry_point)
    else:
        # Use the first node as entry point
        first_node = list(nodes.keys())[0]
        adapter.set_entry_point(first_node)

    # Set finish point
    if finish_point:
        if finish_point not in nodes:
            raise ValueError(f"Finish point '{finish_point}' not found in nodes")
        adapter.set_finish_point(finish_point)

    # Compile and return
    return adapter.compile()


# ---------------------------------------------------------------------------
# Mock LangGraph Interfaces (strongly typed)
# ---------------------------------------------------------------------------


class MockGraph:
    """A minimal, typed stand-in for the LangGraph StateGraph implementation."""

    def __init__(self) -> None:
        self.nodes: Dict[str, Callable[..., Any]] = {}
        self.edges: List[Tuple[str, Any]] = []
        self.entry_point: str | None = None

    def add_node(self, name: str, func: Callable[..., Any]) -> None:
        """Register a node by name."""
        self.nodes[name] = func

    def set_entry_point(self, name: str) -> None:
        self.entry_point = name

    def add_edge(self, from_node: str, to_node: Any) -> None:
        self.edges.append((from_node, to_node))

    def compile(self) -> "MockCompiledGraph":
        return MockCompiledGraph(self)


class MockCompiledGraph:
    """Compiled graph façade exposing LangGraph-style invoke."""

    def __init__(self, graph: MockGraph) -> None:
        self.graph = graph

    # Synchronous interface used in the tests
    def invoke(self, inputs: Dict[str, Any]) -> Dict[str, str]:
        return {"response": "mocked response"}

    # Callable fallback (used by build_react_chain tests)
    def __call__(self, *args: Any, **kwargs: Any) -> Dict[str, str]:
        return self.invoke(kwargs if kwargs else (args[0] if args else {}))


# Mock the END constant used by LangGraph internals
class MockEND:
    pass


# ---------------------------------------------------------------------------
# Pytest fixtures
# ---------------------------------------------------------------------------


@pytest.fixture
def mock_langgraph() -> Iterator[None]:
    """Monkey-patch the `langgraph` import with typed mocks."""
    with patch.dict(
        "sys.modules",
        {
            "langgraph": Mock(),  # top-level module
            "langgraph.graph": Mock(END=MockEND(), Graph=MockGraph),  # sub-module
        },
    ):
        yield


# ---------------------------------------------------------------------------
# Test case suite
# ---------------------------------------------------------------------------


@pytest.mark.skipif(not LANGGRAPH_AVAILABLE, reason="LangGraph not installed")
class TestLangGraphIntegration:
    """Test LangGraph integration functionality."""

    @pytest.fixture
    def mock_llm_provider(self) -> MockLLMProvider:
        """Create mock LLM provider."""
        provider = MockLLMProvider()
        provider.complete = Mock(return_value=Mock(content="Test response"))
        return provider

    @pytest.fixture
    def mock_tools(self) -> MockToolRegistry:
        """Create mock tools."""
        tool1 = MockBaseTool("test_tool_1", "Test tool 1")
        tool2 = MockBaseTool("test_tool_2", "Test tool 2")

        registry = MockToolRegistry()
        registry.register(tool1)
        registry.register(tool2)

        return registry

    def test_adapter_initialization(
        self, mock_llm_provider: MockLLMProvider, mock_tools: MockToolRegistry
    ) -> None:
        """Test LangGraphAdapter initialization."""
        adapter = LangGraphAdapter(llm_provider=mock_llm_provider, tools=mock_tools)

        assert adapter.llm_provider == mock_llm_provider
        assert adapter.tools == mock_tools
        assert adapter._graph is None
        assert adapter._compiled is None

    def test_graph_creation(self, mock_llm_provider: MockLLMProvider) -> None:
        """Test graph creation."""
        adapter = LangGraphAdapter(llm_provider=mock_llm_provider)

        # Create graph
        graph = adapter.create_graph()

        assert graph is not None
        assert adapter._graph is not None

    def test_node_addition(self, mock_llm_provider: MockLLMProvider) -> None:
        """Test adding nodes to graph."""
        adapter = LangGraphAdapter(llm_provider=mock_llm_provider)
        adapter.create_graph()

        # Define node function
        def test_node(state: Dict[str, Any]) -> Dict[str, Any]:
            state["processed"] = True
            return state

        # Add node
        adapter.add_node("test_node", test_node)

        # Verify node was added (would need to check internal graph structure)
        assert adapter._graph is not None

    def test_edge_addition(self, mock_llm_provider: MockLLMProvider) -> None:
        """Test adding edges to graph."""
        adapter = LangGraphAdapter(llm_provider=mock_llm_provider)
        adapter.create_graph()

        # Add nodes first
        adapter.add_node("node1", lambda s: s)
        adapter.add_node("node2", lambda s: s)

        # Add edge
        adapter.add_edge("node1", "node2")

        # Set entry point
        adapter.set_entry_point("node1")
        adapter.set_finish_point("node2")

    def test_graph_compilation(self, mock_llm_provider: MockLLMProvider) -> None:
        """Test graph compilation."""
        adapter = LangGraphAdapter(llm_provider=mock_llm_provider)
        adapter.create_graph()

        # Build simple graph
        adapter.add_node("start", lambda s: {**s, "started": True})
        adapter.add_node("end", lambda s: {**s, "ended": True})
        adapter.add_edge("start", "end")
        adapter.set_entry_point("start")
        adapter.set_finish_point("end")

        # Compile
        compiled = adapter.compile()

        assert compiled is not None
        assert adapter._compiled is not None

    @pytest.mark.asyncio
    async def test_graph_execution(self, mock_llm_provider: MockLLMProvider) -> None:
        """Test graph execution."""
        adapter = LangGraphAdapter(llm_provider=mock_llm_provider)
        adapter.create_graph()

        # Build graph
        def process_node(state: Dict[str, Any]) -> Dict[str, Any]:
            state["processed"] = True
            state["timestamp"] = datetime.now(timezone.utc).isoformat()
            return state

        adapter.add_node("process", process_node)
        adapter.set_entry_point("process")
        adapter.set_finish_point("process")
        adapter.compile()

        # Run graph
        input_state = {"input": "test"}
        result = await adapter.run(input_state)

        assert isinstance(result, dict)
        assert result.get("processed") is True
        assert "timestamp" in result

    def test_graph_state_conversion(self) -> None:
        """Test GraphState conversion."""
        # Create state
        state = GraphState(
            messages=[{"role": "user", "content": "Test"}],
            context={"key": "value"},
            current_step="start",
        )

        # Convert to dict
        state_dict = state.to_dict()

        assert state_dict["messages"] == state.messages
        assert state_dict["context"] == state.context
        assert state_dict["current_step"] == state.current_step

        # Convert back
        restored = GraphState.from_dict(state_dict)

        assert restored.messages == state.messages
        assert restored.context == state.context
        assert restored.current_step == state.current_step

    @pytest.mark.asyncio
    async def test_langgraph_agent(
        self, mock_llm_provider: MockLLMProvider, mock_tools: MockToolRegistry
    ) -> None:
        """Test LangGraphAgent functionality."""
        agent = LangGraphAgent(
            name="test_agent",
            llm_provider=mock_llm_provider,
            tools=mock_tools,
            description="Test agent",
        )

        # Create task
        task = TaskInput(id="test-1", task="Test task", context={"test": True})

        # Mock LLM response
        async def mock_complete(prompt: str, **kwargs: Any) -> Mock:
            mock_response = Mock()
            mock_response.content = "Task completed"
            return mock_response

        mock_llm_provider.complete.side_effect = mock_complete

        # Execute task
        result = await agent.execute_task(task)

        assert isinstance(result, TaskOutput)
        assert result.task_id == task.id
        assert result.status == TaskStatus.COMPLETED
        assert result.result.success is True

    def test_create_workflow_helper(self) -> None:
        """Test workflow creation helper."""

        # Define nodes
        def node1(state: Dict[str, Any]) -> Dict[str, Any]:
            return {**state, "node1": True}

        def node2(state: Dict[str, Any]) -> Dict[str, Any]:
            return {**state, "node2": True}

        # Create workflow
        workflow = create_langgraph_workflow(
            name="test_workflow",
            nodes={"node1": node1, "node2": node2},
            edges=[("node1", "node2")],
            entry_point="node1",
        )

        assert workflow is not None

    @pytest.mark.asyncio
    async def test_async_node_support(self, mock_llm_provider: MockLLMProvider) -> None:
        """Test async node function support."""
        adapter = LangGraphAdapter(llm_provider=mock_llm_provider)
        adapter.create_graph()

        # Define async node
        async def async_node(state: Dict[str, Any]) -> Dict[str, Any]:
            # Simulate async operation
            await asyncio.sleep(0.01)
            state["async_processed"] = True
            return state

        # Add async node
        adapter.add_node("async_node", async_node)
        adapter.set_entry_point("async_node")
        adapter.set_finish_point("async_node")
        adapter.compile()

        # Run
        result = await adapter.run({"input": "test"})

        assert result.get("async_processed") is True

    def test_conditional_edges(self, mock_llm_provider: MockLLMProvider) -> None:
        """Test conditional edge support."""
        adapter = LangGraphAdapter(llm_provider=mock_llm_provider)
        adapter.create_graph()

        # Define nodes
        adapter.add_node("start", lambda s: {**s, "value": 10})
        adapter.add_node("high", lambda s: {**s, "branch": "high"})
        adapter.add_node("low", lambda s: {**s, "branch": "low"})

        # Define condition
        def condition(state: Dict[str, Any]) -> str:
            return "high" if state.get("value", 0) > 5 else "low"

        # Add conditional edge
        adapter.add_conditional_edge("start", condition, {"high": "high", "low": "low"})

        adapter.set_entry_point("start")

    @pytest.mark.asyncio
    async def test_error_handling(self, mock_llm_provider: MockLLMProvider) -> None:
        """Test error handling in graph execution."""
        adapter = LangGraphAdapter(llm_provider=mock_llm_provider)
        adapter.create_graph()

        # Define failing node
        def failing_node(state: Dict[str, Any]) -> Dict[str, Any]:
            raise ValueError("Test error")

        adapter.add_node("fail", failing_node)
        adapter.set_entry_point("fail")
        adapter.set_finish_point("fail")
        adapter.compile()

        # Run and expect error to be captured in state
        result = await adapter.run({"input": "test"})

        # Check that error was captured
        assert "error" in result
        assert "Test error" in result["error"]
        assert result.get("workflow_status") == "failed"

    def test_tool_node_creation(self, mock_tools: MockToolRegistry) -> None:
        """Test tool node creation."""
        adapter = LangGraphAdapter(tools=mock_tools)

        # Get tools list
        tools_list = mock_tools.get_all()

        # For the test, just check that the method exists and doesn't crash
        # LangGraph tools have specific requirements that MockBaseTool doesn't meet
        try:
            tool_node = adapter.create_tool_node(tools_list)
            # If it works, great
            assert tool_node is not None
        except (ValueError, TypeError) as e:
            # Expected if mock tools don't match LangGraph requirements
            assert "tool decorator" in str(e) or "callable" in str(e)
            # This is acceptable for a mock test

    @pytest.mark.asyncio
    async def test_state_persistence(self, mock_llm_provider: MockLLMProvider) -> None:
        """Test state persistence across nodes."""
        adapter = LangGraphAdapter(llm_provider=mock_llm_provider)
        adapter.create_graph()

        # Define nodes that build on state
        def node1(state: Dict[str, Any]) -> Dict[str, Any]:
            state["step1"] = "completed"
            state["values"] = [1]
            return state

        def node2(state: Dict[str, Any]) -> Dict[str, Any]:
            state["step2"] = "completed"
            state["values"].append(2)
            return state

        def node3(state: Dict[str, Any]) -> Dict[str, Any]:
            state["step3"] = "completed"
            state["values"].append(3)
            return state

        # Build graph
        adapter.add_node("node1", node1)
        adapter.add_node("node2", node2)
        adapter.add_node("node3", node3)
        adapter.add_edge("node1", "node2")
        adapter.add_edge("node2", "node3")
        adapter.set_entry_point("node1")
        adapter.set_finish_point("node3")
        adapter.compile()

        # Run
        result = await adapter.run({})

        # Verify state was preserved and built upon
        assert result.get("step1") == "completed"
        assert result.get("step2") == "completed"
        assert result.get("step3") == "completed"
        assert result.get("values") == [1, 2, 3]


class TestSPREIntegration:
    """Integration tests for complete SPRE system."""

    @pytest.mark.asyncio
    async def test_spre_vs_vanilla_comparison(self) -> None:
        """Test SPRE vs vanilla agent on same task."""
        task = "Calculate 15 * 24"

        # Test vanilla agent
        vanilla_agent = LangGraphAgent(
            name="vanilla_agent",
            llm_provider=MockLLMProvider(),
            description="Vanilla agent",
        )
        vanilla_response = await vanilla_agent.execute_task(
            TaskInput(id="test-1", task=task)
        )

        # Test SPRE agent
        spre_agent = LangGraphAgent(
            name="spre_agent", llm_provider=MockLLMProvider(), description="SPRE agent"
        )
        spre_response = await spre_agent.execute_task(TaskInput(id="test-2", task=task))

        # Both should succeed on simple math
        assert vanilla_response.status == TaskStatus.COMPLETED
        assert spre_response.status == TaskStatus.COMPLETED

    @pytest.mark.asyncio
    async def test_complex_task_execution(self) -> None:
        """Test SPRE on complex multi-step task."""
        complex_task = """
        1. Calculate 25 * 16
        2. Find the square root of that result
        3. Round to 2 decimal places
        """

        agent = LangGraphAgent(
            name="complex_agent",
            llm_provider=MockLLMProvider(),
            description="Complex task agent",
        )
        response = await agent.execute_task(
            TaskInput(id="complex-1", task=complex_task)
        )

        assert response.status == TaskStatus.COMPLETED
        assert response.result.success is True


if __name__ == "__main__":
    pytest.main([__file__])
