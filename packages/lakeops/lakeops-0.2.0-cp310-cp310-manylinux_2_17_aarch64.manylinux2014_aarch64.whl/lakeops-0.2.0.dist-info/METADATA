Metadata-Version: 2.4
Name: lakeops
Version: 0.2.0
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Rust
Classifier: Topic :: Scientific/Engineering
Classifier: Typing :: Typed
Requires-Dist: cryptography>=44.0.1
Requires-Dist: databricks-sdk>=0.44.1
Requires-Dist: deltalake>=0.24.0
Requires-Dist: duckdb>=1.2.1
Requires-Dist: pandas>=2.2.3
Requires-Dist: polars>=1.21.0
Requires-Dist: pyarrow>=19.0.0
Requires-Dist: pyspark~=3.5.0 ; extra == 'spark'
Requires-Dist: pyspark[connect]~=3.5.0 ; extra == 'spark-connect'
Requires-Dist: connectorx>=0.4.1 ; extra == 'trino'
Requires-Dist: trino[sqlalchemy]>=0.333.0 ; extra == 'trino'
Requires-Dist: gspread>=6.1.4 ; extra == 'gsheet'
Provides-Extra: spark
Provides-Extra: spark_connect
Provides-Extra: trino
Provides-Extra: gsheet
License-File: LICENSE
Summary: Data lake operations toolkit
Author-email: Huong Vuong <hoaihuongvuonghuynh@gmail.com>
Requires-Python: >=3.10
Description-Content-Type: text/markdown; charset=UTF-8; variant=GFM

# LakeOps

[![PyPI version](https://badge.fury.io/py/lakeops.svg)](https://badge.fury.io/py/lakeops)
[![Python Versions](https://img.shields.io/pypi/pyversions/lakeops.svg)](https://pypi.org/project/lakeops/)
[![Tests](https://github.com/hoaihuongbk/lakeops/actions/workflows/test.yml/badge.svg)](https://github.com/hoaihuongbk/lakeops/actions/workflows/test.yml)
[![codecov](https://codecov.io/gh/hoaihuongbk/lakeops/branch/main/graph/badge.svg)](https://codecov.io/gh/hoaihuongbk/lakeops)


A modern data lake operations toolkit working with multiple table formats (Delta, Iceberg, Parquet) and engines
(Spark, Polars) via the same APIs.

## Features

- Multi-format support: Delta, Iceberg, Parquet
- Multiple engine backends: Apache Spark, Polars (default)
- Storage operations: read, write

To learn more, read the [user guide](https://hoaihuongbk.github.io/lakeops/).

## Quick Start

### Installation
```bash
pip install lakeops
```

### Sample Usage

```python
from pyspark.sql import SparkSession
from lakeops import LakeOps
from lakeops.core.engine import SparkEngine

# Init Spark session and create LakeOps instance
spark = SparkSession.builder.getOrCreate()
engine = SparkEngine(spark)
ops = LakeOps(engine)

# Read data from table name
df = ops.read("s3://local/test/table", format="parquet")

# Write data to table name
ops.write(df, "s3://local/test/table", format="parquet")

```

