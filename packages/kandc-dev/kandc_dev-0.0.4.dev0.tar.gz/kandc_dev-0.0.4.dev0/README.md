# Keys & Caches

![Keys & Caches Banner](assets/banner.png)

Welcome to **Keys & Caches** â€” the fastest way to run PyTorch models on cloud GPUs with automatic profiling and performance insights.

---

## ğŸ“š Documentation Overview

This documentation will help you get started with Keys & Caches and make the most of its powerful features for GPU-accelerated machine learning.

---

## What is Keys & Caches?

Keys & Caches is a command-line tool that makes it effortless to run PyTorch models on high-performance cloud GPUs. With just one command, you can:

* ğŸš€ **Submit jobs to cloud GPUs** â€” Access A100, H100, and L4 GPUs instantly
* ğŸ“Š **Get automatic profiling** â€” Detailed performance traces for every model forward pass
* ğŸ” **Debug performance bottlenecks** â€” Chrome trace format for visual analysis
* âš¡ **Stream real-time logs** â€” Watch your training progress live
* ğŸ’° **Pay only for what you use** â€” No idle time charges

---

## Key Features

### ğŸ¯ One-Command Deployment

```bash
# Run any PyTorch script on cloud GPUs
kandc run python infer.py --model-size large --epochs 100

# Or capture locally with profiling
kandc capture python infer.py --model-size large --epochs 100
```

---

### ğŸ“ˆ Automatic Model Profiling

```python
from kandc import capture_model_class

@capture_model_class(model_name="MyModel")
class MyModel(nn.Module):
    # Your model automatically gets profiled!
```

---

### ğŸ”§ Simple Command Formats

```bash
# Cloud GPU execution:
kandc run python script.py --script-args                                  # Interactive
kandc run --app-name job --gpu H100:4 -- python script.py --script-args  # Separator

# Local execution with capture:
kandc capture python script.py --script-args                              # Interactive
kandc capture --app-name test -- python script.py --script-args           # Direct
```

---

## Who Should Use Keys & Caches?

### ğŸ§‘â€ğŸ”¬ Machine Learning Researchers

* Quickly test models on powerful GPUs without infrastructure setup
* Get detailed performance profiles to optimize model architectures
* Scale experiments from 1 to 8 GPUs seamlessly

### ğŸ¢ ML Engineers & Data Scientists

* Accelerate training jobs without managing cloud infrastructure
* Debug performance issues with automatic profiling
* Iterate faster with real-time progress monitoring

### ğŸ“ Students & Educators

* Email us at **[founders@herdora.com](mailto:founders@herdora.com)** for free credits!
* Access high-end GPUs for coursework and research
* Learn about model optimization with built-in profiling tools
* Focus on ML concepts, not DevOps complexity

### ğŸš€ Startups & Small Teams

* Get enterprise-grade GPU access without upfront costs
* Scale compute resources based on actual needs
* Streamline ML workflows from development to production

---

## Why Choose Keys & Caches?

* ğŸš€ **Instant Access** â€” No account setup, no credit cards, no waiting. Install and run immediately.
* ğŸ’¡ **Built for ML** â€” Purpose-built for PyTorch with automatic model profiling and optimization insights.
* ğŸ¯ **Developer-Friendly** â€” Simple CLI that works with your existing code. No code changes required.
* ğŸ“Š **Performance-First** â€” Every job includes detailed performance traces to help you optimize your models.
* ğŸ’° **Cost-Effective** â€” Pay only for actual GPU time. No idle charges, no minimum commitments.

---

## Ready to Get Started?

ğŸ‘‰ Jump to the **[Getting Started Guide](https://www.keysandcaches.com/docs)** to install Keys & Caches and run your first GPU job in under 5 minutes!

---

## Quick Example

Hereâ€™s how easy it is to run a PyTorch model on cloud GPUs:

```python
# your_model.py
import torch
import torch.nn as nn
from kandc import capture_model_class

@capture_model_class(model_name="SimpleModel")
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(784, 10)
    
    def forward(self, x):
        return self.linear(x)

# Train your model...
model = MyModel()
x = torch.randn(32, 784)
output = model(x)  # This gets automatically profiled!
```

```bash
# Run on cloud GPUs with one command
kandc run python your_model.py

# Or run locally with profiling and upload results
kandc capture python your_model.py
```

Thatâ€™s it! Your model runs on high-performance GPUs with automatic profiling. ğŸ‰


# ğŸ“¦ Publishing to PyPI

## ğŸš€ Publish Stable Release (`kandc`)

1. **Bump the version** in `pyproject.toml` (e.g., `0.0.4`).

2. **Run the following commands:**
   ```bash
   rm -rf dist build *.egg-info
   python -m pip install --upgrade build twine
   python -m build
   export TWINE_USERNAME=__token__
   twine upload dist/*
   pip install -U kandc
   ```

## ğŸ§ª Publish Dev Release (`kandc-dev`)

1. **Bump the dev version** in `pyproject.dev.toml` (e.g., `0.0.4.dev1`).

2. **Run the following commands:**
   ```bash
   rm -rf dist build *.egg-info
   cp pyproject.dev.toml pyproject.toml
   python -m pip install --upgrade build twine
   python -m build
   export TWINE_USERNAME=__token__
   twine upload dist/*
   git checkout -- pyproject.toml   # Restore the original pyproject.toml after publishing (undo the cp above)
   pip install -U kandc-dev
   # If you gave dev its own CLI:
   kandc-dev --help
   # Otherwise, itâ€™s the same `kandc` entry point.
   ```
   ```