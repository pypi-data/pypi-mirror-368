python_exec_path: ${oc.env:PROJECT_ROOT}/forks/RoseTTAFold-All-Atom/RFAA/bin/python3 # the Python executable to use
rfaa_exec_dir: ${oc.env:PROJECT_ROOT}/forks/RoseTTAFold-All-Atom # the RoseTTAFold-All-Atom directory in which to execute the inference scripts
dataset: posebusters_benchmark # the dataset to use - NOTE: must be one of (`posebusters_benchmark`, `astex_diverse`, `dockgen`, `casp15`)
input_dir: ${oc.env:PROJECT_ROOT}/forks/RoseTTAFold-All-Atom/prediction_inputs/${dataset} # the input directory with which to run inference
config_dir: ${oc.env:PROJECT_ROOT}/forks/RoseTTAFold-All-Atom/rf2aa/config/inference # the config directory with which to run inference
output_dir: ${oc.env:PROJECT_ROOT}/forks/RoseTTAFold-All-Atom/prediction_outputs/${dataset}_${repeat_index} # the output directory to which to save the inference results
max_cycles: 10 # the maximum number of recycling iterations to run
cuda_device_index: 0 # the CUDA device to use for inference, or `null` to use CPU
run_inference_directly: false # whether to run the inference code directly (true) or to rely on the user to run the generated scripts manually (false)
inference_config_name: null # the name of the inference config to use - NOTE: if `run_inference_directly` is true, this must reference a valid YAML config file name e.g., that was generated by `python posebench/models/rfaa_inference.py` with `run_inference_directly=false`
inference_dir_name: null # the name of the inference output directory to use
repeat_index: 1 # the repeat index to use for inference
skip_existing: true # whether to skip running inference if the prediction for a target already exists
pocket_only_baseline: false # whether to run the pocket-only baseline
max_num_inputs: null # if provided, the number of (dataset subset) inputs over which to run inference
