"""Context compression utility.

If the full conversation exceeds the model's context window, we ask the LLM
itself to produce a concise summary that can be injected back into the
conversation.  This prevents token-overflow errors while still preserving
important information.
"""

from __future__ import annotations

from typing import List, Optional

from ..providers.openrouter import Message  # shared pydantic model
from .context_manager import ContextManager
from .unified_config import get_config


class ContextCompressor:
    """Compresses long message histories into a single summary message."""

    def __init__(self, provider, max_summary_tokens: Optional[int] = None):
        self.provider = provider
        #   We reuse token-counting logic from ContextManager just for convenience
        context_max_tokens = get_config('limits.context_max_tokens', 100000)
        self.counter = ContextManager(max_tokens=context_max_tokens)
        self.max_summary_tokens = max_summary_tokens or get_config('limits.summary_max_tokens', 1024)

    async def compress(self, messages: List[Message]) -> Message:
        """Ask the provider to summarise *messages* and return a new Message.

        The summary is generated by feeding the entire conversation to the
        model with a special prompt.
        """
        if not messages:
            return Message(role="system", content="(empty summary)")

        # Build a giant user prompt containing the raw conversation
        conversation_text = "\n\n".join(
            [f"{m.role.upper()}: {m.content}" for m in messages]
        )

        summarise_prompt = (
            "You are a context-compression assistant.  Summarise the following "
            "conversation into a concise, bullet-point brief capturing all key "
            "technical decisions, file names, todos and reasoning required to "
            "continue the task.  Keep it under 800 words.  Do **not** lose any "
            "important detail needed for future work.\n\nCONVERSATION:\n"
            + conversation_text
        )

        response = await self.provider.chat(
            messages=[Message(role="user", content=summarise_prompt)],
            temperature=get_config('llm.temperature', 0.2),
            max_tokens=self.max_summary_tokens,
        )

        return Message(
            role="system", content="COMPRESSED CONTEXT SUMMARY:\n" + response.content
        )
