"""Zero-import remote function invocation.

Invoker pattern for running Python functions remotely without requiring Flow
imports in user code. Keeps user code independent from infrastructure.

Example:
    User's ML code (train.py) - Pure Python, no Flow imports::

        def train_model(data_path: str, epochs: int = 100):
            import torch
            model = torch.nn.Linear(10, 1)
            # ... training logic
            return {"accuracy": 0.95, "loss": 0.01}

    Infrastructure code (runner.py) - Only this needs Flow::

        from flow import invoke

        result = invoke("train.py", "train_model",
                       args=["s3://bucket/data"],
                       kwargs={"epochs": 200},
                       gpu="a100",
                       max_price_per_hour=25.0)
        print(result)  # {"accuracy": 0.95, "loss": 0.01}

Serialization approach:
    Arguments and return values must be JSON-serializable. This constraint is
    intentional - it ensures debuggability and prevents version mismatches.

    For complex types (numpy arrays, models), save to disk and pass paths::

        # Instead of: invoke("train.py", "process", args=[numpy_array])
        # Do:
        np.save("/tmp/data.npy", numpy_array)
        result = invoke("train.py", "process", args=["/tmp/data.npy"])

    This explicit approach creates more reliable, debuggable systems.
"""

import json
from pathlib import Path
from typing import Any

from flow import Flow
from flow.api.models import Task, TaskConfig
from flow.errors import ValidationError


def _serialize_to_json(obj: Any, obj_name: str) -> str:
    """Serialize object to JSON with helpful error messages.

    Args:
        obj: Object to serialize.
        obj_name: Human-readable name for error messages.

    Returns:
        JSON string representation.

    Raises:
        TypeError: If object cannot be serialized to JSON.
    """
    try:
        return json.dumps(obj)
    except (TypeError, ValueError) as e:
        # Show a preview of the problematic object
        obj_repr = repr(obj)[:100] + "..." if len(repr(obj)) > 100 else repr(obj)
        obj_type = type(obj).__name__

        # Provide type-specific guidance
        specific_help = ""
        if "numpy" in obj_type.lower() or "ndarray" in obj_type:
            specific_help = "np.save('/tmp/data.npy', obj) then pass '/tmp/data.npy'"
        elif "dataframe" in obj_type.lower():
            specific_help = "df.to_parquet('/tmp/data.parquet') then pass the path"
        elif "tensor" in obj_type.lower():
            specific_help = "torch.save(obj, '/tmp/model.pt') then pass the path"
        elif hasattr(obj, "__dict__"):
            specific_help = "save with pickle.dump() or convert to dict"
        else:
            specific_help = "convert to a JSON-serializable type or save to disk"

        raise TypeError(
            f"Cannot serialize {obj_name} to JSON.\n"
            f"Value: {obj_repr}\n"
            f"Type: {obj_type}\n"
            f"\n"
            f"For {obj_type} objects, {specific_help}\n"
            f"\n"
            f"JSON only supports: dict, list, str, int, float, bool, None\n"
            f"This explicit approach ensures reproducibility and debuggability."
        ) from e


def _create_invoke_script(
    module_path: str,
    function_name: str,
    args: list[Any],
    kwargs: dict[str, Any],
    result_path: str,
    temp_dir: str | None = None,
    max_result_size: int = 10 * 1024 * 1024,
) -> str:
    """Create the wrapper script for remote function invocation.

    Args:
        module_path: Path to the Python module.
        function_name: Name of the function to call.
        args: Positional arguments for the function.
        kwargs: Keyword arguments for the function.
        result_path: Path where result JSON will be written.
        temp_dir: Optional temporary directory to clean up (for async).
        max_result_size: Maximum allowed result size in bytes.

    Returns:
        The Python script as a string.
    """
    # Common script template
    if temp_dir:
        # Async version with cleanup
        return '''
import sys
import json
import importlib.util
import os
import shutil

# Safely load parameters
module_path = {module_path_repr}
function_name = {function_name_repr}
args = {args_json}
kwargs = {kwargs_json}
result_path = {result_path_repr}
temp_dir = {temp_dir_repr}
max_result_size = {max_result_size}

def cleanup():
    """Clean up temporary directory."""
    try:
        shutil.rmtree(temp_dir)
    except:
        pass

# Load the module
try:
    print(f"Loading function '{{function_name}}' from {{module_path}}")
    spec = importlib.util.spec_from_file_location("user_module", module_path)
    if not spec or not spec.loader:
        print("ERROR: Could not load module", file=sys.stderr)
        cleanup()
        sys.exit(1)

    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)

    # Get the function
    if not hasattr(module, function_name):
        print(f"ERROR: Function '{{function_name}}' not found in module", file=sys.stderr)
        cleanup()
        sys.exit(1)

    func = getattr(module, function_name)

    # Call with provided arguments
    print(f"Executing {{function_name}}...")
    result = func(*args, **kwargs)
    print(f"Execution completed successfully")
    
    # Serialize result to check size
    result_json = json.dumps(result)
    result_size = len(result_json.encode('utf-8'))
    
    if result_size > max_result_size:
        print(f"ERROR: Result too large ({{result_size}} bytes > {{max_result_size}} bytes)", file=sys.stderr)
        cleanup()
        sys.exit(1)
    
    # Write result to file
    with open(result_path, 'w') as f:
        f.write(result_json)
    
    print(f"Result written to {{result_path}} ({{result_size}} bytes)")
    
except Exception as e:
    print(f"ERROR: Function execution failed: {{e}}", file=sys.stderr)
    import traceback
    traceback.print_exc()
    cleanup()
    sys.exit(1)
'''.format(
            module_path_repr=repr(str(module_path)),
            function_name_repr=repr(function_name),
            args_json=_serialize_to_json(args, "arguments"),
            kwargs_json=_serialize_to_json(kwargs, "keyword arguments"),
            result_path_repr=repr(str(result_path)),
            temp_dir_repr=repr(str(temp_dir)),
            max_result_size=max_result_size,
        )
    else:
        # Sync version without cleanup
        return """
import sys
import json
import importlib.util
import os

# Safely load parameters
module_path = {module_path_repr}
function_name = {function_name_repr}
args = {args_json}
kwargs = {kwargs_json}
result_path = {result_path_repr}

# Load the module
print(f"Loading function '{{function_name}}' from {{module_path}}")
spec = importlib.util.spec_from_file_location("user_module", module_path)
if not spec or not spec.loader:
    print("ERROR: Could not load module", file=sys.stderr)
    sys.exit(1)

module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(module)

# Get the function
if not hasattr(module, function_name):
    print(f"ERROR: Function '{{function_name}}' not found in module", file=sys.stderr)
    sys.exit(1)

func = getattr(module, function_name)

# Call with provided arguments
try:
    print(f"Executing {{function_name}}...")
    result = func(*args, **kwargs)
    print(f"Execution completed successfully")
    
    # Check result size before writing
    result_json = json.dumps(result)
    result_size = len(result_json.encode('utf-8'))
    max_result_size = {max_result_size}  # User-specified limit
    
    if result_size > max_result_size:
        print(f"ERROR: Result too large ({{result_size}} bytes > {{max_result_size}} bytes)", file=sys.stderr)
        print("Consider saving large results to disk and returning the path instead:", file=sys.stderr)
        print("  return '/tmp/results.npy'  # Instead of returning the array", file=sys.stderr)
        sys.exit(1)
    
    # Write result to file (side-channel)
    with open(result_path, 'w') as f:
        f.write(result_json)
    
    print(f"Result written to {{result_path}} ({{result_size}} bytes)")
except Exception as e:
    print(f"ERROR: Function execution failed: {{e}}", file=sys.stderr)
    import traceback
    traceback.print_exc()
    sys.exit(1)
""".format(
            module_path_repr=repr(str(module_path)),
            function_name_repr=repr(function_name),
            args_json=_serialize_to_json(args, "arguments"),
            kwargs_json=_serialize_to_json(kwargs, "keyword arguments"),
            result_path_repr=repr(str(result_path)),
            max_result_size=max_result_size,
        )


def _parse_invoke_result(result_json: str) -> Any:
    """Parse the JSON result from an invoke operation.

    Args:
        result_json: JSON string to parse.

    Returns:
        The parsed result.

    Raises:
        json.JSONDecodeError: If the JSON is invalid.
    """
    return json.loads(result_json)


class InvokeTask:
    """Result handle for async invoke operations.

    Thin wrapper around Task providing a convenient get_result().
    """

    def __init__(self, task: Task):
        self.task = task

    def get_result(self) -> Any:
        """Block until completion and return function result or raise error."""
        if not self.task.is_terminal:
            self.task.wait()
        return self.task.result()


def invoke(
    module_path: str,
    function_name: str,
    args: list[Any] | None = None,
    kwargs: dict[str, Any] | None = None,
    *,
    code_root: str | Path | None = None,
    max_result_size: int = 10 * 1024 * 1024,
    retries: int = 0,
    retry_delay: float = 1.0,
    retry_backoff: float = 2.0,
    **task_params,
) -> Any:
    """Run a Python function remotely with zero imports needed in user code.

    This function enables true portability by keeping ML code free of Flow
    dependencies. The remote function is executed in its natural environment
    and its return value is captured and returned.

    Compatible with Linux/macOS only (uses Unix file paths and commands).
    Result size limited to 10MB by default (configurable).

    Args:
        module_path: Path to Python module (e.g., "train.py" or "src/model.py")
        function_name: Name of function to call
        args: Positional arguments for the function
        kwargs: Keyword arguments for the function
        max_result_size: Maximum result size in bytes (default 10MB).
            Increase for large data science results.
        retries: Number of retry attempts if task fails (default 0)
        retry_delay: Initial delay between retries in seconds (default 1.0)
        retry_backoff: Exponential backoff multiplier (default 2.0)
        **task_params: Flow task parameters (gpu, instance_type, max_price_per_hour, etc.)

    Returns:
        The return value of the remote function (deserialized from JSON)

    Raises:
        FlowError: If task submission or execution fails
        ValueError: If module or function cannot be found
        JSONDecodeError: If function return value cannot be serialized

    Example:
        Basic usage:
        >>> result = invoke("process.py", "analyze_data", args=["input.csv"])

        With GPU and arguments:
        >>> metrics = invoke(
        ...     "train.py",
        ...     "train_model",
        ...     kwargs={"batch_size": 32, "lr": 0.001},
        ...     gpu="a100",
        ...     max_price_per_hour=20.0
        ... )

        Large results:
        >>> result = invoke("analysis.py", "process_data",
        ...                 max_result_size=100*1024*1024)  # 100MB limit

        With retry logic:
        >>> result = invoke("train.py", "train_model",
        ...                 retries=3,  # Retry up to 3 times
        ...                 retry_delay=2.0,  # Start with 2s delay
        ...                 retry_backoff=2.0)  # Double delay each retry
    """
    # Validate module path and compute remote mapping under /workspace
    local_module_path = Path(module_path)
    if not local_module_path.exists():
        raise ValidationError(
            f"Module not found: {local_module_path}",
            suggestions=[
                "Check the file path is correct and exists",
                "Use an absolute path for clarity",
                "Ensure the file has a .py extension",
            ],
            error_code="VALIDATION_003",
        )

    code_root_path = Path(code_root) if code_root is not None else Path.cwd()
    try:
        rel_module = local_module_path.resolve().relative_to(code_root_path.resolve())
    except Exception:
        raise ValidationError(
            f"Module path must be inside code_root ({code_root_path})",
            suggestions=[
                "Set code_root to your project directory",
                "Or move the module under the project root so it uploads",
            ],
            error_code="VALIDATION_004",
        )

    remote_module_path = "/workspace/" + rel_module.as_posix()

    # Build the inline wrapper script and heredoc command
    def _create_invoke_wrapper_script(max_result_size: int = 10 * 1024 * 1024) -> str:
        return (
            "import json\n"
            "import os\n"
            "import sys\n"
            "import importlib.util\n"
            "import traceback\n"
            "module_path = os.environ.get('INVOKE_MODULE_PATH')\n"
            "function_name = os.environ.get('INVOKE_FUNCTION_NAME')\n"
            "args_json = os.environ.get('INVOKE_ARGS_JSON', '[]')\n"
            "kwargs_json = os.environ.get('INVOKE_KWARGS_JSON', '{}')\n"
            "result_path = os.environ.get('INVOKE_RESULT_PATH', '/tmp/flow_result.json')\n"
            f"max_result_size = int({max_result_size})\n"
            "def write_error(err_type, message, tb=None):\n"
            "    payload = {'success': False, 'error': {'type': err_type, 'message': message, 'traceback': tb}}\n"
            "    try:\n"
            "        with open(result_path, 'w') as f:\n"
            "            json.dump(payload, f)\n"
            "    except Exception:\n"
            "        pass\n"
            "try:\n"
            "    try:\n"
            "        args = json.loads(args_json)\n"
            "        kwargs = json.loads(kwargs_json)\n"
            "        if not isinstance(args, list) or not isinstance(kwargs, dict):\n"
            "            raise ValueError('Invalid args/kwargs shapes')\n"
            "    except Exception as e:\n"
            "        write_error(type(e).__name__, f'Failed to parse arguments: {e}', traceback.format_exc())\n"
            "        sys.exit(1)\n"
            "    try:\n"
            "        spec = importlib.util.spec_from_file_location('user_module', module_path)\n"
            "        if not spec or not spec.loader:\n"
            "            raise ImportError('Could not load module spec')\n"
            "        module = importlib.util.module_from_spec(spec)\n"
            "        spec.loader.exec_module(module)\n"
            "    except Exception as e:\n"
            "        write_error(type(e).__name__, f\"Failed to load module '{module_path}': {e}\", traceback.format_exc())\n"
            "        sys.exit(1)\n"
            "    try:\n"
            "        if not hasattr(module, function_name):\n"
            "            raise AttributeError(f\"Function '{function_name}' not found in module\")\n"
            "        func = getattr(module, function_name)\n"
            "    except Exception as e:\n"
            "        write_error(type(e).__name__, f\"Failed to resolve function '{function_name}': {e}\", traceback.format_exc())\n"
            "        sys.exit(1)\n"
            "    try:\n"
            "        result = func(*args, **kwargs)\n"
            "    except Exception as e:\n"
            "        write_error(type(e).__name__, f'Function raised: {e}', traceback.format_exc())\n"
            "        sys.exit(1)\n"
            "    try:\n"
            "        result_json = json.dumps(result)\n"
            "        result_size = len(result_json.encode('utf-8'))\n"
            "        if result_size > max_result_size:\n"
            '            raise ValueError(f"Result too large ({result_size} bytes > {max_result_size} bytes)")\n'
            "        with open(result_path, 'w') as f:\n"
            "            json.dump({'success': True, 'result': result}, f)\n"
            "    except Exception as e:\n"
            "        write_error(type(e).__name__, f'Failed to serialize/write result: {e}', traceback.format_exc())\n"
            "        sys.exit(1)\n"
            "except SystemExit:\n"
            "    raise\n"
        )

    def _build_remote_command(wrapper_code: str) -> str:
        return "python - <<'PY'\n" + wrapper_code + "\nPY"

    wrapper_code = _create_invoke_wrapper_script(max_result_size=max_result_size)
    command = _build_remote_command(wrapper_code)

    # Build task params
    invoke_env = {
        "INVOKE_MODULE_PATH": remote_module_path,
        "INVOKE_FUNCTION_NAME": function_name,
        "INVOKE_ARGS_JSON": _serialize_to_json(args or [], "arguments"),
        "INVOKE_KWARGS_JSON": _serialize_to_json(kwargs or {}, "keyword arguments"),
        "INVOKE_RESULT_PATH": "/tmp/flow_result.json",
    }

    flow_params = {
        "name": task_params.pop("name", f"invoke-{function_name}"),
        "command": command,
        "upload_code": True,
        "working_dir": "/workspace",
        "num_instances": 1,
        "env": invoke_env,
    }

    if "gpu" in task_params:
        flow_params["instance_type"] = task_params.pop("gpu")
    elif "instance_type" in task_params:
        flow_params["instance_type"] = task_params.pop("instance_type")

    # Merge user env (invocation vars take precedence)
    user_env = task_params.pop("env", {}) or {}
    if not isinstance(user_env, dict):
        raise ValidationError(
            "env must be a dictionary of string->string",
            suggestions=["Provide environment variables as a dict"],
            error_code="VALIDATION_005",
        )
    flow_params["env"] = {**user_env, **flow_params["env"]}

    # Prevent multi-instance overrides
    task_params.pop("num_instances", None)

    # Pass through remaining parameters
    flow_params.update(task_params)

    config = TaskConfig(**flow_params)

    # Run the task with retry logic
    import time

    last_exception = None
    for attempt in range(retries + 1):
        try:
            with Flow() as flow:
                task = flow.run(config, wait=True)
                task.wait()
                return task.result()
        except Exception as e:
            if attempt < retries:
                last_exception = e
                delay = retry_delay * (retry_backoff**attempt)
                time.sleep(delay)
                continue
            raise
    if last_exception:
        raise last_exception


# Convenience function for async execution
def invoke_async(
    module_path: str,
    function_name: str,
    args: list[Any] | None = None,
    kwargs: dict[str, Any] | None = None,
    *,
    code_root: str | Path | None = None,
    max_result_size: int = 10 * 1024 * 1024,
    **task_params,
) -> "InvokeTask":
    """Submit a function for remote execution without waiting.

    Returns an InvokeTask object that provides result retrieval and cleanup.
    Compatible with Linux/macOS only (uses Unix file paths).

    Args:
        module_path: Path to Python module
        function_name: Name of function to call
        args: Positional arguments for the function
        kwargs: Keyword arguments for the function
        max_result_size: Maximum result size in bytes (default 10MB)
        **task_params: Flow task parameters

    Returns:
        InvokeTask object with get_result() method

    Example:
        >>> invoke_task = invoke_async("train.py", "train_model", gpu="a100")
        >>> # Do other work...
        >>> invoke_task.task.wait()  # Access underlying task
        >>> result = invoke_task.get_result()
        >>> print(result)
    """
    # Validate module path and compute remote mapping
    local_module_path = Path(module_path)
    if not local_module_path.exists():
        raise ValidationError(
            f"Module not found: {local_module_path}",
            suggestions=[
                "Check the file path is correct and exists",
                "Use an absolute path for clarity",
                "Ensure the file has a .py extension",
            ],
            error_code="VALIDATION_003",
        )

    code_root_path = Path(code_root) if code_root is not None else Path.cwd()
    try:
        rel_module = local_module_path.resolve().relative_to(code_root_path.resolve())
    except Exception:
        raise ValidationError(
            f"Module path must be inside code_root ({code_root_path})",
            suggestions=[
                "Set code_root to your project directory",
                "Or move the module under the project root so it uploads",
            ],
            error_code="VALIDATION_004",
        )

    remote_module_path = "/workspace/" + rel_module.as_posix()

    # Build wrapper and command
    def _create_invoke_wrapper_script(max_result_size: int = 10 * 1024 * 1024) -> str:
        return (
            "import json\n"
            "import os\n"
            "import sys\n"
            "import importlib.util\n"
            "import traceback\n"
            "module_path = os.environ.get('INVOKE_MODULE_PATH')\n"
            "function_name = os.environ.get('INVOKE_FUNCTION_NAME')\n"
            "args_json = os.environ.get('INVOKE_ARGS_JSON', '[]')\n"
            "kwargs_json = os.environ.get('INVOKE_KWARGS_JSON', '{}')\n"
            "result_path = os.environ.get('INVOKE_RESULT_PATH', '/tmp/flow_result.json')\n"
            f"max_result_size = int({max_result_size})\n"
            "def write_error(err_type, message, tb=None):\n"
            "    payload = {'success': False, 'error': {'type': err_type, 'message': message, 'traceback': tb}}\n"
            "    try:\n"
            "        with open(result_path, 'w') as f:\n"
            "            json.dump(payload, f)\n"
            "    except Exception:\n"
            "        pass\n"
            "try:\n"
            "    try:\n"
            "        args = json.loads(args_json)\n"
            "        kwargs = json.loads(kwargs_json)\n"
            "        if not isinstance(args, list) or not isinstance(kwargs, dict):\n"
            "            raise ValueError('Invalid args/kwargs shapes')\n"
            "    except Exception as e:\n"
            "        write_error(type(e).__name__, f'Failed to parse arguments: {e}', traceback.format_exc())\n"
            "        sys.exit(1)\n"
            "    try:\n"
            "        spec = importlib.util.spec_from_file_location('user_module', module_path)\n"
            "        if not spec or not spec.loader:\n"
            "            raise ImportError('Could not load module spec')\n"
            "        module = importlib.util.module_from_spec(spec)\n"
            "        spec.loader.exec_module(module)\n"
            "    except Exception as e:\n"
            "        write_error(type(e).__name__, f\"Failed to load module '{module_path}': {e}\", traceback.format_exc())\n"
            "        sys.exit(1)\n"
            "    try:\n"
            "        if not hasattr(module, function_name):\n"
            "            raise AttributeError(f\"Function '{function_name}' not found in module\")\n"
            "        func = getattr(module, function_name)\n"
            "    except Exception as e:\n"
            "        write_error(type(e).__name__, f\"Failed to resolve function '{function_name}': {e}\", traceback.format_exc())\n"
            "        sys.exit(1)\n"
            "    try:\n"
            "        result = func(*args, **kwargs)\n"
            "    except Exception as e:\n"
            "        write_error(type(e).__name__, f'Function raised: {e}', traceback.format_exc())\n"
            "        sys.exit(1)\n"
            "    try:\n"
            "        result_json = json.dumps(result)\n"
            "        result_size = len(result_json.encode('utf-8'))\n"
            "        if result_size > max_result_size:\n"
            '            raise ValueError(f"Result too large ({result_size} bytes > {max_result_size} bytes)")\n'
            "        with open(result_path, 'w') as f:\n"
            "            json.dump({'success': True, 'result': result}, f)\n"
            "    except Exception as e:\n"
            "        write_error(type(e).__name__, f'Failed to serialize/write result: {e}', traceback.format_exc())\n"
            "        sys.exit(1)\n"
            "except SystemExit:\n"
            "    raise\n"
        )

    def _build_remote_command(wrapper_code: str) -> str:
        return "python - <<'PY'\n" + wrapper_code + "\nPY"

    wrapper_code = _create_invoke_wrapper_script(max_result_size=max_result_size)
    command = _build_remote_command(wrapper_code)

    invoke_env = {
        "INVOKE_MODULE_PATH": remote_module_path,
        "INVOKE_FUNCTION_NAME": function_name,
        "INVOKE_ARGS_JSON": _serialize_to_json(args or [], "arguments"),
        "INVOKE_KWARGS_JSON": _serialize_to_json(kwargs or {}, "keyword arguments"),
        "INVOKE_RESULT_PATH": "/tmp/flow_result.json",
    }

    flow_params = {
        "name": task_params.pop("name", f"invoke-{function_name}"),
        "command": command,
        "upload_code": True,
        "working_dir": "/workspace",
        "num_instances": 1,
        "env": invoke_env,
    }

    if "gpu" in task_params:
        flow_params["instance_type"] = task_params.pop("gpu")
    elif "instance_type" in task_params:
        flow_params["instance_type"] = task_params.pop("instance_type")

    user_env = task_params.pop("env", {}) or {}
    if not isinstance(user_env, dict):
        raise ValidationError(
            "env must be a dictionary of string->string",
            suggestions=["Provide environment variables as a dict"],
            error_code="VALIDATION_005",
        )
    flow_params["env"] = {**user_env, **flow_params["env"]}

    task_params.pop("num_instances", None)
    flow_params.update(task_params)
    config = TaskConfig(**flow_params)

    flow = Flow()
    task = flow.run(config, wait=False)
    return InvokeTask(task)
