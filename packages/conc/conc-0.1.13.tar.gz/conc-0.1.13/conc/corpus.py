"""Create a conc corpus."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/api/45_corpus.ipynb.

# %% ../nbs/api/45_corpus.ipynb 3
from __future__ import annotations
import re
import polars as pl
import numpy as np
from great_tables import GT
import os
import glob
import spacy
from spacy.attrs import ORTH, LOWER, SPACY # May extend to POS, TAG, SENT_START, LEMMA
import string
from fastcore.basics import patch
import time
from slugify import slugify
import msgspec # tested against orjson - with validation was faster, without around the same
import unicodedata
import sys

# %% auto 0
__all__ = ['NOT_DOC_TOKEN', 'INDEX_HEADER_LENGTH', 'PUNCTUATION_STRINGS', 'README_TEMPLATE', 'Corpus', 'build_test_corpora']

# %% ../nbs/api/45_corpus.ipynb 5
from . import __version__
from .core import logger, CorpusMetadata, PAGE_SIZE, EOF_TOKEN_STR, ERR_TOKEN_STR, REPOSITORY_URL, DOCUMENTATION_URL, CITATION_STR, PYPI_URL
from .result import Result
from .text import Text

# %% ../nbs/api/45_corpus.ipynb 7
polars_conf = pl.Config.set_tbl_hide_column_data_types(True)
polars_conf = pl.Config.set_tbl_hide_dataframe_shape(True)
polars_conf = pl.Config.set_tbl_rows(50)
polars_conf = pl.Config.set_tbl_width_chars(300)
polars_conf = pl.Config.set_fmt_str_lengths(300)

# %% ../nbs/api/45_corpus.ipynb 8
_RE_COMBINE_WHITESPACE = re.compile(r"\s+")
_RE_PUNCT = re.compile(r"^[^\s^\w^\d]$")

# %% ../nbs/api/45_corpus.ipynb 9
NOT_DOC_TOKEN = -1
INDEX_HEADER_LENGTH = 100

# %% ../nbs/api/45_corpus.ipynb 10
PUNCTUATION_STRINGS = ''.join(set(list(string.punctuation) + 
                                [chr(i) for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)).startswith("P")] + 
                                [chr(i) for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)).startswith("Sc")]
								))

# %% ../nbs/api/45_corpus.ipynb 15
class Corpus:
	"""Represention of text corpus, with methods to build, load and save a corpus from a variety of formats and to work with the corpus data."""
	
	def __init__(self, 
				name: str = '', # name of corpus
				description: str = '' # description of corpus
				):
		# information about corpus
		self.name = name
		self.description = description
		self.slug = None

		# conc version that built the corpus
		self.conc_version = None
		
		# paths
		self.corpus_path = None
		self.source_path = None

		# settings
		self.SPACY_MODEL = None
		self.SPACY_MODEL_VERSION = None
		self.SPACY_EOF_TOKEN = None # set below as nlp.vocab[EOF_TOKEN_STR].orth in build or through load  - EOF_TOKEN_STR starts with space so eof_token can't match anything from corpus
		self.EOF_TOKEN = None

		# special token ids
		self.punct_tokens = None
		self.space_tokens = None

		# metadata for corpus
		self.document_count = None
		self.token_count = None
		self.unique_tokens = None

		self.word_token_count = None
		self.unique_word_tokens = None

		self.date_created = None

		# token data
		self.tokens = None
		self.vocab = None

		self.puncts = None
		self.spaces = None

		# metadata for each document
		self.metadata = None

		self.ngram_index = {}
		self.results_cache = {}

		self.expected_files_ = ['corpus.json', 'vocab.parquet', 'tokens.parquet', 'puncts.parquet', 'spaces.parquet']
		self.required_tables_ = ['vocab', 'tokens', 'puncts', 'spaces']

# %% ../nbs/api/45_corpus.ipynb 17
@patch
def _init_spacy_model(self: Corpus,
                model: str = 'en_core_web_sm', # spacy model to use for tokenization
				version: str|None = None, # version of spacy model expected, if mismatch will raise a warning
				standardize_word_token_punctuation_characters: bool = False # whether to standardize apostrophes in word tokens
				):
	try:
		self._nlp = spacy.load(model)
		self._nlp.disable_pipes(['parser', 'ner', 'lemmatizer', 'tagger', 'senter', 'tok2vec', 'attribute_ruler'])
		self._nlp.max_length = 10_000_000 # set max length to a large number to avoid issues with long documents
	except OSError as e:
		logger.error(f'Error loading model {model}. If you are working with texts in English, you need to run python -m spacy download en_core_web_sm to download the model. See https://spacy.io/models for available models for other languages.')
		raise e
	
	if version is not None:
		if self._nlp.meta['version'] != version:
			logger.warning(f'Spacy model version mismatch: expecting {version}, got {self._nlp.meta["version"]}. This may cause issues with tokenization.')

	if standardize_word_token_punctuation_characters:
		rules = self._nlp.tokenizer.rules.copy()
		self._standardize_replacements = {}
		for key in list(rules.keys()):
			if key.strip(PUNCTUATION_STRINGS) != '' and "’" in key and key.replace("’", "'") in rules: # only standardize word tokens
				for token in rules[key]:
					for k, v in token.items():
						if "’" in v:
							self._standardize_replacements[v] = v.replace("’", "'")
				self._nlp.tokenizer.rules[key] = [{k: v.replace("’", "'") for k, v in token.items()} for token in rules[key]]
				logger.debug(f"Updating rule to standardize word token punctuation for key: {key}, value: {rules[key]}, replaced with: {self._nlp.tokenizer.rules[key]}")
		logger.debug(f"Standardized word token rules: {self._standardize_replacements}")
		self._standardize_replacements_ids = {self._nlp.vocab.strings[k]: self._nlp.vocab.strings[v] for k, v in self._standardize_replacements.items()}
		logger.debug(f"Standardized word token rules as ids: {self._standardize_replacements_ids}")


# %% ../nbs/api/45_corpus.ipynb 18
@patch
def _process_punct_positions(self: Corpus):
	""" Process punctuation positions in token data and populates punct_tokens and punct_positions. """

	self.punct_tokens = np.array(list({k: v for k, v in self.vocab.items() if v.strip(PUNCTUATION_STRINGS) == ''}.keys()))
	punct_mask = np.isin(self.lower_index, self.punct_tokens) # faster to retrieve with isin than where
	self.punct_positions = np.nonzero(punct_mask)[0] # storing this as smaller

# %% ../nbs/api/45_corpus.ipynb 19
@patch
def _process_space_positions(self: Corpus):
	""" Process whitespace positions in token data and populates space_tokens and space_positions. """

	self.space_tokens = np.array(list({k: v for k, v in self.vocab.items() if v.strip() == ''}.keys()))
	space_mask = np.isin(self.lower_index, self.space_tokens) 	# faster to retrieve with isin than where
	self.space_positions = np.nonzero(space_mask)[0] # storing this as smaller


# %% ../nbs/api/45_corpus.ipynb 24
@patch
def _init_build_process(self:Corpus,
						save_path: str, # path to save corpus data 
						):
	""" Create slug, corpus_path, and create directory if needed. """

	self.conc_version = __version__
	self.slug = slugify(self.name, stopwords=['corpus'])
	self.corpus_path = os.path.join(save_path, f'{self.slug}.corpus')

	if not os.path.isdir(self.corpus_path):
		os.makedirs(self.corpus_path)

# %% ../nbs/api/45_corpus.ipynb 25
@patch
def _update_build_process(self: Corpus, 
                           orth_index: list[np.ndarray], # orthographic token ids
                           lower_index: list[np.ndarray], # lower case token ids
                           token2doc_index: list[np.ndarray], # token to document mapping
                           has_spaces: list[np.ndarray], # arrays of whether token has space
                           store_pos: int # current store pos
                           ) -> int: # next store pos
    """ Write in-progress build data to Parquet disk store. """

    pl.DataFrame([np.concatenate(orth_index), np.concatenate(lower_index), np.concatenate(token2doc_index), np.concatenate(has_spaces)], schema = [('orth_index', pl.UInt64), ('lower_index', pl.UInt64), ('token2doc_index', pl.Int32), ('has_spaces', pl.Boolean)] ).write_parquet(f'{self.corpus_path}/build_{store_pos}.parquet')
    return store_pos + 1

# %% ../nbs/api/45_corpus.ipynb 27
@patch
def _complete_build_process(self: Corpus, 
							build_process_cleanup: bool = True,  # Remove the build files after build is complete, retained for development and testing purposes
							standardize_word_token_punctuation_characters: bool = False # whether to standardize apostrophes in word tokens
							):
	""" Complete the disk-based build to create representation of the corpus. """

	logger.memory_usage('init', init=True)
	tokens_df = pl.scan_parquet(f'{self.corpus_path}/build_*.parquet')

	if standardize_word_token_punctuation_characters:
		# tokenizsation still seems to let some word tokens through, even with revised rules, so this is a final check to standardize word tokens
		tokens_df = tokens_df.with_columns(pl.col('orth_index').replace(self._standardize_replacements_ids), pl.col('lower_index').replace(self._standardize_replacements_ids)) # replace orth and lower with standardized versions
		
	# get unique vocab ids (combining orth and lower) and create new index
	vocab_df = pl.concat([tokens_df.select(pl.col('orth_index').unique().alias('index')), tokens_df.select(pl.col('lower_index').unique().alias('index'))])
	#vocab_df  = combined_df.select(pl.col('index').unique().sort().alias('source_id')).with_row_index('token_id', offset=1) #.collect(engine='streaming')
	vocab_df = vocab_df.select(pl.col('index').unique().sort().alias('source_id')).with_row_index('token_id', offset=1) 
	logger.memory_usage('collected vocab')

	# combined_df = (combined_df.with_columns(pl.col('index').replace(vocab_df.select(pl.col('source_id'))['source_id'], vocab_df.select(pl.col('token_id'))['token_id']).cast(pl.UInt32)))
	# combined_df = combined_df.with_columns(pl.col('index').cast(pl.UInt32))

	tokens_df = (
		tokens_df
		.join(vocab_df, left_on="orth_index", right_on="source_id", how="left", maintain_order="left")
		.drop("orth_index")
		.rename({"token_id": "orth_index"})
		.with_columns(pl.col("orth_index").cast(pl.UInt32).alias("orth_index"))
	)

	tokens_df = (
		tokens_df
		.join(vocab_df, left_on="lower_index", right_on="source_id", how="left", maintain_order="left")
		.drop("lower_index")
		.rename({"token_id": "lower_index"})
		.with_columns(pl.col("lower_index").cast(pl.UInt32).alias("lower_index"))
	) # should have aligned data for token2doc_index and has_spaces

	# this is currently the most intensive operation in terms of memory usage - this needs attention - can't use sink_parquet currently or collect(engine='streaming') as it doesn't maintain order (including changing order between two columns)
	tokens_df.select([pl.col('orth_index'), pl.col('lower_index'), pl.col('token2doc_index'), pl.col('has_spaces')]).collect().write_parquet(f'{self.corpus_path}/tokens.parquet') # if using streaming or sink will need to verify ordering - check for maintain_order parameters
	logger.memory_usage('wrote pending tokens to disk')
	tokens_df = pl.scan_parquet(f'{self.corpus_path}/tokens.parquet') # re-read as lazy frame

	# could batch this to reduce memory usage - but leaving for now
	vocab_query = vocab_df.select(pl.col('source_id')).collect().to_numpy().flatten() # get vocab ids as numpy array for faster processing
	vocab = {k:self._nlp.vocab[k].text for k in vocab_query} # get vocab strings from spacy vocab
	token_strs = list(vocab.values())
	vocab_df = vocab_df.with_columns(pl.Series(token_strs).alias('token'))
	del vocab_query
	logger.memory_usage('added vocab strings')

	self.EOF_TOKEN = vocab_df.filter(pl.col('source_id') == self.SPACY_EOF_TOKEN).select(pl.col('token_id')).collect().item() # casting to int for storage
	
	self.punct_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip(PUNCTUATION_STRINGS) == '']
	logger.memory_usage(f'got punct tokens')
	self.space_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip() == '']
	logger.memory_usage(f'got space tokens')
	del token_strs

	# new spaces handling
	spaces_df = tokens_df.with_row_index('position').filter(pl.col('lower_index').is_in(self.space_tokens))
	spaces_df = spaces_df.with_row_index('adjust_by').with_columns((pl.col('position') - pl.col('adjust_by')).alias('corrected'))
	spaces_df = spaces_df.with_columns(pl.col('corrected').alias('position')).drop('adjust_by').drop('corrected')
	spaces_df.collect().write_parquet(f'{self.corpus_path}/spaces.parquet') 
	logger.memory_usage('saved space positions')

	# remove spaces from tokens_df
	tokens_df = tokens_df.filter(~pl.col('lower_index').is_in(self.space_tokens))
	tokens_df.collect().write_parquet(f'{self.corpus_path}/tokens.parquet') # if using streaming or sink will need to verify ordering - check for maintain_order parameters
	logger.memory_usage('wrote final tokens to disk')
	tokens_df = pl.scan_parquet(f'{self.corpus_path}/tokens.parquet') # re-read as lazy frame

	# Create LazyFrames for punct_positions
	tokens_df.select(pl.col('lower_index')).with_row_index('position').filter(pl.col('lower_index').is_in(self.punct_tokens)).select('position').sink_parquet(f'{self.corpus_path}/puncts.parquet', maintain_order = True) #.collect(engine='streaming').to_numpy().flatten()
	logger.memory_usage('saved punct positions')

	# get counts from tokens_df
	frequency_lower = tokens_df.filter(pl.col('lower_index') != self.EOF_TOKEN).select(pl.col('lower_index')).group_by('lower_index').agg(pl.count('lower_index').alias('frequency_lower')) #.collect(engine='streaming')
	frequency_orth = tokens_df.filter(pl.col('orth_index') != self.EOF_TOKEN).select(pl.col('orth_index')).group_by('orth_index').agg(pl.count('orth_index').alias('frequency_orth')) #.collect(engine='streaming')
	vocab_df = vocab_df.join(frequency_lower, left_on = 'token_id', right_on = 'lower_index', how='left', maintain_order="left").join(frequency_orth, left_on = 'token_id', right_on = 'orth_index', how='left', maintain_order="left")
	logger.memory_usage('added frequency to vocab')

	self.unique_tokens = frequency_lower.select(pl.len()).collect(engine='streaming').item() # was len(frequency_lower) before used polars streaming
	logger.memory_usage(f'got unique tokens {self.unique_tokens}')

	del frequency_lower
	del frequency_orth

	# add column for is_punct and is_space based on punct_tokens and space_tokens and token_id
	vocab_df = vocab_df.with_columns((pl.col("token_id").is_in(self.punct_tokens)).alias("is_punct"))
	vocab_df = vocab_df.with_columns((pl.col("token_id").is_in(self.space_tokens)).alias("is_space"))
	vocab_df = vocab_df.sort(by = pl.col('token').str.to_lowercase(), descending = False).with_row_index('tokens_sort_order', offset=1) # leave with no zero for handling of error tokens
	vocab_df = vocab_df.drop('source_id').sort(by = pl.col('frequency_orth'), descending = True, nulls_last = True).with_row_index(name='rank', offset=1)
	logger.memory_usage('added is_punct is_space to vocab')

	vocab_df.collect().write_parquet(f'{self.corpus_path}/vocab.parquet') #, maintain_order = True 
	del vocab_df
	logger.memory_usage('wrote vocab to disk')

	#self.document_count = tokens_df.select(pl.col('token2doc_index').filter(pl.col('token2doc_index') != NOT_DOC_TOKEN).unique().count()).collect(engine='streaming').item()
	self.document_count = tokens_df.select(pl.col('token2doc_index')).max().collect().item()
	logger.memory_usage(f'got doc count {self.document_count}')
	# reading now excludes spaces
	input_length = tokens_df.select(pl.len()).collect(engine='streaming').item() # tested vs count - len seems to have slight memory overhead, but more correct (i.e. count only counts non-null)
	logger.memory_usage(f'got input length {input_length} (with eof headers)')

	# adjusting token count for text breaks and headers at start and end of index
	self.token_count = input_length - self.document_count - INDEX_HEADER_LENGTH - INDEX_HEADER_LENGTH 
	logger.memory_usage(f'got token count {self.token_count}')

	self.punct_token_count = pl.scan_parquet(f'{self.corpus_path}/puncts.parquet').select(pl.len()).collect(engine='streaming').item() # may be more efficient to do this prior to disk write
	logger.memory_usage(f'got punct token count ({self.punct_token_count})')
	self.space_token_count = pl.scan_parquet(f'{self.corpus_path}/spaces.parquet').select(pl.len()).collect(engine='streaming').item() # may be more efficient to do this prior to disk write
	logger.memory_usage(f'got space token count ({self.space_token_count})')
	self.word_token_count = self.token_count - self.punct_token_count
	self.unique_word_tokens = self.unique_tokens - len(self.punct_tokens)
	
	self.date_created = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())

	if build_process_cleanup:
		for f in glob.glob(f'{self.corpus_path}/build_*.parquet'):
			os.remove(f)
		logger.memory_usage('removed build files')
	
	logger.memory_usage('done')



# %% ../nbs/api/45_corpus.ipynb 28
@patch
def _create_indices(self: Corpus, 
				   orth_index: list[np.ndarray], # list of np arrays of orth token ids 
				   lower_index: list[np.ndarray], # list of np arrays of lower token ids
				   token2doc_index: list[np.ndarray] # list of np arrays of doc ids
				   ):
	""" (Deprecated) Use Numpy to create internal representation of the corpus for faster analysis and efficient representation on disk. Only used when the disk-based build process is not used. """
	
	raise DeprecationWarning('This method is deprecated, the current build process uses _complete_build_process instead.')

	self.token2doc_index = np.concatenate(token2doc_index)
	unique_values, inverse = np.unique(np.concatenate(orth_index + lower_index), return_inverse=True)

	# adding a dummy value at the 0 index to avoid 0 being used as a token id
	unique_values = np.insert(unique_values, 0, 0)
	inverse += 1
	new_values = np.arange(len(unique_values), dtype=np.uint32)
	self.original_to_new = dict(zip(unique_values, new_values))
	self.new_to_original = dict(zip(new_values, unique_values))

	self.orth_index = np.array(np.split(inverse, 2)[0], dtype=np.uint32)
	self.lower_index = np.array(np.split(inverse, 2)[1], dtype=np.uint32)
	del inverse

	vocab = {k:self._nlp.vocab.strings[k] for k in unique_values}
	vocab[0] = ERR_TOKEN_STR

	self.vocab = {**{k:vocab[self.new_to_original[k]] for k in new_values}}

	self.EOF_TOKEN = self.original_to_new[self.SPACY_EOF_TOKEN]

	self._process_punct_positions()
	self._process_space_positions()

	self.frequency_lookup = dict(zip(*np.unique(self.lower_index, return_counts=True)))
	del self.frequency_lookup[self.EOF_TOKEN]
	del unique_values

# %% ../nbs/api/45_corpus.ipynb 29
@patch
def _init_corpus_dataframes(self: Corpus):
	""" Initialize dataframes after build or load """
	
	for file in self.expected_files_:
		if not os.path.isfile(os.path.join(self.corpus_path, file)):
			raise FileNotFoundError(f"Expected file '{file}' not found in corpus path '{self.corpus_path}'")

	for file in self.required_tables_:
		self.__setattr__(file, pl.scan_parquet(f'{self.corpus_path}/{file}.parquet'))

	if os.path.isfile(f'{self.corpus_path}/metadata.parquet'):
		self.metadata = pl.scan_parquet(f'{self.corpus_path}/metadata.parquet')

# %% ../nbs/api/45_corpus.ipynb 30
README_TEMPLATE = """# {name}

## About

This directory contains a corpus created using the [Conc]({REPOSITORY_URL}) Python library. 

## Corpus Information

{description}

Date created: {date_created}  
Document count: {document_count}  
Token count: {token_count}  
Word token count: {word_token_count}  
Unique tokens: {unique_tokens}  
Unique word tokens: {unique_word_tokens}  
Conc Version Number: {conc_version}  
spaCy model: {SPACY_MODEL}, version {SPACY_MODEL_VERSION}  

## Using this corpus
 
Conc can be installed [via pip]({PYPI_URL}). The [Conc documentation site]({DOCUMENTATION_URL}) 
has tutorials and detailed information to get you started with Conc or to work with the corpus 
data directly.  

## Cite Conc

{CITATION_STR}

"""

# %% ../nbs/api/45_corpus.ipynb 31
@patch
def save_corpus_metadata(self: Corpus, 
						 template: str = README_TEMPLATE, # template for the README file
		 ):
	""" Save corpus metadata. """
	
	start_time = time.time()
	json_bytes = msgspec.json.encode(CorpusMetadata(**{k: getattr(self, k) for k in ['name', 'description', 'slug', 'conc_version', 'document_count', 'token_count', 'word_token_count', 'punct_token_count', 'space_token_count', 'unique_tokens', 'unique_word_tokens', 'date_created', 'EOF_TOKEN', 'SPACY_EOF_TOKEN', 'SPACY_MODEL', 'SPACY_MODEL_VERSION', 'punct_tokens', 'space_tokens']}))

	with open(f'{self.corpus_path}/corpus.json', 'wb') as f:
		f.write(json_bytes)

	with open(f'{self.corpus_path}/README.md', 'w', encoding='utf-8') as f:
		f.write(README_TEMPLATE.format(
			name=self.name,
			REPOSITORY_URL=REPOSITORY_URL,
			PYPI_URL=PYPI_URL,
			DOCUMENTATION_URL=DOCUMENTATION_URL,
			CITATION_STR=CITATION_STR,
			description=self.description,
			date_created=self.date_created,
			document_count=self.document_count,
			token_count=self.token_count,
			word_token_count=self.word_token_count,
			unique_tokens=self.unique_tokens,
			unique_word_tokens=self.unique_word_tokens,
			conc_version=self.conc_version,
			SPACY_MODEL=self.SPACY_MODEL,
			SPACY_MODEL_VERSION=self.SPACY_MODEL_VERSION
		))
		
	logger.info(f'Saved corpus metadata time: {(time.time() - start_time):.3f} seconds')

# %% ../nbs/api/45_corpus.ipynb 32
@patch
def _build(self: Corpus, 
		  save_path:str, # directory where corpus will be created, a subdirectory will be automatically created with the corpus content
		  iterator: iter, # iterator of texts
		  model: str='en_core_web_sm', # spacy model to use for tokenisation
		  spacy_batch_size:int=500, # batch size for spacy tokenizer
		  build_process_batch_size:int=5000, # save in-progress build to disk every n docs
		  build_process_cleanup:bool = True, # Remove the build files after build is complete, retained for development and testing purposes
		  standardize_word_token_punctuation_characters: bool = False # whether to standardize apostrophes in word tokens
		  ):
	"""Build a corpus from an iterator of texts."""

	self._init_spacy_model(model, standardize_word_token_punctuation_characters=standardize_word_token_punctuation_characters)
	
	self.SPACY_MODEL = model
	self.SPACY_MODEL_VERSION = self._nlp.meta['version']
	self.SPACY_EOF_TOKEN = self._nlp.vocab[EOF_TOKEN_STR].orth
	
	if self.corpus_path is None: # leaving for testing ... this should already be set if build has been initiated in standard way via build_from_csv, build_from_files or whatever other methods are implemented to handle build/imports in future
		self._init_build_process(save_path)
	
	logger.memory_usage('init', init=True)

	start_time = time.time()

	eof_arr = np.array([self.SPACY_EOF_TOKEN], dtype=np.uint64)
	not_doc_arr = np.array([NOT_DOC_TOKEN], dtype=np.int16)
	index_header_arr = np.array([self.SPACY_EOF_TOKEN] * INDEX_HEADER_LENGTH, dtype=np.uint64) # this is added to start and end of index to prevent out of bound issues on searches
	has_spaces_eof_arr = np.array([False], dtype=np.bool)

	orth_index = [index_header_arr]
	lower_index = [index_header_arr]
	token2doc_index = [np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32)]
	has_spaces = [np.array([0] * len(index_header_arr), dtype=np.bool)]

	offset = INDEX_HEADER_LENGTH

	store_pos = 0

	doc_order = 1
	for doc in self._nlp.pipe(iterator, batch_size = spacy_batch_size): # was previously using self._nlp.tokenizer.pipe(iterator, batch_size=batch_size): but this is faster, test other options at some point
		orth_index.append(doc.to_array(ORTH))
		orth_index.append(eof_arr)

		lower_index_tmp = doc.to_array(LOWER)
		lower_index.append(lower_index_tmp)
		lower_index.append(eof_arr)

		token2doc_index.append(np.array([doc_order] * len(lower_index_tmp), dtype=np.int32))
		token2doc_index.append(not_doc_arr)

		has_spaces.append(doc.to_array(SPACY))
		has_spaces.append(has_spaces_eof_arr)
		# self.offsets.append(offset) 
		# offset = offset + len(lower_index_tmp) + 1
		doc_order += 1

		# update store every build_process_batch_size docs
		if doc_order % build_process_batch_size == 0:
			#was based on condition build_process_path is not None before disk-based build process
			store_pos = self._update_build_process(orth_index, lower_index, token2doc_index, has_spaces, store_pos)
			lower_index, orth_index, token2doc_index, has_spaces = [], [], [], []
			logger.memory_usage(f'processed {doc_order} documents')
			
	del iterator
	orth_index.append(index_header_arr)
	lower_index.append(index_header_arr)
	token2doc_index.append(np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32))
	has_spaces.append(np.array([0] * len(index_header_arr), dtype=np.bool))

	logger.memory_usage(f'Completing build process')
	if save_path is not None:
		store_pos = self._update_build_process(orth_index, lower_index, token2doc_index, has_spaces, store_pos)
		lower_index, orth_index, token2doc_index, has_spaces = [], [], [], []
		self._complete_build_process(build_process_cleanup = build_process_cleanup, standardize_word_token_punctuation_characters = standardize_word_token_punctuation_characters)
	else:
		# deprecated - leaving for now
		self._create_indices(orth_index, lower_index, token2doc_index)
		# self.document_count = len(self.offsets)

		self.token_count = self.lower_index.shape[0] - self.document_count - len(index_header_arr) - len(index_header_arr) 
		self.unique_tokens = len(self.frequency_lookup)

		self.word_token_count = self.token_count - len(self.punct_positions) - len(self.space_positions)
		self.unique_word_tokens = len(self.frequency_lookup) - len(self.punct_tokens) - len(self.space_tokens)

	del orth_index
	del lower_index
	del token2doc_index
	del has_spaces
	
	logger.memory_usage(f'Completed build process')

	# save corpus metadata
	self.save_corpus_metadata()

	self._init_corpus_dataframes()

	logger.info(f'Build time: {(time.time() - start_time):.3f} seconds')


# %% ../nbs/api/45_corpus.ipynb 33
@patch
def _prepare_files(self: Corpus, 
					source_path: str, # path to folder with text files, path can be a directory, zip or tar/tar.gz file
					file_mask:str='*.txt', # mask to select files 
					metadata_file: str|None=None, # path to a CSV with metadata
					metadata_file_column:str = 'file', # column in metadata file with file names to align texts with metadata
					metadata_columns:list[str]=[], # list of column names to import from metadata
					encoding:str='utf8' # encoding of text files
					):
	"""Prepare text files and metadata for building a corpus. Returns an iterator to get file text for processing."""

	# allowing import from zip and tar files
	if os.path.isdir(source_path):
		files = glob.glob(os.path.join(source_path, file_mask))
		type = 'folder'
	elif os.path.isfile(source_path):
		import fnmatch
		if source_path.endswith('.zip'):
			import zipfile
			with zipfile.ZipFile(source_path, 'r') as z:
				files = []
				for f in z.namelist():
					if fnmatch.fnmatch(f, file_mask):
						files.append(f)
				if len(files) > 0:
					type = 'zip'
		elif source_path.endswith('.tar') or source_path.endswith('.tar.gz'):
			import tarfile
			with tarfile.open(source_path, 'r') as t:
				files = []
				for f in t.getnames():
					if fnmatch.fnmatch(f, file_mask):
						files.append(f)
				if len(files) > 0:
					type = 'tar'
		else:
			raise FileNotFoundError(f"Path '{source_path}' is not a directory, zip or tar file")
	
	if not files:
		raise FileNotFoundError(f"No files matching {file_mask} found in '{source_path}'")

	metadata = pl.LazyFrame({metadata_file_column: [os.path.basename(p) for p in files]})

	if metadata_file:
		if not os.path.isfile(metadata_file):
			raise FileNotFoundError(f"Metadata file '{metadata_file}' not found")
		try:
			if metadata_file_column not in metadata_columns:
				metadata_columns.insert(0, metadata_file_column)
			
			metadata = pl.scan_csv(metadata_file).select(metadata_columns)
			# reordering files on metadata so token data and metadata aligned
			files = metadata.select(pl.col(metadata_file_column)).collect(engine='streaming').to_numpy().flatten().tolist() # get file names from metadata
			files = [os.path.join(source_path, f) for f in files if os.path.basename(f) in files] 
		except pl.exceptions.ColumnNotFoundError as e:
			raise
	
	metadata.sink_parquet(f'{self.corpus_path}/metadata.parquet')

	self.source_path = source_path

	if type == 'folder':
		for p in files:
			yield open(p, "rb").read().decode(encoding)
	elif type == 'zip':
		with zipfile.ZipFile(source_path, 'r') as z:
			for f in files:
				yield z.read(f).decode(encoding)
	elif type == 'tar':
		with tarfile.open(source_path, 'r') as t:
			for f in files:
				yield t.extractfile(f).read().decode(encoding)		
	


# %% ../nbs/api/45_corpus.ipynb 34
@patch
def build_from_files(self: Corpus,
					source_path: str, # path to folder with text files, path can be a directory, zip or tar/tar.gz file
					save_path:str, # directory where corpus will be created, a subdirectory will be automatically created with the corpus content
					file_mask:str='*.txt', # mask to select files 
					metadata_file: str|None=None, # path to a CSV with metadata
					metadata_file_column:str = 'file', # column in metadata file with file names to align texts with metadata
					metadata_columns:list[str]=[], # list of column names to import from metadata
					encoding:str='utf-8', # encoding of text files
					model:str='en_core_web_sm', # spacy model to use for tokenisation
					spacy_batch_size:int=1000, # batch size for spacy tokenizer
					build_process_batch_size:int=5000, # save in-progress build to disk every n docs
					build_process_cleanup:bool = True, # Remove the build files after build is complete, retained for development and testing purposes
					standardize_word_token_punctuation_characters: bool = False # whether to standardize apostrophes in word tokens
					):
	"""Build a corpus from text files in a folder."""
	
	start_time = time.time()
	self._init_build_process(save_path)
	iterator = self._prepare_files(source_path, file_mask, metadata_file, metadata_file_column, metadata_columns, encoding) #, build_process_path=build_process_path
	self._build(save_path = save_path, iterator = iterator, model = model, spacy_batch_size = spacy_batch_size, build_process_batch_size = build_process_batch_size, build_process_cleanup = build_process_cleanup, standardize_word_token_punctuation_characters = standardize_word_token_punctuation_characters) #build_process_path = build_process_path, 
	logger.info(f'Build from files time: {(time.time() - start_time):.3f} seconds')

	return self

# %% ../nbs/api/45_corpus.ipynb 35
@patch
def _prepare_csv(self: Corpus, 
					source_path:str, # path to csv file
					text_column:str='text', # column in csv with text
					metadata_columns:list[str]=[], # list of column names to import from csv
					encoding:str='utf8', # encoding of csv passed to Polars read_csv, see their documentation
					build_process_batch_size:int=5000 # save in-progress build to disk every n rows
					) -> iter: # iterator to return rows for processing
	"""Prepare to import from CSV, including metadata. Returns an iterator to process the text column."""

	if not os.path.isfile(source_path):
		raise FileNotFoundError(f'Path ({source_path}) is not a file')
	
	try:
		df = pl.scan_csv(source_path, encoding = encoding).select([text_column] + metadata_columns)
	except pl.exceptions.ColumnNotFoundError as e:
		raise

	self.source_path = source_path
	
	df.select(metadata_columns).sink_parquet(f'{self.corpus_path}/metadata.parquet')

	for slice_df in df.collect(engine='streaming').iter_slices(n_rows=build_process_batch_size):  
		for row in slice_df.iter_rows():
			yield row[0]  

# %% ../nbs/api/45_corpus.ipynb 36
@patch
def build_from_csv(self: Corpus, 
				   source_path:str, # path to csv file
				   save_path:str, # directory where corpus will be created, a subdirectory will be automatically created with the corpus content
				   text_column:str='text', # column in csv with text
				   metadata_columns:list[str]=[], # list of column names to import from csv
				   encoding:str='utf8', # encoding of csv passed to Polars read_csv, see their documentation
				   model:str='en_core_web_sm', # spacy model to use for tokenisation
				   spacy_batch_size:int=1000, # batch size for Spacy tokenizer
				   #build_process_path:str=None, # path to save an in-progress build to disk to reduce memory usage
				   build_process_batch_size:int=5000, # save in-progress build to disk every n docs
				   build_process_cleanup:bool = True, # Remove the build files after build is complete, retained for development and testing purposes
				   standardize_word_token_punctuation_characters: bool = False # whether to standardize apostrophes in word tokens
				   ):
	"""Build a corpus from a csv file."""
	
	start_time = time.time()
	self._init_build_process(save_path)
	iterator = self._prepare_csv(source_path = source_path, text_column = text_column, metadata_columns = metadata_columns, encoding = encoding, build_process_batch_size = build_process_batch_size)
	self._build(save_path = save_path, iterator = iterator, model = model, spacy_batch_size = spacy_batch_size, build_process_batch_size = build_process_batch_size, build_process_cleanup = build_process_cleanup, standardize_word_token_punctuation_characters = standardize_word_token_punctuation_characters)
	logger.info(f'Build from csv time: {(time.time() - start_time):.3f} seconds')

	return self

# %% ../nbs/api/45_corpus.ipynb 40
@patch
def load(self: Corpus, 
		 corpus_path: str # path to load corpus
		 ):
	""" Load corpus from disk and load the corresponding spaCy model. """

	logger.memory_usage('init', init=True)

	start_time = time.time()

	if not os.path.isdir(corpus_path):
		raise FileNotFoundError(f"Path '{corpus_path}' is not a directory")
	
	if not all(os.path.isfile(os.path.join(corpus_path, f)) for f in self.expected_files_):
		raise FileNotFoundError(f"Path '{corpus_path}' does not contain all expected files: {self.expected_files_}")

	self.corpus_path = corpus_path

	with open(f'{self.corpus_path}/corpus.json', 'rb') as f:
		data = msgspec.json.decode(f.read(), type=CorpusMetadata)

	for k in data.__slots__:
		setattr(self, k, getattr(data, k))

	self._init_spacy_model(self.SPACY_MODEL, version = self.SPACY_MODEL_VERSION)

	self._init_corpus_dataframes()

	logger.info(f'Load time: {(time.time() - start_time):.3f} seconds')

	return self

# %% ../nbs/api/45_corpus.ipynb 45
@patch
def info(self: Corpus, 
		 include_disk_usage:bool = False, # include information of size on disk in output
		 formatted:bool = True # return formatted output
		 ) -> str: # formatted information about the corpus
	""" Return information about the corpus. """
	
	result = []
	attributes = ['name', 'description', 'date_created', 'conc_version', 'corpus_path', 'document_count', 'token_count', 'word_token_count', 'unique_tokens', 'unique_word_tokens']
	for attr in attributes:
		value = getattr(self, attr)
		if isinstance(value, bool):
			result.append('True' if value else 'False')
		elif isinstance(value, int):
			result.append(f'{value:,}')
		else:
			result.append(str(value))

	if include_disk_usage:
		files = {'corpus.json': 'Corpus Metadata', 'metadata.parquet': 'Document Metadata', 'tokens.parquet': 'Tokens', 'vocab.parquet': 'Vocab', 'puncts.parquet': 'Punctuation positions', 'spaces.parquet': 'Space positions'}
		for file, file_descriptor in files.items():
			size = os.path.getsize(f'{self.corpus_path}/{file}')
			attributes.append(file_descriptor + ' (MB)')
			result.append(f'{size/1024/1024:.3f}')

	# maybe add in status of these: 'results_cache', 'ngram_index', 'frequency_table'
	# size = sys.getsizeof(getattr(self, attr))
	
	if formatted:
		attributes = [attr.replace('_', ' ').title() for attr in attributes]

	return pl.DataFrame({'Attribute': attributes, 'Value': result})



# %% ../nbs/api/45_corpus.ipynb 46
@patch
def report(self: Corpus, 
			include_memory_usage:bool = False # include memory usage in output
			) -> Result: # returns Result object with corpus summary information
	""" Get information about the corpus as a result object. """
	return Result('summary', self.info(include_memory_usage), 'Corpus Summary', '', {}, [])	

# %% ../nbs/api/45_corpus.ipynb 47
@patch
def summary(self: Corpus, 
			include_memory_usage:bool = False # include memory usage in output
			):
	""" Print information about the corpus in a formatted table. """
	result = Result('summary', self.info(include_memory_usage), 'Corpus Summary', '', {}, [])
	result.display()

# %% ../nbs/api/45_corpus.ipynb 48
@patch
def __str__(self: Corpus):
	""" Formatted information about the corpus. """
	
	return str(self.info())



# %% ../nbs/api/45_corpus.ipynb 58
@patch
def _init_token_arrays(self: Corpus):
	""" Prepare the temporary token arrays for the corpus. """
	if 'tokens_array' not in self.results_cache:
		start_time = time.time()
		self.results_cache['tokens_array'] = self.vocab.sort(by = pl.col('token_id')).select(pl.col('token')).collect(engine='streaming').to_numpy().flatten()
		self.results_cache['tokens_array'] = np.insert(self.results_cache['tokens_array'], 0, ERR_TOKEN_STR) # adding a dummy value at the 0 index to align token strings with token_ids
		logger.info(f'Created tokens_array in {(time.time() - start_time):.3f} seconds')

		start_time = time.time() 
		# new functionality for disk-based build 
		self.results_cache['tokens_lookup'] = dict(zip(self.results_cache['tokens_array'], range(len(self.results_cache['tokens_array']))))
		logger.info(f'Created tokens_lookup in {(time.time() - start_time):.3f} seconds')
		
		start_time = time.time()
		self.results_cache['tokens_sort_order'] = self.vocab.sort(by = pl.col('token_id')).select(pl.col('tokens_sort_order')).collect(engine='streaming').to_numpy().flatten()
		self.results_cache['tokens_sort_order'] = np.insert(self.results_cache['tokens_sort_order'], 0, 0) # adding a dummy value at the 0 index to align token strings with token_ids
		logger.info(f'Created tokens_sort_order in {(time.time() - start_time):.3f} seconds')

		# start_time = time.time()  # move tokens sort order to build process - takes > 1 second for large corpora, but not needed for all results
		# # building tokens_sort_order was implemented in _init_tokens_sort_order - deprecating to simplify as makes sense to build all these in one go
		# tokens_array_lower = np.char.lower(self.results_cache['tokens_array'].astype(str))
		# self.results_cache['tokens_sort_order'] = np.argsort(np.argsort(tokens_array_lower)) # lowercasing then sorting	
		# logger.info(f'Created tokens_sort_order in {(time.time() - start_time):.3f} seconds')
		# del tokens_array_lower	

# %% ../nbs/api/45_corpus.ipynb 60
@patch
def token_ids_to_tokens(self: Corpus, 
						token_ids: np.ndarray|list # token ids to return token strings for 
						) -> np.ndarray: # return token strings for token ids
	""" Get token strings for a list of token ids. """ 

	self._init_token_arrays()
	
	if isinstance(token_ids, list):
		token_ids = np.array(token_ids)
	if np.any(token_ids < 0):
		raise ValueError("Token ids must be non-negative integers.")
	
	return self.results_cache['tokens_array'][token_ids]

# %% ../nbs/api/45_corpus.ipynb 61
@patch
def tokens_to_token_ids(self: Corpus, 
				tokens: list[str]|np.ndarray[str] # list of tokens to get ids for
				) -> np.ndarray[int]: # array of token ids, 0 for unknown tokens
	""" Convert a list or np.array of token string to token ids """
	
	self._init_token_arrays()
	
	if isinstance(tokens, list):
		tokens = np.array(tokens, dtype=str)
	
	return np.array([self.results_cache['tokens_lookup'].get(token, 0) for token in tokens])

# %% ../nbs/api/45_corpus.ipynb 62
@patch
def token_to_id(self: Corpus, 
				token: str # token to get id for
				) -> int: # return token id (0 if token not found in the corpus)
	""" Get the token id of a token string. """

	token_ids = self.tokens_to_token_ids([token])
	return int(token_ids[0])

# %% ../nbs/api/45_corpus.ipynb 78
@patch
def token_ids_to_sort_order(self: Corpus, 
							token_ids: np.ndarray|list # token ids to return token strings for 
							) -> np.ndarray: # rank of token ids
	""" Get the sort order of token strings corresponding to token ids """

	self._init_token_arrays()	

	if isinstance(token_ids, list):
		token_ids = np.array(token_ids)
	if np.any(token_ids < 0):
		raise ValueError("Token ids must be non-negative integers.")
	
	return self.results_cache['tokens_sort_order'][token_ids]

# %% ../nbs/api/45_corpus.ipynb 81
@patch
def get_token_count_text(self: Corpus, 
					exclude_punctuation:bool = False # exclude punctuation tokens from the count
					) -> tuple[int, str, str]: # token count with adjustments based on exclusions, token descriptor, total descriptor
	""" Get the token count for the corpus with adjustments and text for output """

	count_tokens = self.token_count
	tokens_descriptor = 'word and punctuation tokens'
	total_descriptor = 'Total word and punctuation tokens'
	if exclude_punctuation:
		count_tokens = self.word_token_count
		tokens_descriptor = 'word tokens'
		total_descriptor = 'Total word tokens'

	return count_tokens, tokens_descriptor, total_descriptor

# %% ../nbs/api/45_corpus.ipynb 84
@patch
def tokenize(self: Corpus, 
			 string:str, # string to tokenize 
			#  return_tokens = False, # return token strings
			 simple_indexing = False # use simple indexing
             ): # return tokenized string
	""" Tokenize a string using the Spacy tokenizer. """
	# NOTE: when extending this function - ensure get_token_positions is compatible (e.g. currently assumes fixed sequence length of sequences)

	start_time = time.time()
	placeholder_string = 'zzxxzzplaceholderzzxxzz' # so doesn't split tokens
	is_wildcard_search = False
	if simple_indexing == True:
		index_id = LOWER
		strings_to_tokenize = [string.strip()]
	else:
		raise('only simple_indexing implemented')
		# retained for future rework
		# if '*' in string:
		# 	is_wildcard_search = True
		# 	string = string.replace('*',placeholder_string)
		# if string.islower() == True:
		# 	index_id = LOWER
		# else:
		# 	index_id = ORTH
		# if '|' in string:
		# 	strings_to_tokenize = string.split('|')
		# else:
		# 	strings_to_tokenize = [string.strip()]
	token_sequences = []
	for doc in self._nlp.pipe(strings_to_tokenize): # was tokenizer.pipe(strings_to_tokenize) - retaining for reference
		# token_sequences.append(tuple(doc.to_array(index_id))) # not using spacy indexes once corpus created
		logger.debug(f'Tokens {list(doc)}')
		token_sequences.append(list(doc))
	# if is_wildcard_search == True:
	# 	tmp_token_sequence = []
	# 	sequence_count = 1
	# 	for token in doc:
	# 		tmp_token_sequence.append([])
	# 		if placeholder_string in token.text:
	# 			chunked_string = token.text.split(placeholder_string)
	# 			if len(chunked_string) > 2 or (len(chunked_string) == 2 and chunked_string[0] != '' and chunked_string[1] != ''):
	# 				# use regex
	# 				approach = 'regex'
	# 				regex = re.compile('.*'.join(chunked_string))
	# 			elif chunked_string[0] == '':
	# 				approach = 'endswith'
	# 			else:
	# 				approach = 'startswith'
	# 			for token_id in loaded_corpora[corpus_name]['frequency_lookup']:
	# 				possible_word = False
	# 				word = loaded_corpora[corpus_name]['vocab'][token_id]
	# 				if approach == 'regex':
	# 					if regex.match(word):
	# 						possible_word = word
	# 				elif getattr(word,approach)(''.join(chunked_string)):
	# 					possible_word = word
	# 				if possible_word != False:
	# 					tmp_token_sequence[token.i].append(loaded_corpora[corpus_name]['vocab'][possible_word])
	# 		else:
	# 			tmp_token_sequence[token.i].append(token.orth)
	# 		sequence_count *= len(tmp_token_sequence[token.i])
	# 	rotated_token_sequence = []
	# 	token_repeat = sequence_count
	# 	for pos in range(len(tmp_token_sequence)):
	# 		rotated_token_sequence.append([])
	# 		if len(tmp_token_sequence[pos]) == 1:
	# 			rotated_token_sequence[pos] += sequence_count * [tmp_token_sequence[pos][0]]
	# 		else:
	# 			token_repeat = token_repeat // len(tmp_token_sequence[pos])
	# 			while len(rotated_token_sequence[pos]) < sequence_count:
	# 				for token in tmp_token_sequence[pos]:
	# 					rotated_token_sequence[pos] += token_repeat * [token]
	# 	token_sequences = list(zip(*rotated_token_sequence))
	# 	#for tokens in tmp_token_sequence:
	# 	#    for token in tokens:
	# covert token_sequences to reindexed tokens using original_to_new
	
	# convert sequences to lower case
	if index_id == LOWER:
		token_sequences = [[token.lower_ for token in sequence] for sequence in token_sequences]
	token_sequences = [tuple(self.tokens_to_token_ids(sequence)) for sequence in token_sequences]
	
	logger.info(f'Tokenization time: {(time.time() - start_time):.5f} seconds')
	# if return_tokens == True:
		# return token_sequences, index_id, doc
	# else:
	return token_sequences, index_id

# %% ../nbs/api/45_corpus.ipynb 87
@patch
def _get_text(self:Corpus,
        doc_id: int, # the id of the document
        return_df: bool = True # returns the df with 
        ):
    """ Get tokens, space definitions and metadata for a text in the corpus """
    
    if doc_id < 1 or doc_id > self.document_count:
        raise ValueError(f"Document ID {doc_id} is out of range. Document ID should be between 1 and the count of documents ({self.document_count}).")

    doc_tokens = self.tokens.with_row_index('position').filter(pl.col('token2doc_index') == doc_id).with_columns(pl.lit(1).alias('not_space'))
    doc_space_tokens = self.spaces.filter(pl.col('token2doc_index') == doc_id).with_columns(pl.lit(0).alias('not_space'))
    doc_tokens_df = pl.concat([doc_tokens, doc_space_tokens]).sort('position', 'not_space') #.drop('position').drop('not_space') # here - creating as df for return_df

    doc_tokens = doc_tokens_df.select(['orth_index', 'has_spaces']).collect()
    tokens = self.token_ids_to_tokens(doc_tokens.select(pl.col('orth_index')).to_numpy().flatten())
    has_spaces = doc_tokens.select(pl.col('has_spaces')).to_numpy().flatten()
    metadata = self.metadata.with_row_index(offset = 1, name = 'document_id').filter(pl.col('document_id') == doc_id).collect()
    if return_df == True:
        return tokens, has_spaces, metadata, doc_tokens_df
    else:
        return tokens, has_spaces, metadata

# %% ../nbs/api/45_corpus.ipynb 88
@patch
def text(self:Corpus,
        doc_id: int # the id of the document
        ):
    """ Get a text document """

    return Text(*self._get_text(doc_id))

# %% ../nbs/api/45_corpus.ipynb 91
@patch
def get_tokens_by_index(self: Corpus, 
			   index: str = 'orth_index', # index to get tokens from i.e. 'orth_index' 'lower_index' 'token2doc_index'
			   exclude_punctuation: bool = False, # exclude punctuation tokens from the result (unused currently)
				) -> np.ndarray:
	""" Get tokens for a given index. """

	logger.debug(f'Getting tokens for index: {index}')
	start_time = time.time()
	if index not in ['orth_index', 'lower_index', 'token2doc_index']:
		raise ValueError("Index must be one of 'orth_index', 'lower_index', 'token2doc_index'")

	cache_key = index
	if exclude_punctuation:
		cache_key += '-nopuncts'
	if cache_key in self.results_cache:
		logger.info(f'Tokens for index {index} with exclude_punctuation {exclude_punctuation} already cached, returning cached result in {(time.time() - start_time):.3f} seconds')
		return self.results_cache[cache_key]
	else:
		if index not in self.results_cache: # in case build -nopuncts first - get both sorted
			self.results_cache[index] = self.tokens.select(pl.col(index)).collect(engine='streaming').to_numpy().flatten()
		if exclude_punctuation == False:
			logger.info(f'Got tokens for index {index} with exclude_punctuation {exclude_punctuation} in {(time.time() - start_time):.3f} seconds')
			return self.results_cache[index]
		else:
			if 'puncts' not in self.results_cache:
				self.results_cache['puncts'] = self.puncts.select(pl.col('position')).collect(engine='streaming').to_numpy().flatten()
			self.results_cache[cache_key] = np.delete(self.results_cache[index], self.results_cache['puncts'])
			self.results_cache[f'{cache_key}-positions'] = np.delete(np.arange(len(self.results_cache[index])), self.results_cache['puncts'])
			#self.results_cache[f'{index}-nopuncts'] = self.tokens.with_row_index('position').select(pl.col('position'), pl.col(index)).join(self.puncts.select('position'), on='position', how='anti').drop('position').collect(engine='streaming').to_numpy().flatten()
			logger.info(f'Got tokens for index {index} with exclude_punctuation {exclude_punctuation} in {(time.time() - start_time):.3f} seconds')
			return self.results_cache[cache_key]


# %% ../nbs/api/45_corpus.ipynb 95
@patch
def get_ngrams_by_index(self: Corpus, 
				ngram_length:int, # length of ngrams to get
				index:str,  # index to get tokens from, e.g. 'orth_index' 'lower_index'
				exclude_punctuation: bool = False # exclude punctuation tokens from the result (unused currently)
				) -> np.ndarray:
	""" Get ngrams for a given index and ngram length. """

	if index not in ['orth_index', 'lower_index']:
		raise ValueError("Index must be either 'orth_index' or 'lower_index'")

	if (index, ngram_length, exclude_punctuation) not in self.ngram_index:
		slices = []
		[slices.append(np.roll(self.get_tokens_by_index(index, exclude_punctuation), shift)) for shift in -np.arange(ngram_length)]
		seq = np.vstack(slices).T
		self.ngram_index[(index, ngram_length, exclude_punctuation)] = seq

	return self.ngram_index[(index, ngram_length, exclude_punctuation)]

# %% ../nbs/api/45_corpus.ipynb 98
@patch
def get_token_positions(self: Corpus, 
					token_sequence: list[np.ndarray], # token sequence to get index for 
					index_id: int, # index to search (i.e. ORTH, LOWER)
					exclude_punctuation: bool = False # exclude punctuation tokens from the result (unused currently)
					) -> np.ndarray: # positions of token sequence
	""" Get the positions of a token sequence in the corpus. """
	
	start_time = time.time()

	results = []

	sequence_len = len(token_sequence[0]) # Check when extend tokenization
	variants_len = len(token_sequence)

	if index_id == ORTH:
		index = 'orth_index'
	else:
		index = 'lower_index'

	if variants_len == 1:
		results.append(np.where(np.all(self.get_ngrams_by_index(ngram_length = sequence_len, index = index, exclude_punctuation = exclude_punctuation) == token_sequence[0], axis=1))[0])
	else:
		condition_list = []
		choice_list = variants_len * [True]
		for seq in token_sequence:
			condition_list.append(self.get_ngrams_by_index(ngram_length = sequence_len, index = index, exclude_punctuation = exclude_punctuation) == seq)
		results.append(np.where(np.all(np.select(condition_list, choice_list),axis=1))[0])

	logger.info(f'Token indexing ({len(results[0])}) time: {(time.time() - start_time):.5f} seconds')
	return results

# %% ../nbs/api/45_corpus.ipynb 101
@patch
def _shift_zeroes_to_end(self:Corpus,
						arr:np.ndarray # Numpy array of collocate frequencies to process
						):
	""" Move 0 value positions for punctuation and space removal, zeroes get moved to the end of each column. """
	result = np.empty_like(arr)
	for col in range(arr.shape[1]):
		col_data = arr[:, col]
		mask = col_data != 0
		result[:mask.sum(), col] = col_data[mask]
		result[mask.sum():, col] = 0
	return result

# %% ../nbs/api/45_corpus.ipynb 102
@patch
def _shift_zeroes_to_start(self:Corpus,
						arr:np.ndarray # Numpy array of collocate frequencies to process
						):
	""" Move 0 value positions for punctuation and space removal to the start of each column """
	result = np.empty_like(arr)
	for col in range(arr.shape[1]):
		col_data = arr[:, col]
		mask = col_data != 0
		n_zeros = (~mask).sum()
		result[:n_zeros, col] = 0
		result[n_zeros:, col] = col_data[mask]
	return result

# %% ../nbs/api/45_corpus.ipynb 103
@patch
def _zero_after_value(self:Corpus,
					  arr:np.ndarray, # Numpy array of collocate frequencies to process
					  target: int # Target value to find in the array (e.g., an end-of-file token or a specific collocate frequency)
					  ):
	""" Set values from first occurence of target value to 0 in each column (for processing tokens outside text using eof token) """
	arr = arr.copy()  
	for col in range(arr.shape[1]):
		col_data = arr[:, col]
		idx = np.where(col_data == target)[0]
		if idx.size > 0:
			first_idx = idx[0]
			arr[first_idx:, col] = 0
	return arr

# %% ../nbs/api/45_corpus.ipynb 104
@patch
def get_tokens_in_context(self:Corpus,
							   token_positions:np.ndarray, # Numpy array of token positions in the corpus
							   index:str, # Index to use - lower_index, orth_index
							   context_length:int = 5, # Number of context words to consider on each side of the token
							   position_offset:int = 1, # offset to start retrieving context words - negatve is left of node, positive for right - may want to adjust if sequence_len > 1
							   position_offset_step:int = 1, # step to move position offset by, this sets direct, -1 for left, 1 for right
							   exclude_punctuation:bool = True, # ignore punctuation from context retrieved
							   convert_eof:bool = True # if True (for collocation functionality), contexts with end of file tokens will have eof token and tokens after set to zero, otherwise EOF retained (e.g. False used for ngrams)
							   ) -> Result:
	""" Get tokens in context for given token positions, context length and direction, operates one side at a time. """

	start_time = time.time()

	if context_length < 1:
		# return empty result
		return np.zeros((0, 0), dtype=np.int32)

	tokens_for_removal = []
	if exclude_punctuation:
		tokens_for_removal += self.punct_tokens
	len_tokens_for_removal = len(tokens_for_removal)

	collected = False
	context_tokens_arr = []
	while collected == False:
		new_positions = np.array(token_positions[0] + position_offset, dtype = token_positions[0].dtype)
		context_tokens_arr.append(self.get_tokens_by_index(index)[new_positions])
		position_offset += position_offset_step
		if len(context_tokens_arr) >= context_length: 
			context_tokens = np.array(context_tokens_arr, dtype = token_positions[0].dtype)
			# shape = (context_length, len(token_positions[0]))
			logger.debug(f"Context tokens collected: {context_tokens.shape}")
			if len_tokens_for_removal > 0: # cleaning punctuation and check if need more iterations
				context_tokens = np.where(np.isin(context_tokens, tokens_for_removal), 0, context_tokens)
			counts = np.count_nonzero(context_tokens, axis=0)
			if np.min(counts) < context_length:
				pass
			else:
				collected = True

	context_tokens = self._shift_zeroes_to_end(context_tokens)
	context_tokens = context_tokens[:context_length, :]

	if convert_eof: # delete any context that contains self.EOF_TOKEN
		if self.EOF_TOKEN in context_tokens:
			context_tokens = self._zero_after_value(context_tokens, self.EOF_TOKEN)

	logger.info(f"Context retrieved in {time.time() - start_time:.2f} seconds.")

	return context_tokens

# %% ../nbs/api/45_corpus.ipynb 105
def build_test_corpora(
		source_path:str, # path to folder with corpora
		save_path:str, # path to save corpora
		force_rebuild:bool = False # force rebuild of corpora, useful for development and testing
		):
	"""(Deprecated - moved to conc.corpora) Build all test corpora from source files."""

	raise DeprecationWarning("Calling build_test_corpora via conc.corpus is deprecated and will be removed by v1.0.0, instead import as 'from conc.corpora import build_test_corpora' and use 'build_sample_corpora'.")
