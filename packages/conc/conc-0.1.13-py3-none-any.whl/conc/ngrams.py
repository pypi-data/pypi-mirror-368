"""Functionality for ngram analysis."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/api/71_ngrams.ipynb.

# %% ../nbs/api/71_ngrams.ipynb 3
from __future__ import annotations
import numpy as np
import time
import polars as pl
from spacy.attrs import ORTH, LOWER # remove? - add ENT_TYPE, ENT_IOB
import math
from fastcore.basics import patch

# %% auto 0
__all__ = ['Ngrams']

# %% ../nbs/api/71_ngrams.ipynb 4
from .corpus import Corpus
from .result import Result
from .core import logger, PAGE_SIZE

# %% ../nbs/api/71_ngrams.ipynb 10
class Ngrams:
	""" Class for n-gram analysis reporting. """
	def __init__(self,
			  corpus:Corpus # Corpus instance
			  ): 

		if type(corpus) != Corpus:
			raise ValueError('Conc ngrams functionality is only available for instances of the Corpus class.')

		self.corpus = corpus


# %% ../nbs/api/71_ngrams.ipynb 13
@patch
def _get_ngrams(self:Ngrams, 
			   token_sequence: list[np.ndarray], # token sequence to get index for 
			   index_id: int, # index to search (i.e. ORTH, LOWER)
			   token_positions: list[np.ndarray], # positions of token sequence, returned by get_token_positions 
			   ngram_length: int = 2, # length of ngram
			   ngram_token_position: str = 'LEFT', # specify if token sequence is on LEFT or RIGHT (support for ngrams with token in middle of sequence is in-development))
			   exclude_punctuation:bool=False # exclude ngrams with tokens
			   ) -> tuple[np.ndarray, list[np.ndarray]]: # array of ngrams results, revised token positions

	""" Get ngram data for a token sequence. """
	
	start_time = time.time()
	sequence_len = len(token_sequence[0])
	variants_len = len(token_sequence)
	token_index_len = len(token_positions[0])

	if index_id == ORTH:
		index = 'orth_index'
	else:
		index = 'lower_index'

	if ngram_token_position == 'RIGHT':
		ngrams = self.corpus.get_tokens_in_context(token_positions = token_positions, index = index, context_length = ngram_length, position_offset = (sequence_len-1), position_offset_step = -1, exclude_punctuation = False, convert_eof = False)
		ngrams = ngrams[::-1, :] # reversing order as retrieved right to left
	elif ngram_token_position == 'LEFT':
		ngrams = self.corpus.get_tokens_in_context(token_positions = token_positions, index = index, context_length = ngram_length, position_offset = 0, position_offset_step = 1, exclude_punctuation = False, convert_eof = False)
	elif ngram_token_position == 'MIDDLE':
		length_per_side = math.ceil((ngram_length - sequence_len) / 2)
		if length_per_side * 2 + sequence_len != ngram_length:
			logger.warning(f'Ngram length {ngram_length} does not make sense with the provided tokens and ngram_token_position MIDDLE. Adjusting ngram_length.')

		logger.debug(f'Tokens per side for middle token position: {length_per_side}, with ngram length {ngram_length} and sequence length {sequence_len}')
		right_tokens= self.corpus.get_tokens_in_context(token_positions = token_positions, index = index, context_length = sequence_len + length_per_side, position_offset = 0, position_offset_step = 1, exclude_punctuation = False, convert_eof = False)
		left_tokens = self.corpus.get_tokens_in_context(token_positions = token_positions, index = index, context_length = length_per_side, position_offset = -1, position_offset_step = -1, exclude_punctuation = False, convert_eof = False)
		left_tokens = left_tokens[::-1, :] # reversing order as retrieved right to left
		# combining so that has shape (ngram_length, len(token_positions[0]))
		ngrams = np.concatenate((left_tokens, right_tokens), axis=0)

	logger.debug(f'Ngrams shape after retrieval {ngrams.shape}')

	# elif ngram_token_position == 'MIDDLE': # further development needed - in roadmap
	# 	ngram_range = range(-1 * ngram_length + sequence_len + 1, sequence_len + 1)

	# old retrieval method - aligning with collocates context retrieval
	# ngrams = []
	
	# for pos in ngram_range:
	# 	if variants_len == 1 and pos > -1 and pos < sequence_len:
	# 		ngrams.append(np.full(token_index_len, token_sequence[0][pos]))
	# 	else:
	# 		seq = token_positions[0] + pos
	# 		ngrams.append(self.corpus.get_tokens_by_index(index)[seq])

	# ngrams = np.stack(ngrams)
	# logger.info(f'Ngrams ({ngrams.shape}) retrieval time: {(time.time() - start_time):.5f} seconds')

	# getting positions to search for EOF_TOKEN and filter out ngrams crossing doc boundaries
	#positions = (np.array(ngram_range)[:, None] != np.arange(sequence_len)).all(axis=1)
	# ngrams = np.delete(ngrams, np.where(ngrams[positions] == self.corpus.EOF_TOKEN)[1], axis=1)
	filter = [self.corpus.EOF_TOKEN]

	if exclude_punctuation: # see above - ngrams returned with punctuation tokens - and then cleaned if exclude_punctuation is True
		filter.extend(self.corpus.punct_tokens)
	
	mask = np.isin(ngrams, filter)
	ngrams = np.delete(ngrams, np.where(mask)[1], axis=1)
	logger.debug(f'Ngrams shape after filter {ngrams.shape}')

	token_positions[0] = np.delete(token_positions[0], np.where(mask)[1], axis=0)
	logger.debug(f'Token positions after filter {token_positions[0].shape}')

	logger.info(f'Ngrams ({ngrams.shape[1]}) retrieval time: {(time.time() - start_time):.5f} seconds')
	return ngrams, token_positions


# %% ../nbs/api/71_ngrams.ipynb 21
@patch
def ngrams(self: Ngrams, 
		   token_str: str, # token string to get ngrams for 
		   ngram_length:int|None = 2, # length of ngram, if set to None it will use the number of tokens in the token_str + 1
		   ngram_token_position: str = 'LEFT', # specify if token sequence is on LEFT or RIGHT or MIDDLE (support for other positions is in-development)
		   normalize_by:int=10000, # normalize frequencies by a number (e.g. 10000)
		   page_size:int = PAGE_SIZE, # number of results to display per results page 
		   page_current:int = 1, # current page of results
		   show_all_columns:bool = False, # return raw df with all columns or just ngram and frequency
		   exclude_punctuation:bool=True, # do not return ngrams with punctuation tokens
		   use_cache:bool = True # retrieve the results from cache if available (currently ignored)
		   ) -> Result: # return a Result object with ngram data
	""" Report ngram frequencies containing a token string. """

	if type(normalize_by) != int:
		raise ValueError('normalize_by must be an integer, e.g. 1000000 or 10000')

	token_sequence, index_id = self.corpus.tokenize(token_str, simple_indexing=True)

	if ngram_length is None:
		if ngram_token_position == 'MIDDLE':
			ngram_length = len(token_sequence[0]) + 2
		else:
			ngram_length = len(token_sequence[0]) + 1

	start_time = time.time()
	use_cache = False
	cache_id = tuple(['ngram'] + list(token_sequence) + [ngram_length, ngram_token_position]) # before reenabling will need to make sure the cache_id matches options above (e.g. could get differences based on exclude punctuation etc but cache first currently)

	if use_cache == True and cache_id in self.corpus.results_cache:
		logger.info('Using cached ngrams results')
		ngrams_report = self.corpus.results_cache[cache_id][0]
		total_unique = self.corpus.results_cache[cache_id][1]
		total_count = self.corpus.results_cache[cache_id][2]
	else:
		token_positions = self.corpus.get_token_positions(token_sequence, index_id)
		
		if len(token_positions[0]) == 0:
			logger.info('No tokens found')
			return Result(type = 'ngrams', df=pl.DataFrame(), title=f'Ngrams for "{token_str}"', description=f'No matches', summary_data={}, formatted_data=[])

		logger.info('Retrieving ngrams results')
		ngrams, token_positions = self._get_ngrams(token_sequence, index_id, token_positions, ngram_length = ngram_length, ngram_token_position = ngram_token_position, exclude_punctuation=exclude_punctuation)
		total_count = ngrams.shape[1]
		if ngram_token_position == 'MIDDLE':
			expected_ngram_length = len(token_sequence[0]) +  (2 * (math.ceil((ngram_length - len(token_sequence[0])) / 2)))
			if expected_ngram_length != ngram_length:
				logger.warning(f'Adjusting ngram length from {ngram_length} to {expected_ngram_length}')
				ngram_length = expected_ngram_length
		schema = [f'token_{i+1}' for i in range(ngram_length)]
		ngrams_report = pl.DataFrame(ngrams.T, schema=schema).to_struct(name = 'ngram_token_ids').value_counts(sort=True).rename({"count": "frequency"}) # TODO - potentially improve by adding second sort so frequency lists are stable

		# if show_document_frequency:
			# to be added in with # show_document_frequency:bool = False, # return document frequency in output
		# 	ngrams_report = ngrams_report.with_columns(pl.Series(name='doc_id', values=self.corpus.get_tokens_by_index('token2doc_index')[token_positions[0]]))

		ngrams_report = ngrams_report.with_row_index(name='rank', offset=1)
		total_unique = len(ngrams_report)

		if use_cache == True:
			self.corpus.results_cache[cache_id] = (ngrams_report, total_unique, total_count)
	
	count_tokens, tokens_descriptor, total_descriptor = self.corpus.get_token_count_text(exclude_punctuation)

	resultset_start = page_size*(page_current-1)

	# get specific chunk of report into polars based on resultset_start:
	ngrams_report_page = ngrams_report.slice(resultset_start, page_size).unnest('ngram_token_ids')
	ngrams_report_page = ngrams_report_page.with_columns(((pl.col("frequency") / pl.lit(count_tokens)) * normalize_by).alias('normalized_frequency'))

	token_strs = []
	for i in range(ngram_length):
		token_strs.append(self.corpus.token_ids_to_tokens(ngrams_report_page[f'token_{i+1}'].to_numpy()))
	token_strs = np.array(token_strs)
	ngram_text = [' '.join(column) for column in token_strs.T]
	ngrams_report_page = ngrams_report_page.with_columns(pl.Series(name="ngram", values=ngram_text))

	total_pages = math.ceil(total_unique/page_size)
	summary_data = {'ngram_length': ngram_length, 'ngram_token_position': ngram_token_position, 'total_unique': total_unique, 'total_count': total_count, 'page_current': page_current, 'total_pages': total_pages}
	formatted_data = [f'Report based on {tokens_descriptor}', f'Ngram length: {ngram_length}, Token position: {ngram_token_position.lower()}']

	if exclude_punctuation:
		formatted_data.append(f'Ngrams containing punctuation tokens excluded')

	if normalize_by is not None:
		formatted_data.append(f'Normalized Frequency is per {normalize_by:,.0f} tokens')

	formatted_data.extend([f'Total unique ngrams: {total_unique:,}', f'Total ngrams: {total_count:,}'])

	if page_size != 0 and total_count > page_size:
		formatted_data.extend([f'Showing {min(page_size, total_count)} rows', f'Page {page_current} of {total_pages}']) 

	if show_all_columns == False:
		ngrams_report_page = ngrams_report_page[['rank', 'ngram', 'frequency', 'normalized_frequency']]
	
	logger.info(f'Ngrams report time: {(time.time() - start_time):.5f} seconds')

	return Result(type = 'ngrams', df=ngrams_report_page, title=f'Ngrams for "{token_str}"', description=f'{self.corpus.name}', summary_data=summary_data, formatted_data=formatted_data)


# %% ../nbs/api/71_ngrams.ipynb 30
@patch
def ngram_frequencies(self: Ngrams, 
                ngram_length:int=2, # length of ngram
                case_sensitive:bool=False, # frequencies for tokens lowercased or with case preserved
				normalize_by:int=10000, # normalize frequencies by a number (e.g. 10000)
				page_size:int=PAGE_SIZE, # number of rows to return
				page_current:int=1, # current page
				show_document_frequency:bool=False, #show document frequency in output
				exclude_punctuation:bool=True # exclude ngrams containing punctuation tokens
				) -> Result: # return a Result object with the frequency table
	""" Report frequent ngrams. """
	
	if type(normalize_by) != int:
		raise ValueError('normalize_by must be an integer, e.g. 1000000 or 10000')

	start_time = time.time()

	if case_sensitive:
		index = 'orth_index'
	else:
		index = 'lower_index'

	filter = [self.corpus.EOF_TOKEN]
	if exclude_punctuation == True:
		filter += self.corpus.punct_tokens

	resultset_start = page_size*(page_current-1)

	count_tokens, tokens_descriptor, total_descriptor = self.corpus.get_token_count_text(exclude_punctuation)
	formatted_data = [f'Report based on {tokens_descriptor}']
	formatted_data.append(f'Ngram length: {ngram_length}')

	if exclude_punctuation:
		formatted_data.append(f'Ngrams containing punctuation tokens excluded')

	ngrams = self.corpus.tokens.select(pl.col('token2doc_index'), pl.col(index).alias('token_1')).with_row_index('position')

	ngrams = ngrams.with_columns([pl.col('token_1').shift(-i).alias(f'token_{i+1}') for i in range(1, ngram_length)])

	schema = [f'token_{i+1}' for i in range(ngram_length)]

	ngrams_report = ngrams.group_by(schema).agg(pl.len().alias("frequency")).sort(by="frequency", descending=True) # TODO - potentially improve by adding second sort so frequency lists are stable

	if show_document_frequency:
		ngrams_report = ngrams_report.join(
			ngrams.group_by(schema).agg(pl.col('token2doc_index').n_unique().alias('document_frequency')), on = schema, how='left', maintain_order='left'
		)

	for i in range(ngram_length):
		ngrams_report = ngrams_report.filter(~pl.col(f'token_{i+1}').is_in(filter))

	total_unique = ngrams_report.select(pl.len()).collect().item()
	total_count = ngrams_report.select(pl.col('frequency').sum()).collect().item()

	ngrams_report_page = ngrams_report.slice(resultset_start, page_size).collect(engine = 'streaming')
	logger.info(f'collected report page: {(time.time() - start_time):.5f} seconds')
	token_strs = []
	for i in range(ngram_length):
		token_strs.append(self.corpus.token_ids_to_tokens(ngrams_report_page.select(pl.col(f'token_{i+1}')).to_numpy().flatten()))
	token_strs = np.array(token_strs)
	ngram_text = [' '.join(column) for column in token_strs.T]
	ngrams_report_page = ngrams_report_page.with_columns(pl.Series(name="ngram", values=ngram_text)).with_row_index(name='rank', offset=(page_current-1)*page_size+1)
	ngrams_report_page = ngrams_report_page.with_columns(((pl.col("frequency") / pl.lit(count_tokens)) * normalize_by).alias('normalized_frequency'))
	formatted_data.append(f'Normalized Frequency is per {normalize_by:,.0f} tokens')

	formatted_data.extend([f'Total unique ngrams: {total_unique:,}', f'Total ngrams: {total_count:,}'])

	total_pages = math.ceil(total_unique/page_size)
	if page_size != 0 and total_count > page_size:
		formatted_data.extend([f'Showing {min(page_size, total_count)} rows', f'Page {page_current} of {total_pages}']) 

	columns = ['rank', 'ngram', 'frequency', 'normalized_frequency']
	if show_document_frequency:
		columns.append('document_frequency')
		
	ngrams_report_page = ngrams_report_page[columns]

	return Result(type = 'ngram_frequencies', df=ngrams_report_page, title=f'Ngram Frequencies', description=f'{self.corpus.name}', summary_data = {}, formatted_data = formatted_data)
