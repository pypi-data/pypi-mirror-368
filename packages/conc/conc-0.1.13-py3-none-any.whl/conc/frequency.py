"""Functionality for frequency analysis."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/api/70_frequency.ipynb.

# %% ../nbs/api/70_frequency.ipynb 3
from __future__ import annotations
import time
import polars as pl
from fastcore.basics import patch

# %% auto 0
__all__ = ['Frequency']

# %% ../nbs/api/70_frequency.ipynb 4
from .corpus import Corpus
from .listcorpus import ListCorpus
from .result import Result
from .core import logger, PAGE_SIZE

# %% ../nbs/api/70_frequency.ipynb 10
class Frequency:
	""" Class for frequency analysis reporting """
	def __init__(self,
			  corpus:Corpus|ListCorpus # Corpus instance
			  ): 
		self.corpus = corpus


# %% ../nbs/api/70_frequency.ipynb 12
@patch
def frequencies(self: Frequency,
				case_sensitive:bool=False, # frequencies for tokens with or without case preserved 
				normalize_by:int=10000, # normalize frequencies by a number (e.g. 10000)
				page_size:int=PAGE_SIZE, # number of rows to return, if 0 returns all
				page_current:int=1, # current page, ignored if page_size is 0
				show_token_id:bool=False, # show token_id in output
				show_document_frequency:bool=False, # show document frequency in output
				exclude_tokens:list[str]=[], # exclude specific tokens from frequency report, can be used to remove stopwords
				exclude_tokens_text:str = '', # text to explain which tokens have been excluded, will be added to the report notes
				restrict_tokens:list[str]=[], # restrict frequency report to return frequencies for a list of specific tokens
				restrict_tokens_text:str = '', # text to explain which tokens are included, will be added to the report notes
				exclude_punctuation:bool=True # exclude punctuation tokens
				) -> Result: # return a Result object with the frequency table
	""" Report frequent tokens. """

	if type(normalize_by) != int:
		raise ValueError('normalize_by must be an integer, e.g. 1000000 or 10000')

	start_time = time.time()

	if case_sensitive:
		frequency_column = 'frequency_orth'
		document_count_column = 'orth_index'
		listcorpus_document_count_column = 'document_frequency_orth'
	else:
		frequency_column = 'frequency_lower'
		document_count_column = 'lower_index'
		listcorpus_document_count_column = 'document_frequency_lower'

	if page_size == 0:
		page_current = 1 # if returning all, then only interested in first page

	columns = ['rank', 'token_id', 'token', 'frequency']

	count_tokens, tokens_descriptor, total_descriptor = self.corpus.get_token_count_text(exclude_punctuation)

	formatted_data = []
	formatted_data.append(f'Report based on {tokens_descriptor}')

	df = self.corpus.vocab.filter(pl.col(frequency_column).is_not_null())
	# ALWAYS remove spaces!
	df = df.filter(pl.col('is_space') == False)

	if exclude_tokens:
		excluded_tokens_count = df.filter(pl.col('token').is_in(exclude_tokens)).select(pl.len()).collect(engine='streaming').item()
		df = df.filter(~pl.col('token').is_in(exclude_tokens))
		if exclude_tokens_text == '':
			formatted_data.append(f'Tokens excluded from report: {excluded_tokens_count}')
		else:
			formatted_data.append(f'{exclude_tokens_text}')
			
	if restrict_tokens:
		df = df.filter(pl.col('token').is_in(restrict_tokens))
		if restrict_tokens_text == '':
			formatted_data.append(f'')
		else:
			formatted_data.append(f'{restrict_tokens_text}')

	if exclude_punctuation:
		df = df.filter(pl.col('is_punct') == False)

	df = df.sort(by = frequency_column, descending=True)

	unique_tokens = df.select(pl.len()).collect(engine='streaming').item()

	if page_size == 0:
		rank_offset = 1 # not really needed, but just in case future changes
	else:
		df = df.slice((page_current-1)*page_size, page_size)
		rank_offset = (page_current-1) * page_size+1

	if show_document_frequency:
		columns.append('document_frequency')
		if type(self.corpus) == ListCorpus:
			df = df.with_columns(pl.col(listcorpus_document_count_column).alias('document_frequency'))
		else:
			document_counts = self.corpus.tokens.select(pl.col(document_count_column).alias('token_id'), pl.col('token2doc_index')).group_by('token_id').agg(pl.col('token2doc_index').n_unique().alias('document_frequency'))
			df = df.join(document_counts, on='token_id', how='left', maintain_order='left')

	df = df.rename({frequency_column: "frequency"}).select(*columns)

	df = df.with_columns(((pl.col("frequency") / count_tokens) * normalize_by).alias('normalized_frequency'))
	columns.append('normalized_frequency')

	df = df.drop('rank').with_row_index(name='rank', offset=rank_offset)

	if show_token_id == False:
		df = df.drop('token_id')

	if normalize_by is not None:
		formatted_data.append(f'Normalized Frequency is per {normalize_by:,.0f} tokens')

	formatted_data.append(f'{total_descriptor}: {count_tokens:,.0f}')

	formatted_data.append(f'Unique {tokens_descriptor}: {unique_tokens:,.0f}')
	if page_size != 0 and unique_tokens > page_size:
		formatted_data.append(f'Showing {page_size} rows')
		formatted_data.append(f'Page {page_current} of {unique_tokens // page_size + 1}')

	logger.info(f'Frequencies report time: {(time.time() - start_time):.5f} seconds')

	return Result(type = 'frequencies', df=df, title='Frequencies', description=f'Frequencies of {tokens_descriptor}, {self.corpus.name}', summary_data={}, formatted_data=formatted_data)

