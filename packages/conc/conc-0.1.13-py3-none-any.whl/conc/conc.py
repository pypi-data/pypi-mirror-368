"""An interface to create Conc reports for corpus linguistic analysis of frequency, concordances, ngrams, keyness, and collocation."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/api/50_conc.ipynb.

# %% ../nbs/api/50_conc.ipynb 3
from __future__ import annotations
import time
from fastcore.basics import patch

# %% auto 0
__all__ = ['Conc']

# %% ../nbs/api/50_conc.ipynb 4
from .result import Result
from .plot import Plot
from .core import PAGE_SIZE, logger
from .corpus import Corpus
from .listcorpus import ListCorpus
from .frequency import Frequency
from .ngrams import Ngrams
from .concordance import Concordance
from .keyness import Keyness
from .collocates import Collocates

# %% ../nbs/api/50_conc.ipynb 8
class Conc:
	"""Unified interface to Conc reporting for analysis of frequency, ngrams, concordances, keyness, and collocates."""
	
	def __init__(self, 
				corpus # Corpus instance
				):
		# information about corpus
		if type(corpus) != Corpus:
			raise ValueError('Conc functionality is only available for instances of the Corpus class.')

		self.corpus = corpus
		self.frequency_ = Frequency(corpus)
		self.ngrams_ = Ngrams(corpus)
		self.concordance_ = Concordance(corpus)
		self.keyness_ = None
		self.collocates_ = Collocates(corpus)

# %% ../nbs/api/50_conc.ipynb 13
@patch
def frequencies(self: Conc,
				case_sensitive:bool=False, # frequencies for tokens with or without case preserved 
				normalize_by:int=10000, # normalize frequencies by a number (e.g. 10000)
				page_size:int=PAGE_SIZE, # number of rows to return, if 0 returns all
				page_current:int=1, # current page, ignored if page_size is 0
				show_token_id:bool=False, # show token_id in output
				show_document_frequency:bool=False, # show document frequency in output
				exclude_tokens:list[str]=[], # exclude specific tokens from frequency report, can be used to remove stopwords
				exclude_tokens_text:str = '', # text to explain which tokens have been excluded, will be added to the report notes
				restrict_tokens:list[str]=[], # restrict frequency report to return frequencies for a list of specific tokens
				restrict_tokens_text:str = '', # text to explain which tokens are included, will be added to the report notes
				exclude_punctuation:bool=True # exclude punctuation tokens
				) -> Result: # return a Result object with the frequency table
	""" Report frequent tokens. """
	return self.frequency_.frequencies(case_sensitive=case_sensitive,
										normalize_by=normalize_by,
										page_size=page_size,
										page_current=page_current,
										show_token_id=show_token_id,
										show_document_frequency=show_document_frequency,
										exclude_tokens=exclude_tokens,
										exclude_tokens_text=exclude_tokens_text,
										restrict_tokens=restrict_tokens,
										restrict_tokens_text=restrict_tokens_text,
										exclude_punctuation=exclude_punctuation)

# %% ../nbs/api/50_conc.ipynb 17
@patch
def ngrams(self: Conc, 
		   token_str: str, # token string to get ngrams for 
		   ngram_length:int = 2, # length of ngram
		   ngram_token_position: str = 'LEFT', # specify if token sequence is on LEFT or RIGHT or MIDDLE (support for other positions is in-development)
		   normalize_by:int=10000, # normalize frequencies by a number (e.g. 10000)
		   page_size:int = PAGE_SIZE, # number of results to display per results page 
		   page_current:int = 1, # current page of results
		   show_all_columns:bool = False, # return raw df with all columns or just ngram and frequency
		   exclude_punctuation:bool=True, # do not return ngrams with punctuation tokens
		   use_cache:bool = True # retrieve the results from cache if available
		   ) -> Result: # return a Result object with ngram data
	""" Report ngram frequencies containing a token string. """

	return self.ngrams_.ngrams(token_str, 
							ngram_length=ngram_length, 
							ngram_token_position=ngram_token_position, 
							normalize_by=normalize_by,
							page_size=page_size, 
							page_current=page_current, 
							show_all_columns=show_all_columns, 
							exclude_punctuation=exclude_punctuation,
							use_cache=use_cache)

# %% ../nbs/api/50_conc.ipynb 20
@patch
def ngram_frequencies(self: Conc, 
                ngram_length:int=2, # length of ngram
                case_sensitive:bool=False, # frequencies for tokens lowercased or with case preserved
				normalize_by:int=10000, # normalize frequencies by a number (e.g. 10000)
				page_size:int=PAGE_SIZE, # number of rows to return
				page_current:int=1, # current page
                show_document_frequency:bool=False, #show document frequency in output (slow to compute for large corpora)
				exclude_punctuation:bool=True # exclude ngrams containing punctuation tokens
				) -> Result: # return a Result object with the frequency table
    """ Report frequent ngrams. """
    return self.ngrams_.ngram_frequencies(ngram_length=ngram_length,
                                    case_sensitive=case_sensitive,
                                    normalize_by=normalize_by,
                                    page_size=page_size,
                                    page_current=page_current,
                                    show_document_frequency=show_document_frequency,
                                    exclude_punctuation=exclude_punctuation)

# %% ../nbs/api/50_conc.ipynb 23
@patch
def concordance(self: Conc, 
				token_str: str, # token string to get concordance for 
				context_length:int = 5, # number of words to show on left and right of token string
				order:str='1R2R3R', # order of sort columns - one of 1L2L3L, 3L2L1L, 2L1L1R, 1L1R2R, 1R2R3R (default if ommitted), LEFT, RIGHT
				page_size:int=PAGE_SIZE, # number of results to display per results page
				page_current:int=1, # current page of results
				show_all_columns:bool = False, # df with all columns or just essentials
				use_cache:bool = True, # retrieve the results from cache if available (currently ignored)
				ignore_punctuation:bool = True, # whether to ignore punctuation in the concordance sort
				filter_context_str:str|None = None, # if a string is provided, the concordance lines will be filtered to show lines with contexts containing this string
				filter_context_length:int|tuple[int, int]=5, # ignored if filter_context_str is None, otherwise this is the context window size per side in tokens - if an int (e.g. 5) context lengths on left and right will be the same, for independent control of left and right context length pass a tuple (context_length_left, context_left_right)
				) -> Result: # concordance report results
	""" Report concordance for a token string. """
	return self.concordance_.concordance(token_str = token_str, 
									  context_length=context_length, 
									  order=order, page_size=page_size, 
									  page_current=page_current, 
									  show_all_columns=show_all_columns, 
									  use_cache=use_cache,
									  ignore_punctuation=ignore_punctuation,
									  filter_context_str=filter_context_str,
									  filter_context_length=filter_context_length
									  )

# %% ../nbs/api/50_conc.ipynb 26
@patch
def concordance_plot(self: Conc,
				token_str: str, # token string for concordance plot
				page_size: int = 10, # number of plots per page
				append_info: bool = True # append token position info to the concordance line preview screens visible when hover over the plot lines
				) -> Plot: # concordance plot object, add .display() to view in notebook
	"""Create a concordance plot."""
	return self.concordance_.concordance_plot(token_str=token_str,
												page_size=page_size,
												append_info=append_info)

# %% ../nbs/api/50_conc.ipynb 28
@patch
def set_reference_corpus(self: Conc, 
                    corpus: Corpus | ListCorpus # Reference corpus
                    ) -> None:
    """ Set a reference corpus for keyness analysis. """
    self.keyness_ = Keyness(self.corpus, corpus)

# %% ../nbs/api/50_conc.ipynb 30
@patch
def keywords(self: Conc,
				effect_size_measure:str = 'log_ratio', # effect size measure to use, currently only 'log_ratio' is supported
				statistical_significance_measure:str = 'log_likelihood', # statistical significance measure to use, currently only 'log_likelihood' is supported
				order:str|None = None, # default of None orders by effect size measure, results can also be ordered by: frequency, frequency_reference, document_frequency, document_frequency_reference, log_likelihood
				order_descending:bool = True, # order is descending or ascending
				statistical_significance_cut: float|None = None, # statistical significance p-value to filter results, e.g. 0.05 or 0.01 or 0.001 - ignored if None or 0
				apply_bonferroni:bool = False, # apply Bonferroni correction to the statistical significance cut-off
				min_document_frequency: int = 0, # minimum document frequency in target for token to be included in the report
				min_document_frequency_reference: int = 0, # minimum document frequency in reference for token to be included in the report
				min_frequency: int = 0, # minimum frequency in target for token to be included in the report
				min_frequency_reference: int = 0, # minimum document frequency in reference for token to be included in the report
				case_sensitive:bool=False, # frequencies for tokens with or without case preserved 
				normalize_by:int=10000, # normalize frequencies by a number (e.g. 10000)
				page_size:int=PAGE_SIZE, # number of rows to return, if 0 returns all
				page_current:int=1, # current page, ignored if page_size is 0
				show_document_frequency:bool=False, # show document frequency in output
				exclude_tokens:list[str]=[], # exclude specific tokens from report results
				exclude_tokens_text:str = '', # text to explain which tokens have been excluded, will be added to the report notes
				restrict_tokens:list[str]=[], # restrict report to return results for a list of specific tokens
				restrict_tokens_text:str = '', # text to explain which tokens are included, will be added to the report notes
				exclude_punctuation:bool=True, # exclude punctuation tokens
				handle_common_typographic_differences:bool=True, # whether to detect and normalize common differences in word tokens due to typographic differences (i.e. currently focused on apostrophes in common English contractions), ignored when exclude_punctuation is False
				exclude_negative_keywords:bool=True # whether to exclude negative keywords from the report
				) -> Result: # return a Result object with the frequency table
	""" Get keywords for the corpus. """
	if self.keyness_ is None:
		raise ValueError("Reference corpus is not set. Use 'set_reference_corpus' to set the reference corpus.")
	return self.keyness_.keywords(effect_size_measure=effect_size_measure,
									statistical_significance_measure=statistical_significance_measure,
									order=order,
									order_descending=order_descending,
									statistical_significance_cut=statistical_significance_cut,
									apply_bonferroni=apply_bonferroni,
									min_document_frequency=min_document_frequency,
									min_document_frequency_reference=min_document_frequency_reference,
									min_frequency=min_frequency,
									min_frequency_reference=min_frequency_reference,
									case_sensitive=case_sensitive,
									normalize_by=normalize_by,
									page_size=page_size,
									page_current=page_current,
									show_document_frequency=show_document_frequency,
									exclude_tokens=exclude_tokens,
									exclude_tokens_text=exclude_tokens_text,
									restrict_tokens=restrict_tokens,
									restrict_tokens_text=restrict_tokens_text,
									exclude_punctuation=exclude_punctuation,
									handle_common_typographic_differences = handle_common_typographic_differences,
									exclude_negative_keywords=exclude_negative_keywords
									)

# %% ../nbs/api/50_conc.ipynb 33
@patch
def collocates(self: Conc, 
				token_str:str, # Token to search for
				effect_size_measure:str = 'logdice', # statistical measure to use for collocation calculation: logdice, mutual_information
				statistical_significance_measure:str = 'log_likelihood', # statistical significance measure to use, currently only 'log_likelihood' is supported
				order:str|None = None, # default of None orders by collocation_measure, results can also be ordered by: collocate_frequency, frequency, log_likelihood
				order_descending:bool = True, # order is descending or ascending
				statistical_significance_cut: float|None = None, # statistical significance p-value to filter results, e.g. 0.05 or 0.01 or 0.001 - ignored if None or 0
				apply_bonferroni:bool = False, # apply Bonferroni correction to the statistical significance cut-off
				context_length:int|tuple[int, int]=5, # Window size per side in tokens - if an int (e.g. 5) context lengths on left and right will be the same, for independent control of left and right context length pass a tuple (context_length_left, context_left_right) (e.g. (0, 5)) 
				min_collocate_frequency:int=5, # Minimum count of collocates
				page_size:int=PAGE_SIZE, # number of rows to return, if 0 returns all
				page_current:int=1, # current page, ignored if page_size is 0
				exclude_punctuation:bool=True # exclude punctuation tokens				
				) -> Result:

	""" Report collocates for a given token string. """

	return self.collocates_.collocates(token_str, 
										effect_size_measure=effect_size_measure, 
										statistical_significance_measure=statistical_significance_measure, 
										order=order, 
										order_descending=order_descending, 
										statistical_significance_cut=statistical_significance_cut, 
										apply_bonferroni=apply_bonferroni, 
										context_length=context_length, 
										min_collocate_frequency=min_collocate_frequency, 
										page_size=page_size, 
										page_current=page_current,
										exclude_punctuation=exclude_punctuation)
