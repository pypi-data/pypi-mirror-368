"""Helper functions and classes for Conc."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/api/80_core.ipynb.

# %% ../nbs/api/80_core.ipynb 3
from __future__ import annotations
import re
import os
import logging
from great_tables import GT
import polars as pl
import msgspec
import spacy
from memory_profiler import _get_memory

# %% auto 0
__all__ = ['PAGE_SIZE', 'EOF_TOKEN_STR', 'ERR_TOKEN_STR', 'DOCUMENTATION_URL', 'REPOSITORY_URL', 'PYPI_URL', 'CITATION_STR',
           'logger', 'set_logger_state', 'spacy_attribute_name', 'CorpusMetadata', 'get_stop_words', 'list_corpora',
           'create_toy_corpus_sources', 'show_toy_corpus', 'get_nltk_corpus_sources', 'get_garden_party',
           'get_large_dataset', 'create_large_dataset_sizes']

# %% ../nbs/api/80_core.ipynb 4
from . import __version__

# %% ../nbs/api/80_core.ipynb 5
PAGE_SIZE = 20
EOF_TOKEN_STR = ' conc-end-of-file-token'
ERR_TOKEN_STR = 'ERROR: not a token'

# %% ../nbs/api/80_core.ipynb 6
DOCUMENTATION_URL = 'https://geoffford.nz/conc'
REPOSITORY_URL = 'https://github.com/polsci/conc'
PYPI_URL = 'https://pypi.org/project/conc/'
CITATION_STR = f'If you use Conc in your work, please cite it as follows: Ford, G. (2025). Conc: a Python library for efficient corpus analysis (Version {__version__}) [Computer software]. https://doi.org/10.5281/zenodo.16358752'

# %% ../nbs/api/80_core.ipynb 10
class ConcLogger(logging.Logger):
	""" Custom logger for conc module. """
	def __init__(self, name, level=logging.WARNING, log_file=None):
		super().__init__(name, level)
		self._setup_handler(log_file)
		self.last_memory_usage = None

	def _setup_handler(self, log_file = None):
		console_handler = logging.StreamHandler()
		formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(funcName)s - %(message)s', 
									  datefmt='%Y-%m-%d %H:%M:%S')
		console_handler.setFormatter(formatter)
		self.addHandler(console_handler)

		if log_file is not None:
			file_handler = logging.FileHandler(log_file)
			file_handler.setFormatter(formatter)
			self.addHandler(file_handler)

	def set_state(self, state:str # 'quiet' or 'verbose'
				  ):
		if state == 'quiet':
			level = logging.WARNING
		elif state == 'verbose':
			level = logging.DEBUG
		else:
			raise ValueError(f"Invalid state: {state}")
		
		self.setLevel(level)

	def memory_usage(self, message = '', init=False):
		if init:
			self.last_memory_usage = None
		usage = _get_memory(-1, 'psutil', include_children=True)
		if self.last_memory_usage is not None:
			difference = usage - self.last_memory_usage
			memory_message = f', memory usage: {usage} MB, difference: {difference} MB'
		else:
			memory_message = f', memory usage: {usage} MB'
		self.info(f"{message}{memory_message}")
		self.last_memory_usage = usage


# %% ../nbs/api/80_core.ipynb 11
logging.setLoggerClass(ConcLogger)

logger = logging.getLogger(__name__)


# %% ../nbs/api/80_core.ipynb 12
def set_logger_state(state:str # 'quiet' or 'verbose'
					 ):
	""" Set the state of the conc logger to either 'quiet' or 'verbose' """
	logger.set_state(state)

# %% ../nbs/api/80_core.ipynb 15
def spacy_attribute_name(index):
	"""Get name of index from spacy."""

	return list(spacy.attrs.IDS.keys())[list(spacy.attrs.IDS.values()).index(index)]

# %% ../nbs/api/80_core.ipynb 17
class CorpusMetadata(msgspec.Struct): 
    """ JSON validation schema for corpus metadata """
    name: str
    description: str
    slug: str
    conc_version: str
    document_count: int
    token_count: int
    word_token_count: int
    punct_token_count: int
    space_token_count: int
    unique_tokens: int
    unique_word_tokens: int
    date_created: str
    EOF_TOKEN: int
    SPACY_EOF_TOKEN: int
    SPACY_MODEL: str
    SPACY_MODEL_VERSION: str
    punct_tokens: list[int]
    space_tokens: list[int]



# %% ../nbs/api/80_core.ipynb 20
def get_stop_words(save_path:str, # directory to save stop words to, file name will be created based on spaCy model name
				   spacy_model:str = 'en_core_web_sm' # model to get stop words for
					):
	""" Get stop words from spaCy and cache to disk """

	stop_words = None

	filename = f'{spacy_model}_stop_words.txt'
	save_to = os.path.join(save_path, filename)

	if os.path.exists(save_to):
		with open(save_to, 'r', encoding='utf-8') as f:
			stop_words = sorted(set(f.read().splitlines()))

	if stop_words is None:
		nlp = spacy.load(spacy_model)
		stop_words = nlp.Defaults.stop_words
		del nlp

		if not os.path.exists(save_path):
			os.makedirs(save_path)

		stop_words = sorted(stop_words)

		with open(save_to, 'w', encoding='utf-8') as f:
			for word in stop_words:
				f.write(word + '\n')

	return stop_words

# %% ../nbs/api/80_core.ipynb 23
def list_corpora(
		path: str # path to load corpus
		) -> pl.DataFrame: # Dataframe with path, corpus, corpus name, document count, token count
	""" (Deprecated - call via conc.corpora) Scan a directory for available corpora """
	
	logger.warning(DeprecationWarning("Calling list_corpora via conc.core is deprecated and will be removed by v1.0.0, instead import with 'from conc.corpora import list_corpora' and call as before."))

	from conc.corpora import list_corpora as _list_corpora
	return _list_corpora(path=path)


# %% ../nbs/api/80_core.ipynb 24
def create_toy_corpus_sources(source_path:str # path to location of sources for building corpora
							 ):
	""" (Deprecated - call via conc.corpora) Create txt files and csv to test build of toy corpus. """

	logger.warning(DeprecationWarning("Calling create_toy_corpus_sources via conc.core is deprecated and will be removed by v1.0.0, instead import with 'from conc.corpora import create_toy_corpus_sources' and call as before."))

	from conc.corpora import create_toy_corpus_sources as _create_toy_corpus_sources
	return _create_toy_corpus_sources(source_path=source_path)

# %% ../nbs/api/80_core.ipynb 25
def show_toy_corpus(
        csv_path:str # path to location of csv for building corpora
        ) -> GT: 
    """ (Deprecated - call via conc.corpora) Show toy corpus in a table. """

    logger.warning(DeprecationWarning("Calling show_toy_corpus via conc.core is deprecated and will be removed by v1.0.0, instead import with 'from conc.corpora import show_toy_corpus' and call as before."))
    
    from conc.corpora import show_toy_corpus as _show_toy_corpus
    return _show_toy_corpus(csv_path=csv_path)

# %% ../nbs/api/80_core.ipynb 26
def get_nltk_corpus_sources(source_path:str # path to location of sources for building corpora
							 ):
	""" (Deprecated - call via conc.corpora) Get NLTK corpora as sources for development or testing Conc functionality. """

	logger.warning(DeprecationWarning("Calling get_nltk_corpus_sources via conc.core is deprecated and will be removed by v1.0.0, instead import with 'from conc.corpora import get_nltk_corpus_sources' and call as before."))

	from conc.corpora import get_nltk_corpus_sources as _get_nltk_corpus_sources
	return _get_nltk_corpus_sources(source_path=source_path)

# %% ../nbs/api/80_core.ipynb 27
def get_garden_party(source_path: str #path to location of sources for building corpora
					):
	""" (Deprecated - call via conc.corpora) Get corpus of The Garden Party by Katherine Mansfield for development of Conc and testing Conc functionality. """

	logger.warning(DeprecationWarning("Calling get_garden_party via conc.core is deprecated and will be removed by v1.0.0, instead import with 'from conc.corpora import get_garden_party' and call as before."))

	from conc.corpora import get_garden_party as _get_garden_party
	return _get_garden_party(source_path=source_path)

# %% ../nbs/api/80_core.ipynb 28
def get_large_dataset(source_path: str #path to location of sources for building corpora
                    ):
    """ (Deprecated - call via conc.corpora) Get 1m rows of https://huggingface.co/datasets/Eugleo/us-congressional-speeches-subset for testing. """

    logger.warning(DeprecationWarning("Calling get_large_dataset via conc.core is deprecated and will be removed by v1.0.0, instead import with 'from conc.corpora import get_large_dataset' and call as before."))

    from conc.corpora import get_large_dataset as _get_large_dataset
    return _get_large_dataset(source_path=source_path)

# %% ../nbs/api/80_core.ipynb 29
def create_large_dataset_sizes(source_path: str, #path to location of sources for building corpora
						sizes: list = [10000, 100000, 200000, 500000] # list of sizes for test data-sets
						):
	""" (Deprecated - call via conc.corpora) Create datasets of different sizes from data source retrieved by get_large_dataset for testing. """
	
	logger.warning(DeprecationWarning("Calling create_large_dataset_sizes via conc.core is deprecated and will be removed by v1.0.0, instead import with 'from conc.corpora import create_large_dataset_sizes' and call as before."))

	from conc.corpora import create_large_dataset_sizes as _create_large_dataset_sizes
	return _create_large_dataset_sizes(source_path=source_path, sizes = sizes)

