from mcp.server.fastmcp import FastMCP
from core.ragflow_api import (
    get_datasets, get_dataset_documents, create_dataset, create_chunk, 
    retrieve_chunks, create_empty_document, find_document_dataset, 
    download_document, upload_document, parse_documents
)
import sys
import argparse
import os
import json

mcp = FastMCP("ragflow_mcp")

# å…¨å±€é…ç½®å˜é‡
ADDRESS = None
API_KEY = None
DATASET_NAME = None
DATASET_ID = None

@mcp.tool(name="list_all_datasets", description="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†")
def list_all_datasets() -> str:
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†ï¼Œè¿”å›ç®€åŒ–çš„æ•°æ®é›†ä¿¡æ¯ã€‚
    :return: è¿”å›åŒ…å«è¯­è¨€ã€åç§°ã€IDå’Œchunkæ–¹æ³•çš„æ•°æ®é›†åˆ—è¡¨ã€‚
    ä¾‹å¦‚ï¼š[{"language": "Chinese", "name": "ç¬¬äºŒå¤§è„‘", "id": "c3303d4ee45611ef9b610242ac180003", "chunk_method": "qa"}]
    """
    raw = get_datasets(
        address=ADDRESS,
        api_key=API_KEY,
        page=1,
        page_size=20  # è·å–æ›´å¤šæ•°æ®é›†
    )
    
    # è§£æè¿”å›çš„æ•°æ®
    if raw and raw.get('code') == 0 and 'data' in raw:
        datasets = raw['data']
        # æå–æŒ‡å®šçš„å­—æ®µ
        dataset_list = []
        for dataset in datasets:
            simplified_dataset = {
                "language": dataset.get('language', 'Unknown'),
                "name": dataset.get('name', 'Unknown'),
                "id": dataset.get('id', 'Unknown'),
                "chunk_method": dataset.get('chunk_method', 'Unknown')
            }
            dataset_list.append(simplified_dataset)
        return str(dataset_list)
    else:
        return "[]"  # å¦‚æœæ²¡æœ‰æ•°æ®æˆ–è¯·æ±‚å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨

@mcp.tool(name="list_all_documents", description="åˆ—å‡ºæ‰€æœ‰æ•°æ®ä¸‹çš„æ–‡æ¡£")
def list_all_documents(dataset_id = DATASET_ID) -> str:
    """
    åˆ—å‡ºæŒ‡å®šæ•°æ®é›†ä¸‹çš„æ‰€æœ‰æ–‡æ¡£ã€‚
    :param dataset_id: æ•°æ®é›†IDï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨é»˜è®¤æ•°æ®é›†IDã€‚é€šå¸¸æƒ…å†µä¸‹ä½¿ç”¨é»˜è®¤æ•°æ®é›†IDã€‚
    :return: è¿”å›æ‰€æœ‰æ–‡æ¡£çš„åç§°å’ŒIDåˆ—è¡¨ã€‚
    ä¾‹å¦‚ï¼š[{"id": "doc1", "name": "æ–‡æ¡£1"}, {"id": "doc2", "name": "æ–‡æ¡£2"}]
    """
    raw = get_dataset_documents(
        address=ADDRESS,
        api_key=API_KEY,
        dataset_id=dataset_id,
        page=1,
        page_size=100,  # è®¾ç½®ä¸º100ä»¥è·å–æ›´å¤šæ–‡æ¡£
        keywords=None,  # å¯é€‰å‚æ•°
        orderby="update_time"
    )
    
    # è§£æè¿”å›çš„æ•°æ®
    if raw and raw.get('code') == 0 and 'data' in raw:
        docs = raw['data'].get('docs', [])
        # æå–æ–‡æ¡£çš„IDå’Œåç§°
        document_list = [{"id": doc['id'], "name": doc['name']} for doc in docs]
        return str(document_list)
    else:
        return "[]"  # å¦‚æœæ²¡æœ‰æ•°æ®æˆ–è¯·æ±‚å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨

@mcp.tool(name="create_new_dataset", description="åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®é›†")
def create_new_dataset(name: str, description: str = None, language: str = "Chinese", 
                      chunk_method: str = "naive", embedding_model: str = None) -> str:
    """
    åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®é›†ã€‚
    
    :param name: æ•°æ®é›†åç§°ï¼Œå¿…éœ€å‚æ•°ã€‚åªèƒ½åŒ…å«è‹±æ–‡å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼Œä»¥å­—æ¯æˆ–ä¸‹åˆ’çº¿å¼€å¤´
    :param description: æ•°æ®é›†æè¿°ï¼Œå¯é€‰å‚æ•°
    :param language: è¯­è¨€è®¾ç½®ï¼Œå¯é€‰å‚æ•°ã€‚å¯é€‰å€¼: "Chinese" (é»˜è®¤), "English"
    :chunk_method: åˆ†å—æ–¹æ³•ï¼Œå¯é€‰å€¼:
            "naive": é€šç”¨ (é»˜è®¤)
            "manual": æ‰‹åŠ¨
            "qa": é—®ç­”
            "table": è¡¨æ ¼
            "paper": è®ºæ–‡
            "book": ä¹¦ç±
            "laws": æ³•å¾‹
            "presentation": æ¼”ç¤ºæ–‡ç¨¿
            "picture": å›¾ç‰‡
            "one": å•ä¸€
            "knowledge_graph": çŸ¥è¯†å›¾è°±
            "email": é‚®ä»¶
    :param embedding_model: åµŒå…¥æ¨¡å‹åç§°ï¼Œå¯é€‰å‚æ•°ã€‚ä¾‹å¦‚: "BAAI/bge-zh-v1.5"
    :return: è¿”å›åˆ›å»ºç»“æœçš„çŠ¶æ€ä¿¡æ¯
    """
    result = create_dataset(
        address=ADDRESS,
        api_key=API_KEY,
        name=name,
        description=description,
        language=language,
        chunk_method=chunk_method,
        embedding_model=embedding_model,
        permission="me"  # å›ºå®šä¸º "me"
    )
    
    if result and result.get('code') == 0:
        dataset_info = result.get('data', {})
        dataset_id = dataset_info.get('id', 'æœªçŸ¥ID')
        dataset_name = dataset_info.get('name', name)
        return f"æˆåŠŸåˆ›å»ºæ•°æ®é›† '{dataset_name}'ï¼ŒID: {dataset_id}"
    else:
        error_msg = result.get('message', 'æœªçŸ¥é”™è¯¯') if result else 'è¯·æ±‚å¤±è´¥'
        return f"åˆ›å»ºæ•°æ®é›†å¤±è´¥: {error_msg}"

@mcp.tool(name="create_empty_document", description="åœ¨æ•°æ®é›†ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºç™½æ–‡æ¡£")
def create_empty_document_tool(document_name: str, dataset_id: str = DATASET_ID) -> str:
    """
    åœ¨æŒ‡å®šæ•°æ®é›†ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºç™½æ–‡æ¡£ã€‚
    
    :param document_name: æ–‡æ¡£åç§°ï¼Œå¿…éœ€å‚æ•°
    :param dataset_id: æ•°æ®é›†IDï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨é»˜è®¤æ•°æ®é›†ID
    :return: è¿”å›åˆ›å»ºç»“æœçš„çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…å«æ–°æ–‡æ¡£çš„ID
    """
    result = create_empty_document(
        address=ADDRESS,
        api_key=API_KEY,
        dataset_id=dataset_id,
        document_name=document_name
    )
    
    if result and result.get('code') == 0:
        # å¤„ç† API è¿”å›çš„æ•°æ®ç»“æ„ï¼ˆå¯èƒ½æ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼‰
        data = result.get('data', {})
        
        # æ ¹æ®å®é™…è¿”å›ç»“æ„è°ƒæ•´
        if isinstance(data, list) and len(data) > 0:
            doc_info = data[0]  # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
        elif isinstance(data, dict):
            doc_info = data
        else:
            return f"åˆ›å»ºç©ºç™½æ–‡æ¡£æˆåŠŸï¼Œä½†è¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸: {data}"
            
        doc_id = doc_info.get('id', 'æœªçŸ¥ID')
        doc_name = doc_info.get('name', document_name)
        return f"âœ… æˆåŠŸåˆ›å»ºç©ºç™½æ–‡æ¡£ '{doc_name}'ï¼ŒID: {doc_id}ã€‚ç°åœ¨å¯ä»¥ä½¿ç”¨æ­¤IDæ·»åŠ chunksã€‚"
    else:
        error_msg = result.get('message', 'æœªçŸ¥é”™è¯¯') if result else 'è¯·æ±‚å¤±è´¥'
        return f"âŒ åˆ›å»ºç©ºç™½æ–‡æ¡£å¤±è´¥: {error_msg}"

@mcp.tool(name="create_chunk_to_document", description="åœ¨æŒ‡å®šæ–‡æ¡£ä¸­åˆ›å»ºæ–°çš„æ–‡æœ¬å—")
def create_chunk_to_document(document_id: str, content: str, important_keywords: list = None, dataset_id: str = None) -> str:
    """
    åœ¨æŒ‡å®šçš„æ–‡æ¡£ä¸­åˆ›å»ºæ–°çš„æ–‡æœ¬å—(chunk)ã€‚
    
    :param document_id: æ–‡æ¡£IDï¼Œå¿…éœ€å‚æ•°
    :param content: chunkçš„æ–‡æœ¬å†…å®¹ï¼Œå¿…éœ€å‚æ•°
    :param important_keywords: ä¸chunkç›¸å…³çš„å…³é”®è¯åˆ—è¡¨ï¼Œå¯é€‰å‚æ•°
    :param dataset_id: æ•°æ®é›†IDï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨æŸ¥æ‰¾æ–‡æ¡£æ‰€å±çš„æ•°æ®é›†
    :return: è¿”å›åˆ›å»ºç»“æœçš„çŠ¶æ€ä¿¡æ¯
    """
    # å¦‚æœæ²¡æœ‰æä¾›æ•°æ®é›†IDï¼Œå°è¯•è‡ªåŠ¨æŸ¥æ‰¾
    if not dataset_id:
        print(f"æœªæä¾›æ•°æ®é›†IDï¼Œæ­£åœ¨æŸ¥æ‰¾æ–‡æ¡£ {document_id} æ‰€å±çš„æ•°æ®é›†...")
        dataset_id = find_document_dataset(
            address=ADDRESS,
            api_key=API_KEY,
            document_id=document_id
        )
        
        if not dataset_id:
            return f"âŒ æ— æ³•æ‰¾åˆ°æ–‡æ¡£ {document_id} æ‰€å±çš„æ•°æ®é›†ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®š dataset_id å‚æ•°"
    
    result = create_chunk(
        address=ADDRESS,
        api_key=API_KEY,
        dataset_id=dataset_id,
        document_id=document_id,
        content=content,
        important_keywords=important_keywords
    )
    
    if result and result.get('code') == 0:
        return f"âœ… æˆåŠŸåˆ›å»ºchunkåˆ°æ–‡æ¡£ {document_id}ã€‚chunkå†…å®¹: {content[:50]}..."
    else:
        error_msg = result.get('message', 'æœªçŸ¥é”™è¯¯') if result else 'è¯·æ±‚å¤±è´¥'
        return f"âŒ åˆ›å»ºchunkå¤±è´¥: {error_msg}"

def get_all_dataset_ids():
    """
    è·å–æ‰€æœ‰æ•°æ®é›†çš„IDåˆ—è¡¨
    
    :return: æ•°æ®é›†IDåˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›ç©ºåˆ—è¡¨
    """
    result = get_datasets(
        address=ADDRESS,
        api_key=API_KEY,
        page=1,
        page_size=100  # å‡è®¾ä¸ä¼šè¶…è¿‡100ä¸ªæ•°æ®é›†
    )
    
    if result and result.get('code') == 0:
        datasets = result.get('data', {})
        # å¤„ç†æ•°æ®é›†æ•°æ®çš„ä¸åŒæ ¼å¼
        if isinstance(datasets, list):
            # å¦‚æœdataæ˜¯åˆ—è¡¨
            return [ds.get('id') for ds in datasets if ds.get('id')]
        elif isinstance(datasets, dict) and 'datasets' in datasets:
            # å¦‚æœdataæ˜¯å­—å…¸ä¸”åŒ…å«datasetså­—æ®µ
            datasets_list = datasets.get('datasets', [])
            return [ds.get('id') for ds in datasets_list if ds.get('id')]
        else:
            print(f"æœªçŸ¥çš„æ•°æ®é›†æ•°æ®æ ¼å¼: {datasets}")
            return []
    else:
        print(f"è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {result}")
        return []

@mcp.tool(name="search_chunks", description="ä»æ•°æ®é›†ä¸­æ£€ç´¢ç›¸å…³çš„æ–‡æœ¬å—")
def search_chunks(question: str, dataset_id: str = None, page_size: int = 5, similarity_threshold: float = 0.1) -> str:
    """
    ä»æŒ‡å®šæ•°æ®é›†ä¸­æ£€ç´¢ä¸é—®é¢˜ç›¸å…³çš„æ–‡æœ¬å—ã€‚
    
    :param question: è¦æœç´¢çš„é—®é¢˜æˆ–å…³é”®è¯ï¼Œå¿…éœ€å‚æ•°
    :param dataset_id: æ•°æ®é›†IDï¼Œå¦‚æœæœªæä¾›åˆ™åœ¨æ‰€æœ‰æ•°æ®é›†ä¸­æœç´¢
    :param page_size: è¿”å›çš„æœ€å¤§ç»“æœæ•°é‡ï¼Œé»˜è®¤ä¸º5
    :param similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.1
    :return: è¿”å›æ£€ç´¢ç»“æœçš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    # ç¡®å®šè¦æœç´¢çš„æ•°æ®é›†IDåˆ—è¡¨
    if dataset_id:
        dataset_ids = [dataset_id]
        search_scope = f"æ•°æ®é›† {dataset_id}"
    else:
        # è·å–æ‰€æœ‰æ•°æ®é›†ID
        dataset_ids = get_all_dataset_ids()
        if not dataset_ids:
            return "âŒ æ— æ³•è·å–æ•°æ®é›†åˆ—è¡¨ï¼Œæ— æ³•è¿›è¡Œæœç´¢"
        search_scope = f"æ‰€æœ‰æ•°æ®é›†ï¼ˆå…±{len(dataset_ids)}ä¸ªï¼‰"
    
    result = retrieve_chunks(
        address=ADDRESS,
        api_key=API_KEY,
        question=question,
        dataset_ids=dataset_ids,
        page=1,
        page_size=page_size,
        similarity_threshold=similarity_threshold,
        vector_similarity_weight=0.5,
        top_k=50,
        keyword=False,  # å¯ç”¨å…³é”®è¯åŒ¹é…
        highlight=False  # å¯ç”¨é«˜äº®æ˜¾ç¤º
    )
    
    if result and result.get('code') == 0:
        chunks = result.get('data', {}).get('chunks', [])
        total_count = result.get('data', {}).get('total', len(chunks))
        
        if not chunks:
            return f"åœ¨{search_scope}ä¸­æœªæ‰¾åˆ°ä¸ '{question}' ç›¸å…³çš„å†…å®¹"
        
        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        formatted_results = []
        for i, chunk in enumerate(chunks[:page_size], 1):
            # ä¼˜å…ˆä½¿ç”¨é«˜äº®å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åŸå§‹å†…å®¹
            content = chunk.get('highlight', chunk.get('content', ''))
            similarity = chunk.get('similarity', 0)
            # ä¿®æ­£æ–‡æ¡£åç§°å­—æ®µ
            doc_name = chunk.get('document_keyword', chunk.get('document_name', 'æœªçŸ¥æ–‡æ¡£'))
            # è·å–å…³é”®è¯ä¿¡æ¯
            keywords = chunk.get('important_keywords', [])
            keywords_str = ', '.join(keywords) if keywords else ''
            
            # æ„å»ºç»“æœå­—ç¬¦ä¸²
            result_str = f"{i}. ã€{doc_name}ã€‘(ç›¸ä¼¼åº¦: {similarity:.3f})"
            if keywords_str:
                result_str += f"\nå…³é”®è¯: {keywords_str}"
            result_str += f"\n{content[:500]}{'...' if len(content) > 500 else ''}"
            
            formatted_results.append(result_str)
        
        return f"åœ¨{search_scope}ä¸­æ‰¾åˆ° {total_count} ä¸ªç›¸å…³ç»“æœï¼ˆæ˜¾ç¤ºå‰{len(chunks)}ä¸ªï¼‰ï¼š\n\n" + "\n\n".join(formatted_results)
    else:
        error_msg = result.get('message', 'æœªçŸ¥é”™è¯¯') if result else 'è¯·æ±‚å¤±è´¥'
        return f"æ£€ç´¢å¤±è´¥: {error_msg}"

@mcp.tool(name="download_document", description="ä»æ•°æ®é›†ä¸­ä¸‹è½½æŒ‡å®šçš„æ–‡æ¡£")
def download_document_tool(document_id: str, output_path: str = None, dataset_id: str = None) -> str:
    """
    ä»æŒ‡å®šæ•°æ®é›†ä¸­ä¸‹è½½æ–‡æ¡£åˆ°æœ¬åœ°ã€‚
    
    :param document_id: æ–‡æ¡£IDï¼Œå¿…éœ€å‚æ•°
    :param output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¯é€‰å‚æ•°ã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä¿å­˜åˆ°å½“å‰ç›®å½•ä¸‹ä»¥document_idå‘½åçš„æ–‡ä»¶
    :param dataset_id: æ•°æ®é›†IDï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨æŸ¥æ‰¾æ–‡æ¡£æ‰€å±çš„æ•°æ®é›†
    :return: è¿”å›ä¸‹è½½ç»“æœçš„çŠ¶æ€ä¿¡æ¯
    """
    # å¦‚æœæ²¡æœ‰æä¾›æ•°æ®é›†IDï¼Œå°è¯•è‡ªåŠ¨æŸ¥æ‰¾
    if not dataset_id:
        print(f"æœªæä¾›æ•°æ®é›†IDï¼Œæ­£åœ¨æŸ¥æ‰¾æ–‡æ¡£ {document_id} æ‰€å±çš„æ•°æ®é›†...")
        dataset_id = find_document_dataset(
            address=ADDRESS,
            api_key=API_KEY,
            document_id=document_id
        )
        
        if not dataset_id:
            return f"âŒ æ— æ³•æ‰¾åˆ°æ–‡æ¡£ {document_id} æ‰€å±çš„æ•°æ®é›†ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®š dataset_id å‚æ•°"
    
    # è°ƒç”¨ä¸‹è½½å‡½æ•°
    result_path = download_document(
        address=ADDRESS,
        api_key=API_KEY,
        dataset_id=dataset_id,
        document_id=document_id,
        output_path=output_path
    )
    
    if result_path:
        return f"âœ… æ–‡æ¡£ä¸‹è½½æˆåŠŸï¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {result_path}"
    else:
        return f"âŒ æ–‡æ¡£ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£IDå’Œæƒé™è®¾ç½®"

@mcp.tool(name="upload_documents", description="ä¸Šä¼ å•ä¸ªæˆ–å¤šä¸ªæ–‡æ¡£åˆ°RagFlow")
def upload_documents_tool(file_paths: list = None, dataset_id: str = None) -> str:
    """
    ç®€åŒ–çš„æ–‡æ¡£ä¸Šä¼ å·¥å…·ï¼Œæ”¯æŒå•ä¸ªæˆ–å¤šä¸ªæ–‡ä»¶ä¸Šä¼ ã€‚
    
    ç‰¹ç‚¹ï¼š
    - æ¥å—å•ä¸ªæ–‡ä»¶è·¯å¾„(å­—ç¬¦ä¸²)æˆ–å¤šä¸ªæ–‡ä»¶è·¯å¾„(åˆ—è¡¨)
    - è‡ªåŠ¨ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼Œä¸å…è®¸è‡ªå®šä¹‰åç§°
    - è®©APIå¤„ç†æ–‡ä»¶ç±»å‹éªŒè¯
    - ç®€åŒ–çš„é”™è¯¯å¤„ç†
    
    :param file_paths: æ–‡ä»¶è·¯å¾„ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²(å•ä¸ªæ–‡ä»¶)æˆ–åˆ—è¡¨(å¤šä¸ªæ–‡ä»¶)
    :param dataset_id: æ•°æ®é›†IDï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨é»˜è®¤æ•°æ®é›†
    :return: JSONæ ¼å¼çš„ä¸Šä¼ ç»“æœ
    """
    try:
        # å¦‚æœæ²¡æœ‰æä¾›æ•°æ®é›†IDï¼Œè·å–é»˜è®¤æ•°æ®é›†
        if not dataset_id:
            datasets_raw = get_datasets(
                address=ADDRESS,
                api_key=API_KEY,
                page=1,
                page_size=1
            )
            if not datasets_raw or datasets_raw.get('code') != 0:
                return json.dumps({
                    "error": "No datasets available. Please create a dataset first."
                }, ensure_ascii=False, indent=2)
            
            # è·å–ç¬¬ä¸€ä¸ªæ•°æ®é›†
            datasets = datasets_raw.get('data', [])
            if not datasets:
                return json.dumps({
                    "error": "No datasets found."
                }, ensure_ascii=False, indent=2)
                
            dataset_id = datasets[0]['id']
            
        # å¦‚æœfile_pathsæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(file_paths, str):
            file_paths = [file_paths]
            
        if not file_paths:
            return json.dumps({
                "error": "No file paths provided"
            }, ensure_ascii=False, indent=2)
            
        results = []
        
        # é€ä¸ªä¸Šä¼ æ–‡ä»¶
        for file_path in file_paths:
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(file_path):
                    results.append({
                        "file_path": file_path,
                        "error": "File not found"
                    })
                    continue
                
                # ä½¿ç”¨åŸå§‹æ–‡ä»¶å
                filename = os.path.basename(file_path)
                
                # ä¸Šä¼ æ–‡ä»¶
                result = upload_document(
                    address=ADDRESS,
                    api_key=API_KEY,
                    dataset_id=dataset_id,
                    file_path=file_path,
                    document_name=filename
                )
                
                results.append({
                    "file_path": file_path,
                    "filename": filename,
                    "result": result
                })
                
            except Exception as e:
                results.append({
                    "file_path": file_path,
                    "error": str(e)
                })
        
        return json.dumps({
            "dataset_id": dataset_id,
            "upload_results": results
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "error": f"Upload failed: {str(e)}"
        }, ensure_ascii=False, indent=2)

@mcp.tool(name="parse_documents", description="è§£ææŒ‡å®šæ•°æ®é›†ä¸­çš„æ–‡æ¡£ï¼Œç”Ÿæˆchunks")
def parse_documents_tool(document_ids: list = None, dataset_id: str = None) -> str:
    """
    è§£ææŒ‡å®šæ•°æ®é›†ä¸­çš„æ–‡æ¡£ï¼Œç”Ÿæˆæ–‡æœ¬å—(chunks)ã€‚
    
    ç‰¹ç‚¹ï¼š
    - æ¥å—å•ä¸ªæ–‡æ¡£ID(å­—ç¬¦ä¸²)æˆ–å¤šä¸ªæ–‡æ¡£ID(åˆ—è¡¨)
    - è§¦å‘RagFlowå¯¹æ–‡æ¡£è¿›è¡Œè§£æå’Œåˆ†å—å¤„ç†
    - ç®€åŒ–çš„é”™è¯¯å¤„ç†
    
    :param document_ids: æ–‡æ¡£IDï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²(å•ä¸ªæ–‡æ¡£)æˆ–åˆ—è¡¨(å¤šä¸ªæ–‡æ¡£)
    :param dataset_id: æ•°æ®é›†IDï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨é»˜è®¤æ•°æ®é›†
    :return: JSONæ ¼å¼çš„è§£æç»“æœ
    """
    try:
        # å¦‚æœæ²¡æœ‰æä¾›æ•°æ®é›†IDï¼Œè·å–é»˜è®¤æ•°æ®é›†
        if not dataset_id:
            datasets_raw = get_datasets(
                address=ADDRESS,
                api_key=API_KEY,
                page=1,
                page_size=1
            )
            if not datasets_raw or datasets_raw.get('code') != 0:
                return json.dumps({
                    "error": "No datasets available. Please create a dataset first."
                }, ensure_ascii=False, indent=2)
            
            # è·å–ç¬¬ä¸€ä¸ªæ•°æ®é›†
            datasets = datasets_raw.get('data', [])
            if not datasets:
                return json.dumps({
                    "error": "No datasets found."
                }, ensure_ascii=False, indent=2)
                
            dataset_id = datasets[0]['id']
            
        # å¦‚æœdocument_idsæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(document_ids, str):
            document_ids = [document_ids]
            
        if not document_ids:
            return json.dumps({
                "error": "No document IDs provided"
            }, ensure_ascii=False, indent=2)
            
        # è°ƒç”¨è§£ææ–‡æ¡£API
        result = parse_documents(
            address=ADDRESS,
            api_key=API_KEY,
            dataset_id=dataset_id,
            document_ids=document_ids
        )
        
        if result and result.get('code') == 0:
            return json.dumps({
                "success": True,
                "dataset_id": dataset_id,
                "document_ids": document_ids,
                "message": f"Successfully started parsing {len(document_ids)} documents",
                "result": result
            }, ensure_ascii=False, indent=2)
        else:
            error_msg = result.get('message', 'æœªçŸ¥é”™è¯¯') if result else 'è¯·æ±‚å¤±è´¥'
            return json.dumps({
                "error": f"Parse documents failed: {error_msg}",
                "dataset_id": dataset_id,
                "document_ids": document_ids
            }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "error": f"Parse documents failed: {str(e)}"
        }, ensure_ascii=False, indent=2)

def main():
    """
    ä¸»å‡½æ•°ï¼Œè§£æå‘½ä»¤è¡Œå‚æ•°å¹¶å¯åŠ¨MCPæœåŠ¡å™¨
    """
    global ADDRESS, API_KEY, DATASET_ID, DATASET_NAME
    
    parser = argparse.ArgumentParser(description='RagFlow MCP æœåŠ¡å™¨ (ç®€åŒ–ç‰ˆ)')
    parser.add_argument('--address', required=True, help='RagFlowæœåŠ¡å™¨åœ°å€ (ä¾‹å¦‚: ragflow.example.com)')
    parser.add_argument('--api-key', required=True, help='RagFlow APIå¯†é’¥')
    parser.add_argument('--dataset-id', help='é»˜è®¤æ•°æ®é›†ID')
    parser.add_argument('--dataset-name', default='ç¬¬äºŒå¤§è„‘', help='é»˜è®¤æ•°æ®é›†åç§°')
    
    args = parser.parse_args()
    
    # è®¾ç½®å…¨å±€é…ç½®
    ADDRESS = args.address
    API_KEY = args.api_key
    DATASET_ID = args.dataset_id
    DATASET_NAME = args.dataset_name
    
    # å¯åŠ¨å‰éªŒè¯è¿æ¥
    print(f"ğŸ”§ RagFlow MCP æœåŠ¡å™¨é…ç½® (ç®€åŒ–ç‰ˆ):", file=sys.stderr)
    print(f"   æœåŠ¡å™¨åœ°å€: {ADDRESS}", file=sys.stderr)
    print(f"   APIå¯†é’¥: {'âœ… å·²é…ç½®' if API_KEY else 'âŒ æœªé…ç½®'}", file=sys.stderr)
    print(f"   é»˜è®¤æ•°æ®é›†: {DATASET_NAME} ({DATASET_ID if DATASET_ID else 'æœªæŒ‡å®š'})", file=sys.stderr)
    
    # éªŒè¯è¿æ¥
    try:
        result = get_datasets(address=ADDRESS, api_key=API_KEY, page=1, page_size=1)
        if result and result.get('code') == 0:
            print("âœ… RagFlowè¿æ¥éªŒè¯æˆåŠŸ", file=sys.stderr)
        else:
            print("âŒ RagFlowè¿æ¥éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥åœ°å€å’ŒAPIå¯†é’¥", file=sys.stderr)
            sys.exit(1)
    except Exception as e:
        print(f"âŒ RagFlowè¿æ¥éªŒè¯å¤±è´¥: {e}", file=sys.stderr)
        sys.exit(1)
    
    # å¯åŠ¨MCPæœåŠ¡å™¨
    mcp.run(transport='stdio')

if __name__ == "__main__":
    main()
