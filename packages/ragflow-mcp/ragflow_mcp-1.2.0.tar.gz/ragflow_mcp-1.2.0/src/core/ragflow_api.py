import requests
import tempfile
import os

def get_datasets(address, api_key, page=1, page_size=10, orderby=None, desc=None, dataset_name=None, dataset_id=None):
    """
    è°ƒç”¨æ•°æ®é›†APIè·å–æ•°æ®é›†åˆ—è¡¨
    
    Args:
        address: APIæœåŠ¡å™¨åœ°å€
        api_key: APIå¯†é’¥
        page: é¡µç ï¼Œé»˜è®¤ä¸º1
        page_size: æ¯é¡µå¤§å°ï¼Œé»˜è®¤ä¸º10
        orderby: æ’åºå­—æ®µ
        desc: æ˜¯å¦é™åº
        dataset_name: æ•°æ®é›†åç§°è¿‡æ»¤
        dataset_id: æ•°æ®é›†IDè¿‡æ»¤
    
    Returns:
        å“åº”çš„JSONæ•°æ®
    """
    url = f"{address}/api/v1/datasets"
    
    # æ„å»ºæŸ¥è¯¢å‚æ•°
    params = {
        'page': page,
        'page_size': page_size
    }
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if orderby:
        params['orderby'] = orderby
    if desc is not None:
        params['desc'] = desc
    if dataset_name:
        params['name'] = dataset_name
    if dataset_id:
        params['id'] = dataset_id
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'Authorization': f'Bearer {api_key}'
    }
    
    try:
        response = requests.get(url, params=params, headers=headers)
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return None

def get_dataset_documents(address, api_key, dataset_id, page=1, page_size=30, orderby='create_time', desc=True, keywords=None, document_id=None, document_name=None):
    """
    è°ƒç”¨æ•°æ®é›†æ–‡æ¡£APIè·å–æŒ‡å®šæ•°æ®é›†çš„æ–‡æ¡£åˆ—è¡¨
    
    Args:
        address: APIæœåŠ¡å™¨åœ°å€
        api_key: APIå¯†é’¥
        dataset_id: æ•°æ®é›†ID (å¿…éœ€)
        page: é¡µç ï¼Œé»˜è®¤ä¸º1
        page_size: æ¯é¡µå¤§å°ï¼Œé»˜è®¤ä¸º30
        orderby: æ’åºå­—æ®µï¼Œå¯é€‰å€¼: create_time (é»˜è®¤), update_time
        desc: æ˜¯å¦é™åºï¼Œé»˜è®¤ä¸ºTrue
        keywords: ç”¨äºåŒ¹é…æ–‡æ¡£æ ‡é¢˜çš„å…³é”®è¯
        document_id: è¦æ£€ç´¢çš„æ–‡æ¡£ID
        document_name: æ–‡æ¡£åç§°è¿‡æ»¤
    
    Returns:
        å“åº”çš„JSONæ•°æ®
    """
    url = f"{address}/api/v1/datasets/{dataset_id}/documents"
    
    # æ„å»ºæŸ¥è¯¢å‚æ•°
    params = {
        'page': page,
        'page_size': page_size,
        'orderby': orderby,
        'desc': desc
    }
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if keywords:
        params['keywords'] = keywords
    if document_id:
        params['id'] = document_id
    if document_name:
        params['name'] = document_name
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'Authorization': f'Bearer {api_key}'
    }
    
    try:
        response = requests.get(url, params=params, headers=headers)
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return None

def create_dataset(address, api_key, name, avatar=None, description=None, language="English", 
                  embedding_model="textembedding3large", permission="me", chunk_method="naive", parser_config=None):
    """
    åˆ›å»ºæ•°æ®é›†API
    
    Args:
        address: APIæœåŠ¡å™¨åœ°å€
        api_key: APIå¯†é’¥
        name: æ•°æ®é›†åç§° (å¿…éœ€) - åªèƒ½åŒ…å«è‹±æ–‡å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼Œä»¥å­—æ¯æˆ–ä¸‹åˆ’çº¿å¼€å¤´ï¼Œæœ€å¤§65535å­—ç¬¦
        avatar: å¤´åƒçš„Base64ç¼–ç 
        description: æ•°æ®é›†æè¿°
        language: è¯­è¨€è®¾ç½®ï¼Œå¯é€‰å€¼: "English" (é»˜è®¤), "Chinese"
        embedding_model: åµŒå…¥æ¨¡å‹åç§°ï¼Œä¾‹å¦‚: "BAAI/bge-zh-v1.5"ï¼Œé»˜è®¤ç”¨textembedding3large@Azure-OpenAI
        permission: è®¿é—®æƒé™ï¼Œç›®å‰åªèƒ½è®¾ç½®ä¸º "me"
        chunk_method: åˆ†å—æ–¹æ³•ï¼Œå¯é€‰å€¼:
            "naive": é€šç”¨ (é»˜è®¤)
            "manual": æ‰‹åŠ¨
            "qa": é—®ç­”
            "table": è¡¨æ ¼
            "paper": è®ºæ–‡
            "book": ä¹¦ç±
            "laws": æ³•å¾‹
            "presentation": æ¼”ç¤ºæ–‡ç¨¿
            "picture": å›¾ç‰‡
            "one": å•ä¸€
            "knowledge_graph": çŸ¥è¯†å›¾è°±
            "email": é‚®ä»¶
        parser_config: è§£æå™¨é…ç½®ï¼ŒJSONå¯¹è±¡ï¼Œæ ¹æ®chunk_methodä¸åŒè€Œå˜åŒ–
    
    Returns:
        å“åº”çš„JSONæ•°æ®
    """
    url = f"{address}/api/v1/datasets"
    
    # æ„å»ºè¯·æ±‚ä½“
    data = {
        "name": name,
        "language": language,
        "permission": permission,
        "chunk_method": chunk_method
    }
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if avatar:
        data["avatar"] = avatar
    if description:
        data["description"] = description
    if embedding_model:
        data["embedding_model"] = embedding_model
    
    # å¤„ç†parser_config
    if parser_config is None:
        # æ ¹æ®chunk_methodè®¾ç½®é»˜è®¤parser_config
        if chunk_method == "naive":
            data["parser_config"] = {
                "chunk_token_count": 128,
                "layout_recognize": True,
                "html4excel": False,
                "delimiter": "\n!?ã€‚ï¼›ï¼ï¼Ÿ",
                "task_page_size": 12,
                "raptor": {"use_raptor": False}
            }
        elif chunk_method in ["qa", "manual", "paper", "book", "laws", "presentation"]:
            data["parser_config"] = {
                "raptor": {"use_raptor": False}
            }
        elif chunk_method in ["table", "picture", "one", "email"]:
            data["parser_config"] = {}
        elif chunk_method == "knowledge_graph":
            data["parser_config"] = {
                "chunk_token_count": 128,
                "delimiter": "\n!?ã€‚ï¼›ï¼ï¼Ÿ",
                "entity_types": ["organization", "person", "location", "event", "time"]
            }
    else:
        data["parser_config"] = parser_config
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    try:
        response = requests.post(url, json=data, headers=headers)
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        print(f"æ•°æ®é›†åˆ›å»ºæˆåŠŸ: {response.json()}")
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return None

def create_chunk(address, api_key, dataset_id, document_id, content, important_keywords=None):
    """
    åˆ›å»ºæ–‡æ¡£chunkçš„API
    
    Args:
        address: APIæœåŠ¡å™¨åœ°å€
        api_key: APIå¯†é’¥
        dataset_id: æ•°æ®é›†ID (å¿…éœ€)
        document_id: æ–‡æ¡£ID (å¿…éœ€)
        content: chunkçš„æ–‡æœ¬å†…å®¹ (å¿…éœ€)
        important_keywords: ä¸chunkç›¸å…³çš„å…³é”®è¯åˆ—è¡¨
    
    Returns:
        å“åº”çš„JSONæ•°æ®
    """
    url = f"{address}/api/v1/datasets/{dataset_id}/documents/{document_id}/chunks"
    
    # æ„å»ºè¯·æ±‚ä½“
    data = {
        "content": content
    }
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if important_keywords:
        data["important_keywords"] = important_keywords
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    try:
        response = requests.post(url, json=data, headers=headers)
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return None

def retrieve_chunks(address, api_key, question, dataset_ids=None, document_ids=None, page=1, 
                   page_size=30, similarity_threshold=0.2, vector_similarity_weight=0.3, 
                   top_k=1024, rerank_id=None, keyword=False, highlight=False):
    """
    ä»æŒ‡å®šæ•°æ®é›†ä¸­æ£€ç´¢chunksçš„API
    
    Args:
        address: APIæœåŠ¡å™¨åœ°å€
        api_key: APIå¯†é’¥
        question: ç”¨æˆ·æŸ¥è¯¢æˆ–æŸ¥è¯¢å…³é”®è¯ (å¿…éœ€)
        dataset_ids: è¦æœç´¢çš„æ•°æ®é›†IDåˆ—è¡¨
        document_ids: è¦æœç´¢çš„æ–‡æ¡£IDåˆ—è¡¨
        page: é¡µç ï¼Œé»˜è®¤ä¸º1
        page_size: æ¯é¡µæœ€å¤§chunkæ•°é‡ï¼Œé»˜è®¤ä¸º30
        similarity_threshold: æœ€å°ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œé»˜è®¤ä¸º0.2
        vector_similarity_weight: å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦æƒé‡ï¼Œé»˜è®¤ä¸º0.3
        top_k: å‚ä¸å‘é‡ä½™å¼¦è®¡ç®—çš„chunkæ•°é‡ï¼Œé»˜è®¤ä¸º1024
        rerank_id: é‡æ’åºæ¨¡å‹ID
        keyword: æ˜¯å¦å¯ç”¨åŸºäºå…³é”®è¯çš„åŒ¹é…ï¼Œé»˜è®¤ä¸ºFalse
        highlight: æ˜¯å¦å¯ç”¨åŒ¹é…è¯é«˜äº®æ˜¾ç¤ºï¼Œé»˜è®¤ä¸ºFalse
    
    Returns:
        å“åº”çš„JSONæ•°æ®
    
    Note:
        å¿…é¡»è®¾ç½®dataset_idsæˆ–document_idsä¸­çš„è‡³å°‘ä¸€ä¸ª
    """
    url = f"{address}/api/v1/retrieval"
    
    # æ„å»ºè¯·æ±‚ä½“
    data = {
        "question": question,
        "page": page,
        "page_size": page_size,
        "similarity_threshold": similarity_threshold,
        "vector_similarity_weight": vector_similarity_weight,
        "top_k": top_k,
        "keyword": keyword,
        "highlight": highlight
    }
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if dataset_ids:
        data["dataset_ids"] = dataset_ids
    if document_ids:
        data["document_ids"] = document_ids
    if rerank_id:
        data["rerank_id"] = rerank_id
    
    # éªŒè¯å¿…éœ€å‚æ•°
    if not dataset_ids and not document_ids:
        print("é”™è¯¯: å¿…é¡»æä¾›dataset_idsæˆ–document_idsä¸­çš„è‡³å°‘ä¸€ä¸ª")
        return None
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    try:
        response = requests.post(url, json=data, headers=headers)
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        result = response.json()
        
        # æ£€æŸ¥è¿”å›çš„é”™è¯¯ç 
        if result.get('code') != 0:
            print(f"APIè¿”å›é”™è¯¯ - é”™è¯¯ç : {result.get('code')}, é”™è¯¯ä¿¡æ¯: {result.get('message')}")
            
        return result
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return None

def upload_document(address, api_key, dataset_id, file_path, document_name=None):
    """
    ä¸Šä¼ æ–‡æ¡£åˆ°æŒ‡å®šæ•°æ®é›†
    
    Args:
        address: APIæœåŠ¡å™¨åœ°å€
        api_key: APIå¯†é’¥
        dataset_id: æ•°æ®é›†ID (å¿…éœ€)
        file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„ (å¿…éœ€)
        document_name: æ–‡æ¡£åç§°ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨æ–‡ä»¶å
    
    Returns:
        å“åº”çš„JSONæ•°æ®
    """
    url = f"{address}/api/v1/datasets/{dataset_id}/documents"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {file_path}")
        return None
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    file_size = os.path.getsize(file_path)
    print(f"å‡†å¤‡ä¸Šä¼ æ–‡ä»¶: {file_path} (å¤§å°: {file_size} å­—èŠ‚)")
    
    # è®¾ç½®è¯·æ±‚å¤´ - æ³¨æ„ä¸è¦æ‰‹åŠ¨è®¾ç½®Content-Typeï¼Œè®©requestsè‡ªåŠ¨å¤„ç†
    headers = {
        'Authorization': f'Bearer {api_key}'
    }
    
    try:
        # å‡†å¤‡æ–‡ä»¶ä¸Šä¼ 
        with open(file_path, 'rb') as file:
            # è·å–æ–‡ä»¶å
            filename = document_name if document_name else os.path.basename(file_path)
            
            # æ ¹æ®APIæ–‡æ¡£ï¼Œä½¿ç”¨multipart/form-dataæ ¼å¼ä¸Šä¼ 
            files = {'file': (filename, file, 'application/octet-stream')}
            
            print(f"å¼€å§‹ä¸Šä¼ æ–‡æ¡£: {filename}")
            response = requests.post(url, files=files, headers=headers)
            
            # æ£€æŸ¥HTTPçŠ¶æ€ç 
            if response.status_code == 200:
                result = response.json()
                
                # æ£€æŸ¥APIè¿”å›çš„é”™è¯¯ç 
                if result.get('code') == 0:
                    uploaded_docs = result.get('data', [])
                    if uploaded_docs:
                        doc_info = uploaded_docs[0]
                        print(f"âœ… æ–‡æ¡£ä¸Šä¼ æˆåŠŸ!")
                        print(f"   æ–‡æ¡£ID: {doc_info.get('id')}")
                        print(f"   æ–‡æ¡£åç§°: {doc_info.get('name')}")
                        print(f"   æ–‡ä»¶å¤§å°: {doc_info.get('size')} å­—èŠ‚")
                        print(f"   çŠ¶æ€: {doc_info.get('run', 'UNKNOWN')}")
                    return result
                else:
                    print(f"âŒ APIè¿”å›é”™è¯¯ - é”™è¯¯ç : {result.get('code')}, é”™è¯¯ä¿¡æ¯: {result.get('message')}")
                    return result
            else:
                print(f"âŒ HTTPé”™è¯¯: {response.status_code}")
                try:
                    error_result = response.json()
                    print(f"   é”™è¯¯è¯¦æƒ…: {error_result}")
                    return error_result
                except:
                    print(f"   é”™è¯¯å†…å®¹: {response.text}")
                    return None
                    
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return None
    except Exception as e:
        print(f"âŒ ä¸Šä¼ å¼‚å¸¸: {e}")
        return None

def upload_multiple_documents(address, api_key, dataset_id, file_paths, document_names=None):
    """
    æ‰¹é‡ä¸Šä¼ å¤šä¸ªæ–‡æ¡£åˆ°æŒ‡å®šæ•°æ®é›†
    
    Args:
        address: APIæœåŠ¡å™¨åœ°å€
        api_key: APIå¯†é’¥
        dataset_id: æ•°æ®é›†ID (å¿…éœ€)
        file_paths: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ (å¿…éœ€)
        document_names: æ–‡æ¡£åç§°åˆ—è¡¨ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨æ–‡ä»¶å
    
    Returns:
        å“åº”çš„JSONæ•°æ®
    """
    url = f"{address}/api/v1/datasets/{dataset_id}/documents"
    
    # éªŒè¯è¾“å…¥
    if not file_paths:
        print("é”™è¯¯: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        return None
    
    # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    missing_files = []
    for file_path in file_paths:
        if not os.path.exists(file_path):
            missing_files.append(file_path)
    
    if missing_files:
        print(f"é”™è¯¯: ä»¥ä¸‹æ–‡ä»¶ä¸å­˜åœ¨:")
        for file_path in missing_files:
            print(f"  - {file_path}")
        return None
    
    # è®¡ç®—æ€»æ–‡ä»¶å¤§å°
    total_size = sum(os.path.getsize(fp) for fp in file_paths)
    print(f"å‡†å¤‡æ‰¹é‡ä¸Šä¼  {len(file_paths)} ä¸ªæ–‡ä»¶ (æ€»å¤§å°: {total_size} å­—èŠ‚)")
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'Authorization': f'Bearer {api_key}'
    }
    
    try:
        # å‡†å¤‡å¤šæ–‡ä»¶ä¸Šä¼ 
        files = []
        for i, file_path in enumerate(file_paths):
            # è·å–æ–‡ä»¶å
            if document_names and i < len(document_names):
                filename = document_names[i]
            else:
                filename = os.path.basename(file_path)
            
            # æ‰“å¼€æ–‡ä»¶å¹¶æ·»åŠ åˆ°filesåˆ—è¡¨
            file_obj = open(file_path, 'rb')
            files.append(('file', (filename, file_obj, 'application/octet-stream')))
            print(f"  - æ·»åŠ æ–‡ä»¶: {filename}")
        
        try:
            print("å¼€å§‹æ‰¹é‡ä¸Šä¼ ...")
            response = requests.post(url, files=files, headers=headers)
            
            # æ£€æŸ¥HTTPçŠ¶æ€ç 
            if response.status_code == 200:
                result = response.json()
                
                # æ£€æŸ¥APIè¿”å›çš„é”™è¯¯ç 
                if result.get('code') == 0:
                    uploaded_docs = result.get('data', [])
                    print(f"âœ… æ‰¹é‡ä¸Šä¼ æˆåŠŸ! å…±ä¸Šä¼  {len(uploaded_docs)} ä¸ªæ–‡æ¡£:")
                    for doc in uploaded_docs:
                        print(f"   - æ–‡æ¡£ID: {doc.get('id')}, åç§°: {doc.get('name')}, å¤§å°: {doc.get('size')} å­—èŠ‚")
                    return result
                else:
                    print(f"âŒ APIè¿”å›é”™è¯¯ - é”™è¯¯ç : {result.get('code')}, é”™è¯¯ä¿¡æ¯: {result.get('message')}")
                    return result
            else:
                print(f"âŒ HTTPé”™è¯¯: {response.status_code}")
                try:
                    error_result = response.json()
                    print(f"   é”™è¯¯è¯¦æƒ…: {error_result}")
                    return error_result
                except:
                    print(f"   é”™è¯¯å†…å®¹: {response.text}")
                    return None
        finally:
            # ç¡®ä¿å…³é—­æ‰€æœ‰æ–‡ä»¶
            for _, (_, file_obj, _) in files:
                file_obj.close()
                    
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return None
    except Exception as e:
        print(f"âŒ æ‰¹é‡ä¸Šä¼ å¼‚å¸¸: {e}")
        return None

def create_empty_document(address, api_key, dataset_id, document_name):
    """
    åˆ›å»ºä¸€ä¸ªç©ºç™½æ–‡æ¡£ï¼ˆé€šè¿‡ä¸Šä¼ æœ€å°æ–‡ä»¶å®ç°ï¼‰
    
    Args:
        address: APIæœåŠ¡å™¨åœ°å€
        api_key: APIå¯†é’¥
        dataset_id: æ•°æ®é›†ID (å¿…éœ€)
        document_name: æ–‡æ¡£åç§° (å¿…éœ€)
    
    Returns:
        å“åº”çš„JSONæ•°æ®ï¼ŒåŒ…å«æ–°åˆ›å»ºçš„æ–‡æ¡£ID
    """
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    temp_content = f"# {document_name}\n\nè¿™æ˜¯ä¸€ä¸ªç©ºç™½æ–‡æ¡£ï¼Œå‡†å¤‡é€šè¿‡APIæ·»åŠ å†…å®¹ã€‚"
    
    try:
        # åˆ›å»ºä¸€ä¸ªå…·æœ‰æŒ‡å®šåç§°çš„ä¸´æ—¶æ–‡ä»¶
        # ä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶åï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
        safe_filename = "".join(c for c in document_name if c.isalnum() or c in (' ', '-', '_')).strip()
        if not safe_filename:
            safe_filename = "document"
        
        temp_file_path = f"/tmp/{safe_filename}.txt"
        
        # å†™å…¥æ–‡ä»¶å†…å®¹
        with open(temp_file_path, 'w', encoding='utf-8') as temp_file:
            temp_file.write(temp_content)
        
        # ä½¿ç”¨ä¸Šä¼ APIåˆ›å»ºæ–‡æ¡£
        result = upload_document(
            address=address,
            api_key=api_key,
            dataset_id=dataset_id,
            file_path=temp_file_path,
            document_name=document_name  # ä»ç„¶ä¼ é€’æ–‡æ¡£åç§°å‚æ•°
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        
        return result
    
    except Exception as e:
        print(f"åˆ›å»ºç©ºç™½æ–‡æ¡£å¤±è´¥: {e}")
        # ç¡®ä¿æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        return None

def test_upload_document_with_different_params(address, api_key, dataset_id, file_path, document_name):
    """
    æµ‹è¯•ä¸åŒçš„å‚æ•°åæ¥ä¸Šä¼ æ–‡æ¡£ï¼Œç”¨äºè°ƒè¯•
    """
    url = f"{address}/api/v1/datasets/{dataset_id}/documents"
    headers = {'Authorization': f'Bearer {api_key}'}
    
    # æµ‹è¯•ä¸åŒçš„å‚æ•°åç»„åˆ
    param_combinations = [
        {'name': document_name},
        {'filename': document_name},
        {'document_name': document_name},
        {'title': document_name},
        {'display_name': document_name},
    ]
    
    for i, data in enumerate(param_combinations):
        print(f"\n=== æµ‹è¯•ç»„åˆ {i+1}: {data} ===")
        try:
            with open(file_path, 'rb') as file:
                files = {'file': file}
                response = requests.post(url, files=files, data=data, headers=headers)
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get('code') == 0:
                        doc_info = result.get('data', {})
                        created_name = doc_info.get('name', 'æœªçŸ¥')
                        print(f"âœ… æˆåŠŸ! åˆ›å»ºçš„æ–‡æ¡£å: {created_name}")
                        if document_name in created_name:
                            print(f"ğŸ‰ æ‰¾åˆ°æ­£ç¡®çš„å‚æ•°ç»„åˆ: {data}")
                            return data
                    else:
                        print(f"âŒ APIé”™è¯¯: {result}")
                else:
                    print(f"âŒ HTTPé”™è¯¯: {response.status_code} - {response.text}")
        except Exception as e:
            print(f"âŒ å¼‚å¸¸: {e}")
    
    return None

def find_document_dataset(address, api_key, document_id):
    """
    æŸ¥æ‰¾æ–‡æ¡£æ‰€å±çš„æ•°æ®é›†ID
    
    Args:
        address: APIæœåŠ¡å™¨åœ°å€
        api_key: APIå¯†é’¥  
        document_id: æ–‡æ¡£ID
    
    Returns:
        æ•°æ®é›†IDï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    # è·å–æ‰€æœ‰æ•°æ®é›†
    datasets_result = get_datasets(
        address=address,
        api_key=api_key,
        page=1,
        page_size=100
    )
    
    if not datasets_result or datasets_result.get('code') != 0:
        print("è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥")
        return None
    
    datasets = datasets_result.get('data', [])
    
    # åœ¨æ¯ä¸ªæ•°æ®é›†ä¸­æŸ¥æ‰¾æ–‡æ¡£
    for dataset in datasets:
        dataset_id = dataset.get('id')
        if not dataset_id:
            continue
            
        # æŸ¥æ‰¾æ–‡æ¡£
        docs_result = get_dataset_documents(
            address=address,
            api_key=api_key,
            dataset_id=dataset_id,
            document_id=document_id,  # æŒ‰æ–‡æ¡£IDæŸ¥æ‰¾
            page=1,
            page_size=1
        )
        
        if docs_result and docs_result.get('code') == 0:
            docs = docs_result.get('data', {}).get('docs', [])
            if docs and len(docs) > 0:
                found_doc = docs[0]
                if found_doc.get('id') == document_id:
                    print(f"æ‰¾åˆ°æ–‡æ¡£ {document_id} åœ¨æ•°æ®é›† {dataset.get('name')} (ID: {dataset_id})")
                    return dataset_id
    
    print(f"æœªæ‰¾åˆ°æ–‡æ¡£ {document_id} æ‰€å±çš„æ•°æ®é›†")
    return None

def download_document(address, api_key, dataset_id, document_id, output_path=None):
    """
    ä»æŒ‡å®šæ•°æ®é›†ä¸‹è½½æ–‡æ¡£
    
    Args:
        address: APIæœåŠ¡å™¨åœ°å€
        api_key: APIå¯†é’¥
        dataset_id: æ•°æ®é›†ID (å¿…éœ€)
        document_id: æ–‡æ¡£ID (å¿…éœ€)
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™ä¿å­˜åˆ°å½“å‰ç›®å½•ä¸‹ä»¥document_idå‘½åçš„æ–‡ä»¶
    
    Returns:
        ä¸‹è½½æˆåŠŸè¿”å›æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
    """
    url = f"{address}/api/v1/datasets/{dataset_id}/documents/{document_id}"
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'Authorization': f'Bearer {api_key}'
    }
    
    try:
        response = requests.get(url, headers=headers, stream=True)
        
        # æ£€æŸ¥HTTPçŠ¶æ€ç 
        if response.status_code == 200:
            # ç¡®å®šä¿å­˜è·¯å¾„
            if not output_path:
                # å°è¯•ä»å“åº”å¤´è·å–æ–‡ä»¶å
                content_disposition = response.headers.get('content-disposition', '')
                if 'filename=' in content_disposition:
                    # æå–æ–‡ä»¶å
                    filename = content_disposition.split('filename=')[1].strip('"')
                    output_path = f"./{filename}"
                else:
                    # ä½¿ç”¨document_idä½œä¸ºæ–‡ä»¶å
                    output_path = f"./document_{document_id}.txt"
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = os.path.dirname(output_path)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir)
            
            # ä¿å­˜æ–‡ä»¶
            with open(output_path, 'wb') as file:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        file.write(chunk)
            
            print(f"æ–‡æ¡£ä¸‹è½½æˆåŠŸï¼Œä¿å­˜åˆ°: {output_path}")
            return output_path
        else:
            # å°è¯•è§£æé”™è¯¯å“åº”
            try:
                error_data = response.json()
                error_msg = error_data.get('message', f'HTTPé”™è¯¯: {response.status_code}')
                print(f"æ–‡æ¡£ä¸‹è½½å¤±è´¥: {error_msg}")
            except:
                print(f"æ–‡æ¡£ä¸‹è½½å¤±è´¥: HTTPé”™è¯¯ {response.status_code}")
            return None
            
    except requests.exceptions.RequestException as e:
        print(f"ä¸‹è½½è¯·æ±‚å¤±è´¥: {e}")
        return None
    except Exception as e:
        print(f"æ–‡æ¡£ä¸‹è½½å¼‚å¸¸: {e}")
        return None

def get_supported_file_types():
    """
    è·å–RagFlowæ”¯æŒçš„æ–‡ä»¶ç±»å‹åˆ—è¡¨
    
    Returns:
        dict: åŒ…å«æ”¯æŒçš„æ–‡ä»¶ç±»å‹å’Œæè¿°
    """
    return {
        'text': ['.txt', '.md', '.markdown'],
        'document': ['.pdf', '.doc', '.docx', '.rtf'],
        'spreadsheet': ['.xls', '.xlsx', '.csv'],
        'presentation': ['.ppt', '.pptx'],
        'web': ['.html', '.htm'],
        'code': ['.py', '.js', '.java', '.cpp', '.c', '.go', '.rs', '.php'],
        'data': ['.json', '.xml', '.yaml', '.yml'],
        'other': ['.log', '.ini', '.cfg', '.conf']
    }

def validate_file_type(file_path):
    """
    éªŒè¯æ–‡ä»¶ç±»å‹æ˜¯å¦è¢«RagFlowæ”¯æŒ
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
    
    Returns:
        tuple: (is_supported: bool, file_type: str, extension: str)
    """
    _, ext = os.path.splitext(file_path.lower())
    supported_types = get_supported_file_types()
    
    for file_type, extensions in supported_types.items():
        if ext in extensions:
            return True, file_type, ext
    
    return False, 'unknown', ext

def upload_document_with_validation(address, api_key, dataset_id, file_path, document_name=None, force_upload=False):
    """
    ä¸Šä¼ æ–‡æ¡£åˆ°æŒ‡å®šæ•°æ®é›†ï¼ˆå¸¦æ–‡ä»¶ç±»å‹éªŒè¯ï¼‰
    
    Args:
        address: APIæœåŠ¡å™¨åœ°å€
        api_key: APIå¯†é’¥
        dataset_id: æ•°æ®é›†ID (å¿…éœ€)
        file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„ (å¿…éœ€)
        document_name: æ–‡æ¡£åç§°ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨æ–‡ä»¶å
        force_upload: æ˜¯å¦å¼ºåˆ¶ä¸Šä¼ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹
    
    Returns:
        å“åº”çš„JSONæ•°æ®
    """
    # éªŒè¯æ–‡ä»¶ç±»å‹
    is_supported, file_type, extension = validate_file_type(file_path)
    
    if not is_supported and not force_upload:
        print(f"âš ï¸  è­¦å‘Š: æ–‡ä»¶ç±»å‹ '{extension}' å¯èƒ½ä¸è¢«RagFlowæ”¯æŒ")
        print("æ”¯æŒçš„æ–‡ä»¶ç±»å‹:")
        supported_types = get_supported_file_types()
        for category, extensions in supported_types.items():
            print(f"  {category}: {', '.join(extensions)}")
        print("å¦‚æœè¦å¼ºåˆ¶ä¸Šä¼ ï¼Œè¯·è®¾ç½® force_upload=True")
        return None
    
    if is_supported:
        print(f"âœ… æ–‡ä»¶ç±»å‹éªŒè¯é€šè¿‡: {extension} ({file_type})")
    else:
        print(f"âš ï¸  å¼ºåˆ¶ä¸Šä¼ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {extension}")
    
    # è°ƒç”¨åŸå§‹ä¸Šä¼ å‡½æ•°
    return upload_document(address, api_key, dataset_id, file_path, document_name)

def parse_documents(address, api_key, dataset_id, document_ids):
    """
    è§£ææŒ‡å®šæ•°æ®é›†ä¸­çš„æ–‡æ¡£ï¼Œç”Ÿæˆchunks
    
    Args:
        address: APIæœåŠ¡å™¨åœ°å€
        api_key: APIå¯†é’¥
        dataset_id: æ•°æ®é›†ID
        document_ids: è¦è§£æçš„æ–‡æ¡£IDåˆ—è¡¨
    
    Returns:
        å“åº”çš„JSONæ•°æ®
    """
    url = f"{address}/api/v1/datasets/{dataset_id}/chunks"
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    # æ„å»ºè¯·æ±‚ä½“
    data = {
        "document_ids": document_ids
    }
    
    try:
        response = requests.post(url, json=data, headers=headers)
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"è§£ææ–‡æ¡£è¯·æ±‚å¤±è´¥: {e}")
        return None

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    ADDRESS = "ragflow.iepose.cn"  # æ›¿æ¢ä¸ºå®é™…åœ°å€
    API_KEY = "ragflow-g4ZTU4ZjM4YTAwMTExZWZhZjkyMDI0Mm"  # æ›¿æ¢ä¸ºå®é™…çš„APIå¯†é’¥
    
    # # è°ƒç”¨æ•°æ®é›†API
    # result = get_datasets(
    #     address=ADDRESS,
    #     api_key=API_KEY,
    #     page=1,
    #     page_size=20
    #     # dataset_name="ç¬¬äºŒå¤§è„‘"  # å¯é€‰å‚æ•°
    # )
    
    # if result:
    #     print("æ•°æ®é›†APIè°ƒç”¨æˆåŠŸ:")
    #     print(result)
    # else:
    #     print("æ•°æ®é›†APIè°ƒç”¨å¤±è´¥")
    
    # # è°ƒç”¨æ–‡æ¡£APIç¤ºä¾‹ï¼ˆéœ€è¦å®é™…çš„dataset_idï¼‰
    dataset_id = "c3303d4ee45611ef9b610242ac180003"  # æ›¿æ¢ä¸ºå®é™…çš„æ•°æ®é›†ID
    # docs_result = get_dataset_documents(
    #     address=ADDRESS,
    #     api_key=API_KEY,
    #     dataset_id=dataset_id,
    #     page=1,
    #     page_size=10,
    #     keywords=None  # å¯é€‰å‚æ•°
    #     orderby="update_time"
    # )
    
    # if docs_result:
    #     print("æ–‡æ¡£APIè°ƒç”¨æˆåŠŸ:")
    #     print(docs_result)
    # else:
    #     print("æ–‡æ¡£APIè°ƒç”¨å¤±è´¥")
    
    # åˆ›å»ºchunkç¤ºä¾‹ï¼ˆéœ€è¦å®é™…çš„document_idï¼‰
    document_id = "6e18ef4be45911ef9d800242ac180003"  # æ›¿æ¢ä¸ºå®é™…çš„æ–‡æ¡£ID
    # chunk_result = create_chunk(
    #     address=ADDRESS,
    #     api_key=API_KEY,
    #     dataset_id=dataset_id,
    #     document_id=document_id,
    #     content="openmemoryæ˜¯ä¸€ä¸ªå¼€æºçš„RAGæ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·æ„å»ºè‡ªå·±çš„çŸ¥è¯†åº“ã€‚",
    #     important_keywords=["openmemory", "rag", "å…¨å±€è®°å¿†", "çŸ¥è¯†åº“"]  # å¯é€‰å‚æ•°
    # )
    
    # if chunk_result:
    #     print("chunkåˆ›å»ºæˆåŠŸ:")
    #     print(chunk_result)
    # else:
    #     print("chunkåˆ›å»ºå¤±è´¥")
    
    # æ£€ç´¢chunksç¤ºä¾‹
    # retrieval_result = retrieve_chunks(
    #     address=ADDRESS,
    #     api_key=API_KEY,
    #     question="ä»€ä¹ˆæ˜¯openmemoryï¼Ÿ",
    #     dataset_ids=[dataset_id],  # ä½¿ç”¨æ•°æ®é›†IDåˆ—è¡¨
    #     page=1,
    #     page_size=5,  # å‡å°‘é¡µé¢å¤§å°
    #     similarity_threshold=0.1,  # é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼
    #     vector_similarity_weight=0.5,  # è°ƒæ•´æƒé‡
    #     top_k=50,  # å‡å°‘top_kæ•°é‡
    #     keyword=False,  # å…ˆç¦ç”¨å…³é”®è¯åŒ¹é…
    #     highlight=False  # å…ˆç¦ç”¨é«˜äº®æ˜¾ç¤º
    # )
    
    # if retrieval_result:
    #     print("æ£€ç´¢æˆåŠŸ:")
    #     print(retrieval_result)
    # else:
    #     print("æ£€ç´¢å¤±è´¥")
    
    # # åˆ›å»ºæ•°æ®é›†ç¤ºä¾‹
    # create_result = create_dataset(
    #     address=ADDRESS,
    #     api_key=API_KEY,
    #     name="my_new_dataset",
    #     description="è¿™æ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†",
    #     language="English",
    #     permission="me",
    #     chunk_method="naive"
    # )
    
    # if create_result:
    #     print("æ•°æ®é›†åˆ›å»ºæˆåŠŸ:")
    #     print(create_result)
    # else:
    #     print("æ•°æ®é›†åˆ›å»ºå¤±è´¥")
    
    # # åˆ›å»ºæ•°æ®é›†ç¤ºä¾‹
    # create_result = create_dataset(
    #     address=ADDRESS,
    #     api_key=API_KEY,
    #     name="my_new_dataset",
    #     description="è¿™æ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†",
    #     language="English",
    #     embedding_model="BAAI/bge-zh-v1.5",
    #     permission="me",
    #     chunk_method="naive"
    # )
    
    # if create_result:
    #     print("æ•°æ®é›†åˆ›å»ºæˆåŠŸ:")
    #     print(create_result)
    # else:
    #     print("æ•°æ®é›†åˆ›å»ºå¤±è´¥")
    
    # æµ‹è¯•æ–‡æ¡£ä¸Šä¼ çš„ä¸åŒå‚æ•°å
    # test_result = test_upload_document_with_different_params(
    #     address=ADDRESS,
    #     api_key=API_KEY,
    #     dataset_id=dataset_id,
    #     file_path="path/to/your/file.txt",  # æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
    #     document_name="æµ‹è¯•æ–‡æ¡£"
    # )
    
    # æŸ¥æ‰¾æ–‡æ¡£æ‰€å±æ•°æ®é›†ç¤ºä¾‹
    document_id_to_find = "6e18ef4be45911ef9d800242ac180003"  # æ›¿æ¢ä¸ºå®é™…çš„æ–‡æ¡£ID
    # dataset_id_found = find_document_dataset(
    #     address=ADDRESS,
    #     api_key=API_KEY,
    #     document_id=document_id_to_find
    # )
    
    # if dataset_id_found:
    #     print(f"æ–‡æ¡£ {document_id_to_find} æ‰€å±çš„æ•°æ®é›†ID: {dataset_id_found}")
    # else:
    #     print(f"æœªæ‰¾åˆ°æ–‡æ¡£ {document_id_to_find} æ‰€å±çš„æ•°æ®é›†")
    
    # === æ–°å¢åŠŸèƒ½ä½¿ç”¨ç¤ºä¾‹ ===
    
    # 1. ä¸Šä¼ å•ä¸ªæ–‡æ¡£ç¤ºä¾‹
    # upload_result = upload_document(
    #     address=ADDRESS,
    #     api_key=API_KEY,
    #     dataset_id=dataset_id,
    #     file_path="/path/to/your/document.pdf",  # æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
    #     document_name="æˆ‘çš„æ–‡æ¡£"  # å¯é€‰ï¼Œä¸æä¾›åˆ™ä½¿ç”¨æ–‡ä»¶å
    # )
    # 
    # if upload_result:
    #     print("å•ä¸ªæ–‡æ¡£ä¸Šä¼ æˆåŠŸ!")
    #     print(upload_result)
    
    # 2. å¸¦éªŒè¯çš„ä¸Šä¼ ç¤ºä¾‹
    # upload_with_validation_result = upload_document_with_validation(
    #     address=ADDRESS,
    #     api_key=API_KEY,
    #     dataset_id=dataset_id,
    #     file_path="/path/to/your/document.txt",
    #     document_name="éªŒè¯æ–‡æ¡£",
    #     force_upload=False  # è®¾ç½®ä¸ºTrueå¯å¼ºåˆ¶ä¸Šä¼ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹
    # )
    
    # 3. æ‰¹é‡ä¸Šä¼ æ–‡æ¡£ç¤ºä¾‹
    # file_paths = [
    #     "/path/to/document1.pdf",
    #     "/path/to/document2.txt", 
    #     "/path/to/document3.docx"
    # ]
    # document_names = ["æ–‡æ¡£1", "æ–‡æ¡£2", "æ–‡æ¡£3"]  # å¯é€‰
    # 
    # batch_upload_result = upload_multiple_documents(
    #     address=ADDRESS,
    #     api_key=API_KEY,
    #     dataset_id=dataset_id,
    #     file_paths=file_paths,
    #     document_names=document_names
    # )
    # 
    # if batch_upload_result:
    #     print("æ‰¹é‡ä¸Šä¼ æˆåŠŸ!")
    #     print(batch_upload_result)
    
    # 4. æŸ¥çœ‹æ”¯æŒçš„æ–‡ä»¶ç±»å‹
    # supported_types = get_supported_file_types()
    # print("RagFlowæ”¯æŒçš„æ–‡ä»¶ç±»å‹:")
    # for category, extensions in supported_types.items():
    #     print(f"  {category}: {', '.join(extensions)}")
    
    # 5. éªŒè¯æ–‡ä»¶ç±»å‹ç¤ºä¾‹
    # test_file = "/path/to/test.pdf"
    # is_supported, file_type, extension = validate_file_type(test_file)
    # print(f"æ–‡ä»¶ {test_file}:")
    # print(f"  - æ˜¯å¦æ”¯æŒ: {is_supported}")
    # print(f"  - æ–‡ä»¶ç±»å‹: {file_type}")
    # print(f"  - æ‰©å±•å: {extension}")