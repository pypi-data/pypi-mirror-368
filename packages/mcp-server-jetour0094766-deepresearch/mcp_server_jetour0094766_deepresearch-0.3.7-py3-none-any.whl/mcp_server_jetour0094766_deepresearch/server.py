import os
import asyncio
import argparse
import json
from typing import List, Dict, Any, TypedDict, Optional
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from mcp.server.fastmcp import FastMCP

# LangGraph å’Œ LangChain ç›¸å…³å¯¼å…¥
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_community.tools import DuckDuckGoSearchRun
import logging
import sys

# é…ç½®æ—¥å¿— - è¾“å‡ºåˆ°stderrï¼Œé¿å…å¹²æ‰°MCPé€šä¿¡
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stderr,  # é‡è¦ï¼šè¾“å‡ºåˆ°stderr
    encoding='utf-8'    # æ·»åŠ ç¼–ç è®¾ç½®
)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–MCPæœåŠ¡
mcp = FastMCP("DeepResearch")
USER_AGENT = "deepresearch-app/1.0"

# å…¨å±€å˜é‡
current_llm = None

def load_environment():
    """åŠ è½½ç¯å¢ƒå˜é‡"""
    env_loaded = False
    env_paths = ["./.env", "../.env", ".env"]
    
    for env_path in env_paths:
        if os.path.exists(env_path):
            load_dotenv(env_path)
            env_loaded = True
            logger.info(f"âœ… åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_path}")
            break
    
    if not env_loaded:
        logger.warning("âŒ è­¦å‘Š: æœªæ‰¾åˆ° .env æ–‡ä»¶")
    
    return env_loaded

def create_llm_instance(api_key: str) -> ChatOpenAI:
    """æ ¹æ®æä¾›çš„API keyåˆ›å»ºLLMå®ä¾‹"""
    global current_llm
    current_llm = ChatOpenAI(
        model="openai/gpt-4o-mini",
        openai_api_key=api_key,
        openai_api_base="https://openrouter.ai/api/v1",
        temperature=0.7,
        max_tokens=4000,
        request_timeout=30
    )
    logger.info("âœ… LLMå®ä¾‹åˆ›å»ºæˆåŠŸ")
    return current_llm

def get_current_llm() -> ChatOpenAI:
    """è·å–å½“å‰LLMå®ä¾‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™æŠ›å‡ºå¼‚å¸¸"""
    if current_llm is None:
        raise ValueError("LLMå®ä¾‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨create_llm_instance()")
    return current_llm

# æ•°æ®æ¨¡å‹å®šä¹‰
class WebSearchItem(BaseModel):
    reason: str = Field(description="ä¸ºä»€ä¹ˆè¿™ä¸ªæœç´¢å¯¹æŸ¥è¯¢å¾ˆé‡è¦çš„åŸå› ")
    query: str = Field(description="ç”¨äºç½‘ç»œæœç´¢çš„æœç´¢è¯")

class WebSearchPlan(BaseModel):
    searches: List[WebSearchItem] = Field(description="æ‰§è¡Œçš„ç½‘ç»œæœç´¢åˆ—è¡¨")

class ReportData(BaseModel):
    short_summary: str = Field(description="ç ”ç©¶ç»“æœçš„ç®€çŸ­2-3å¥æ‘˜è¦")
    markdown_report: str = Field(description="æœ€ç»ˆæŠ¥å‘Š")
    follow_up_questions: List[str] = Field(description="å»ºè®®è¿›ä¸€æ­¥ç ”ç©¶çš„ä¸»é¢˜")

# çŠ¶æ€å®šä¹‰
class ResearchState(TypedDict):
    query: str
    search_plan: Optional[WebSearchPlan]
    search_results: List[str]
    final_report: Optional[ReportData]
    messages: List[BaseMessage]

# å·¥å…·å®šä¹‰
@tool
def web_search_tool(query: str) -> str:
    """æ‰§è¡Œç½‘ç»œæœç´¢å¹¶è¿”å›ç»“æœ"""
    search = DuckDuckGoSearchRun()
    try:
        result = search.run(query)
        return result
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
        return f"æœç´¢å¤±è´¥: {str(e)}"

def extract_json_from_response(content: str) -> dict:
    """ä»å“åº”ä¸­æå–JSONå†…å®¹çš„è¾…åŠ©å‡½æ•°"""
    try:
        # å¦‚æœå“åº”åŒ…å« ```json ä»£ç å—ï¼Œæå–å…¶ä¸­çš„å†…å®¹
        if "```json" in content:
            json_start = content.find("```json") + 7
            json_end = content.find("```", json_start)
            json_str = content[json_start:json_end].strip()
        elif "{" in content and "}" in content:
            # æå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
            start = content.find("{")
            end = content.rfind("}") + 1
            json_str = content[start:end]
        else:
            raise ValueError("æœªæ‰¾åˆ°JSONæ ¼å¼æ•°æ®")
        
        return json.loads(json_str)
    except Exception as e:
        logger.error(f"JSONè§£æå¤±è´¥: {e}")
        raise

async def planning_node(state: ResearchState) -> Dict[str, Any]:
    """è§„åˆ’èŠ‚ç‚¹ - ç”Ÿæˆæœç´¢è®¡åˆ’"""
    query = state["query"]
    logger.info(f"ğŸ“‹ å¼€å§‹è§„åˆ’æœç´¢: {query}")
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åŠ©ç†ã€‚ç»™å®šä¸€ä¸ªæŸ¥è¯¢ï¼Œåˆ¶å®šä¸€å¥—ç½‘ç»œæœç´¢è®¡åˆ’æ¥æœ€å¥½åœ°å›ç­”æŸ¥è¯¢ã€‚
    è¯·ç”Ÿæˆ6-8ä¸ªæœç´¢é¡¹ï¼Œæ¯ä¸ªæœç´¢é¡¹åŒ…å«æœç´¢åŸå› å’ŒæŸ¥è¯¢è¯ã€‚
    æœç´¢è¯åº”è¯¥å¤šæ ·åŒ–ï¼Œæ¶µç›–ä¸åŒè§’åº¦å’Œæ–¹é¢ã€‚
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š

    ```json
    {
        "searches": [
            {
                "reason": "æœç´¢åŸå› ",
                "query": "æœç´¢æŸ¥è¯¢è¯"
            }
        ]
    }
    ```"""
    
    try:
        llm = get_current_llm()
        prompt = f"{system_prompt}\n\nè¯·ä¸ºä»¥ä¸‹æŸ¥è¯¢åˆ¶å®šæœç´¢è®¡åˆ’ï¼š{query}"
        
        response = await llm.ainvoke(prompt)
        content = response.content
        logger.info(f"ğŸ“‹ è§„åˆ’ Agent å“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
        
        try:
            parsed_data = extract_json_from_response(content)
            search_items = []
            
            for item in parsed_data.get("searches", []):
                search_items.append(WebSearchItem(
                    reason=item.get("reason", "æœç´¢ç›¸å…³ä¿¡æ¯"),
                    query=item.get("query", query)
                ))
                
            search_plan = WebSearchPlan(searches=search_items)
            logger.info(f"âœ… æˆåŠŸè§£ææœç´¢è®¡åˆ’ï¼Œå…± {len(search_items)} é¡¹")
            
        except Exception:
            logger.warning("âš ï¸ ä½¿ç”¨é»˜è®¤æœç´¢è®¡åˆ’...")
            
            # fallback æœç´¢è®¡åˆ’
            search_terms = [
                f"{query} åŸºç¡€æ¦‚å¿µ", f"{query} åº”ç”¨åœºæ™¯", f"{query} æŠ€æœ¯åŸç†",
                f"{query} å‘å±•å†å²", f"{query} æœªæ¥è¶‹åŠ¿", f"{query} å®é™…æ¡ˆä¾‹",
                f"{query} ä¼˜åŠ¿ç‰¹ç‚¹", f"{query} æŒ‘æˆ˜é—®é¢˜"
            ]
            
            search_items = [
                WebSearchItem(reason=f"äº†è§£{query}çš„ç¬¬{i+1}ä¸ªæ–¹é¢", query=term)
                for i, term in enumerate(search_terms)
            ]
            
            search_plan = WebSearchPlan(searches=search_items)
        
    except Exception as e:
        logger.error(f"âŒ è§„åˆ’èŠ‚ç‚¹å‡ºé”™: {e}")
        # åˆ›å»ºæœ€åŸºæœ¬çš„æœç´¢è®¡åˆ’
        search_plan = WebSearchPlan(searches=[
            WebSearchItem(reason=f"åŸºç¡€äº†è§£{query}", query=query)
        ])
    
    return {"search_plan": search_plan}

async def search_node(state: ResearchState) -> Dict[str, Any]:
    """æœç´¢èŠ‚ç‚¹ - æ‰§è¡Œå¹¶è¡Œæœç´¢"""
    search_plan = state["search_plan"]
    logger.info(f"ğŸ” å¼€å§‹æœç´¢ï¼Œå…± {len(search_plan.searches)} é¡¹")
    
    async def perform_single_search(search_item: WebSearchItem, index: int) -> str:
        """æ‰§è¡Œå•ä¸ªæœç´¢"""
        logger.info(f"  æœç´¢ {index+1}/{len(search_plan.searches)}: {search_item.query}")
        
        try:
            # ç›´æ¥è°ƒç”¨æœç´¢å·¥å…·
            search_result = web_search_tool.run(search_item.query)
            
            # å¦‚æœæœç´¢ç»“æœè¿‡é•¿ï¼Œä½¿ç”¨LLMæ€»ç»“
            if len(search_result) > 1000:
                llm = get_current_llm()
                summary_prompt = f"""è¯·ä¸ºä»¥ä¸‹æœç´¢ç»“æœç”Ÿæˆç®€æ´æ‘˜è¦ï¼š

æœç´¢è¯ï¼š{search_item.query}
æœç´¢åŸå› ï¼š{search_item.reason}

æœç´¢ç»“æœï¼š
{search_result[:1500]}

è¯·æä¾›2-3æ®µæ–‡å­—çš„æ‘˜è¦ï¼Œå°‘äº300å­—ï¼Œæ•æ‰æ ¸å¿ƒè¦ç‚¹ã€‚åªè¿”å›æ‘˜è¦å†…å®¹ã€‚"""

                summary_response = await llm.ainvoke(summary_prompt)
                result_text = summary_response.content.strip()
            else:
                result_text = search_result
            
            logger.info(f"  âœ… æœç´¢ {index+1} å®Œæˆ")
            return f"ã€{search_item.query}ã€‘\n{result_text}"
            
        except Exception as e:
            logger.error(f"  âŒ æœç´¢ {index+1} å¤±è´¥: {e}")
            return f"ã€{search_item.query}ã€‘\næœç´¢å¤±è´¥: {str(e)}"
    
    # é™åˆ¶å¹¶å‘æ•°é‡
    semaphore = asyncio.Semaphore(3)
    
    async def limited_search(search_item: WebSearchItem, index: int) -> str:
        async with semaphore:
            return await perform_single_search(search_item, index)
    
    # æ‰§è¡Œå¹¶è¡Œæœç´¢
    search_tasks = [limited_search(item, i) for i, item in enumerate(search_plan.searches)]
    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
    
    # è¿‡æ»¤æœ‰æ•ˆç»“æœ
    valid_results = []
    for i, result in enumerate(search_results):
        if isinstance(result, str):
            valid_results.append(result)
        else:
            logger.warning(f"  âš ï¸ æœç´¢ {i+1} å¼‚å¸¸: {result}")
    
    logger.info(f"âœ… æœç´¢å®Œæˆï¼Œè·å¾— {len(valid_results)} ä¸ªæœ‰æ•ˆç»“æœ")
    return {"search_results": valid_results}

async def writing_node(state: ResearchState) -> Dict[str, Any]:
    """å†™ä½œèŠ‚ç‚¹ - ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    query = state["query"]
    search_results = state["search_results"]
    logger.info(f"âœï¸ å¼€å§‹å†™ä½œæŠ¥å‘Šï¼ŒåŸºäº {len(search_results)} ä¸ªæœç´¢ç»“æœ")
    
    # ç»„åˆæœç´¢ç»“æœ
    combined_results = "\n\n".join(search_results)
    
    system_prompt = """ä½ æ˜¯ä¸€åä¸“ä¸šçš„ç ”ç©¶æŠ¥å‘Šæ’°å†™ä¸“å®¶ã€‚
è¯·åŸºäºæä¾›çš„æœç´¢ç»“æœæ’°å†™ä¸€ä»½è¯¦ç»†ã€ç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Šã€‚

æŠ¥å‘Šè¦æ±‚ï¼š
1. ä½¿ç”¨ä¸­æ–‡æ’°å†™
2. Markdownæ ¼å¼
3. ç»“æ„æ¸…æ™°ï¼ŒåŒ…å«ï¼šæ ‡é¢˜ã€æ‘˜è¦ã€ç›®å½•ã€æ­£æ–‡å„ç« èŠ‚ã€ç»“è®º
4. å†…å®¹è¯¦å®ï¼Œè‡³å°‘1500å­—
5. åŸºäºæœç´¢ç»“æœï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
6. åŒ…å«é€‚å½“çš„å°æ ‡é¢˜å’Œæ®µè½ç»“æ„

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š

```json
{
    "short_summary": "2-3å¥è¯çš„ç®€çŸ­æ‘˜è¦",
    "markdown_report": "å®Œæ•´çš„markdownæ ¼å¼æŠ¥å‘Šå†…å®¹",
    "follow_up_questions": ["åç»­ç ”ç©¶é—®é¢˜1", "åç»­ç ”ç©¶é—®é¢˜2", "åç»­ç ”ç©¶é—®é¢˜3"]
}
```"""
    
    try:
        llm = get_current_llm()
        prompt = f"""{system_prompt}

åŸå§‹æŸ¥è¯¢: {query}

æœç´¢ç»“æœæ‘˜è¦:
{combined_results[:4000]}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯æ’°å†™è¯¦ç»†çš„ç ”ç©¶æŠ¥å‘Šã€‚"""
        
        response = await llm.ainvoke(prompt)
        content = response.content
        logger.info(f"âœï¸ å†™ä½œ Agent å“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
        
        try:
            parsed_data = extract_json_from_response(content)
            
            report = ReportData(
                short_summary=parsed_data.get("short_summary", f"å…³äº{query}çš„ç ”ç©¶æŠ¥å‘Š"),
                markdown_report=parsed_data.get("markdown_report", content),
                follow_up_questions=parsed_data.get("follow_up_questions", [
                    f"{query}çš„æ·±å…¥åº”ç”¨",
                    f"{query}çš„æŠ€æœ¯ç»†èŠ‚", 
                    f"{query}çš„æœªæ¥å‘å±•"
                ])
            )
            logger.info("âœ… æˆåŠŸè§£ææŠ¥å‘ŠJSON")
            
        except Exception:
            logger.warning("âš ï¸ ä½¿ç”¨åŸå§‹å†…å®¹ä½œä¸ºæŠ¥å‘Š...")
            
            report = ReportData(
                short_summary=f"å…³äº{query}çš„ç ”ç©¶æŠ¥å‘Šå·²å®Œæˆï¼ŒåŸºäº{len(search_results)}ä¸ªæœç´¢ç»“æœç”Ÿæˆã€‚",
                markdown_report=content,
                follow_up_questions=[
                    f"{query}çš„æ·±å…¥åº”ç”¨",
                    f"{query}çš„æŠ€æœ¯ç»†èŠ‚", 
                    f"{query}çš„æœªæ¥å‘å±•"
                ]
            )
        
    except Exception as e:
        logger.error(f"âŒ å†™ä½œèŠ‚ç‚¹å‡ºé”™: {e}")
        report = ReportData(
            short_summary=f"å…³äº{query}çš„ç ”ç©¶æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜",
            markdown_report=f"# {query} ç ”ç©¶æŠ¥å‘Š\n\næŠ±æ­‰ï¼ŒæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­é‡åˆ°æŠ€æœ¯é—®é¢˜ã€‚é”™è¯¯ä¿¡æ¯ï¼š{str(e)}",
            follow_up_questions=[]
        )
    
    logger.info("âœ… æŠ¥å‘Šå†™ä½œå®Œæˆ")
    return {"final_report": report}

def create_deepresearch_workflow():
    """åˆ›å»ºæ·±åº¦ç ”ç©¶å·¥ä½œæµ"""
    logger.info("ğŸ”§ æ„å»ºå·¥ä½œæµ...")
    workflow = StateGraph(ResearchState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("planning", planning_node)
    workflow.add_node("search", search_node) 
    workflow.add_node("writing", writing_node)
    
    # å®šä¹‰è¾¹
    workflow.add_edge("planning", "search")
    workflow.add_edge("search", "writing")
    workflow.add_edge("writing", END)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("planning")
    
    # ç¼–è¯‘å·¥ä½œæµ
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    
    logger.info("âœ… å·¥ä½œæµæ„å»ºå®Œæˆ")
    return app

@mcp.tool()
async def deepresearch(query: str, api_key: str) -> dict:
    """
    è¾“å…¥ä¸€ä¸ªç ”ç©¶ä¸»é¢˜ï¼Œè‡ªåŠ¨å®Œæˆæœç´¢è§„åˆ’ã€æœç´¢ã€å†™æŠ¥å‘Šã€‚
    è¿”å›åŒ…å«ç ”ç©¶æŠ¥å‘Šçš„å­—å…¸
    
    Args:
        query: ç ”ç©¶ä¸»é¢˜
        api_key: OpenAI API Key
    
    Returns:
        dict: åŒ…å«æ‘˜è¦ã€å®Œæ•´æŠ¥å‘Šå’Œåç»­é—®é¢˜çš„å­—å…¸
    """
    logger.info(f"ğŸš€ å¼€å§‹æ·±åº¦ç ”ç©¶: {query}")
    
    try:
        # åˆ›å»ºLLMå®ä¾‹
        create_llm_instance(api_key)
        
        # åˆ›å»ºå·¥ä½œæµ
        app = create_deepresearch_workflow()
        
        # åˆå§‹çŠ¶æ€
        initial_state = {
            "query": query,
            "search_plan": None,
            "search_results": [],
            "final_report": None,
            "messages": []
        }
        
        # æ‰§è¡Œå·¥ä½œæµ
        config = {"configurable": {"thread_id": f"deepresearch-{abs(hash(query)) % 10000}"}}
        final_state = await app.ainvoke(initial_state, config)
        
        report = final_state["final_report"]
        logger.info("ğŸ‰ æ·±åº¦ç ”ç©¶å®Œæˆ!")
        
        # è¿”å›å­—å…¸æ ¼å¼ï¼Œä¾¿äºMCP Inspectoræ˜¾ç¤º
        return {
            "success": True,
            "short_summary": report.short_summary,
            "markdown_report": report.markdown_report,
            "follow_up_questions": report.follow_up_questions
        }
        
    except Exception as e:
        logger.error(f"âŒ æ·±åº¦ç ”ç©¶å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
            "short_summary": f"å…³äº{query}çš„ç ”ç©¶è¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯",
            "markdown_report": f"# {query} ç ”ç©¶æŠ¥å‘Š\n\nç ”ç©¶è¿‡ç¨‹ä¸­é‡åˆ°æŠ€æœ¯é—®é¢˜: {str(e)}",
            "follow_up_questions": []
        }

async def test_connection(api_key: str) -> bool:
    """æµ‹è¯•LLMè¿æ¥"""
    try:
        create_llm_instance(api_key)
        llm = get_current_llm()
        response = await llm.ainvoke("Hello, please reply briefly to confirm connection.")
        logger.info("âœ… LLMè¿æ¥æµ‹è¯•æˆåŠŸ!")
        return True
    except Exception as e:
        logger.error(f"âŒ LLMè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False

def run_server(api_key: str):
    """è¿è¡ŒMCPæœåŠ¡å™¨ - åŒæ­¥ç‰ˆæœ¬"""
    logger.info("ğŸš€ å¯åŠ¨DeepResearch MCPæœåŠ¡å™¨...")
    logger.info("ç­‰å¾…å®¢æˆ·ç«¯è¿æ¥...")
    
    # è®¾ç½®å…¨å±€API Keyï¼ˆå¦‚æœéœ€è¦é»˜è®¤å€¼ï¼‰
    global current_llm
    try:
        create_llm_instance(api_key)
        logger.info("âœ… é»˜è®¤LLMå®ä¾‹å·²åˆ›å»º")
    except Exception as e:
        logger.warning(f"âš ï¸ é»˜è®¤LLMå®ä¾‹åˆ›å»ºå¤±è´¥: {e}")
    
    # è¿è¡ŒMCPæœåŠ¡å™¨
    mcp.run()

async def run_test(api_key: str):
    """è¿è¡Œæµ‹è¯•"""
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•...")
    
    # æµ‹è¯•è¿æ¥
    if not await test_connection(api_key):
        return
    
    # æµ‹è¯•ç ”ç©¶åŠŸèƒ½
    query = "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨"
    result = await deepresearch(query, api_key)
    
    # è¾“å‡ºç»“æœ
    print("\n" + "="*50, file=sys.stderr)
    print("=== æµ‹è¯•ç»“æœ ===", file=sys.stderr)
    print(f"æˆåŠŸ: {result.get('success')}", file=sys.stderr)
    print(f"æ‘˜è¦: {result.get('short_summary', '')}", file=sys.stderr)
    
    if result.get('success'):
        print("\n=== å®Œæ•´æŠ¥å‘Š ===", file=sys.stderr)
        print(result.get('markdown_report', '')[:500] + "...", file=sys.stderr)
        
        print("\n=== åç»­ç ”ç©¶å»ºè®® ===", file=sys.stderr)
        for i, question in enumerate(result.get('follow_up_questions', []), 1):
            print(f"{i}. {question}", file=sys.stderr)
    
    logger.info("ğŸ‰ æµ‹è¯•å®Œæˆ!")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="DeepResearch MCP Server")
    parser.add_argument("--api_key", type=str, required=True, help="OpenAI API Key")
    parser.add_argument("--test", action="store_true", help="è¿è¡Œæµ‹è¯•")
    args = parser.parse_args()
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_environment()
    
    try:
        if args.test:
            # è¿è¡Œæµ‹è¯•ï¼ˆå¼‚æ­¥ï¼‰
            asyncio.run(run_test(args.api_key))
        else:
            # å¯åŠ¨MCPæœåŠ¡å™¨ï¼ˆåŒæ­¥ï¼‰
            run_server(args.api_key)
            
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯ä¸­
    try:
        # å¦‚æœå·²ç»åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œç›´æ¥è¿è¡Œ
        loop = asyncio.get_running_loop()
        logger.warning("âš ï¸ æ£€æµ‹åˆ°æ­£åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œä½¿ç”¨åŒæ­¥æ¨¡å¼")
        main()
    except RuntimeError:
        # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œæ­£å¸¸å¯åŠ¨
        main()