"""asl-signs dataset for Google - Isolated Sign Language Recognition (kaggle)"""

import csv
import functools
from os import path

import numpy as np
import pyarrow.parquet as pq
import tensorflow_datasets as tfds
from pose_format import Pose, PoseHeader
from pose_format.numpy import NumPyPoseBody
from pose_format.pose_header import PoseHeaderDimensions
from tensorflow.io.gfile import GFile

from sign_language_datasets.utils.features import PoseFeature
from ..warning import dataset_warning
from ...datasets.config import SignDatasetConfig

_DESCRIPTION = """
The Isolated Sign Language Recognition corpus (version 1.0.0) is a collection of hand and facial landmarks generated by Mediapipe version 0.9.0.1 on ~100k videos of isolated signs performed by 21 Deaf signers from a 250-sign vocabulary.
"""

_CITATION = """
@misc{asl-signs,
    author = {Ashley Chow, Glenn Cameron, Mark Sherwood, Phil Culliton, Sam Sepah, Sohier Dane, Thad Starner},
    title = {Google - Isolated Sign Language Recognition},
    publisher = {Kaggle},
    year = {2023},
    url = {https://kaggle.com/competitions/asl-signs}
}
"""

_POSE_HEADERS = {"holistic": path.join(path.dirname(path.realpath(__file__)), "holistic.poseheader")}

_KNOWN_SPLITS = {"1.0.0-uzh": path.join(path.dirname(path.realpath(__file__)), "splits/1.0.0-uzh")}


@functools.lru_cache()
def get_pose_header():
    from pose_format.utils.holistic import holistic_components

    # create a new Pose instance
    # FIXME: raw video resolution and fps as well as estimated keypoint confidence are unknown
    # resolution and fps are hardcoded to the default values for a Pixel 4a smartphone at the moment
    width = 1080
    height = 720
    dimensions = PoseHeaderDimensions(width=width, height=height, depth=1000)
    return PoseHeader(version=0.1, dimensions=dimensions, components=holistic_components("XYZC")[:-1])  # no world landmarks


class ASLSigns(tfds.core.GeneratorBasedBuilder):
    """DatasetBuilder for asl-signs dataset."""

    VERSION = tfds.core.Version("1.0.0")
    RELEASE_NOTES = {"1.0.0": "Initial release."}

    BUILDER_CONFIGS = [SignDatasetConfig(name="default", include_pose="holistic")]

    def _info(self) -> tfds.core.DatasetInfo:
        """Returns the dataset metadata."""

        features = {"id": tfds.features.Text(), "text": tfds.features.Text(), "signer_id": tfds.features.Text()}

        if self._builder_config.include_pose == "holistic":
            pose_header_path = _POSE_HEADERS[self._builder_config.include_pose]
            stride = 1 if self._builder_config.fps is None else 30 / self._builder_config.fps
            features["pose"] = PoseFeature(shape=(None, 1, 543, 3), header_path=pose_header_path, stride=stride)

        return tfds.core.DatasetInfo(
            builder=self,
            description=_DESCRIPTION,
            features=tfds.features.FeaturesDict(features),
            homepage="https://www.kaggle.com/competitions/asl-signs/overview",
            supervised_keys=None,
            citation=_CITATION,
        )

    def _load_split_ids(self, split: str):
        split_dir = _KNOWN_SPLITS[self._builder_config.extra["split"]]

        with open(path.join(split_dir, f"{split}.txt")) as f:
            ids = []
            for line in f:
                id = line.rstrip("\n")
                ids.append(id)

        return ids

    def _split_generators(self, dl_manager: tfds.download.DownloadManager):
        """Returns SplitGenerators."""
        dataset_warning(self)

        # Note: This function requires the Kaggle CLI tool. Read the installation guide at https://www.kaggle.com/docs/api
        archive = dl_manager.download_kaggle_data("asl-signs")

        if "split" in self._builder_config.extra:
            train_args = {"archive_path": archive, "ids": self._load_split_ids("train")}
            val_args = {"archive_path": archive, "ids": self._load_split_ids("val")}
            test_args = {"archive_path": archive, "ids": self._load_split_ids("test")}

            return [
                tfds.core.SplitGenerator(name=tfds.Split.TRAIN, gen_kwargs=train_args),
                tfds.core.SplitGenerator(name=tfds.Split.VALIDATION, gen_kwargs=val_args),
                tfds.core.SplitGenerator(name=tfds.Split.TEST, gen_kwargs=test_args),
            ]
        else:
            return [tfds.core.SplitGenerator(name=tfds.Split.TRAIN, gen_kwargs={"archive_path": archive})]

    def _generate_examples(self, archive_path: str, ids: list = []):
        """Yields examples."""

        with GFile(path.join(archive_path, "train.csv"), "r") as csv_file:
            csv_data = csv.reader(csv_file, delimiter=",")
            next(csv_data)  # Ignore the header

            for i, row in enumerate(csv_data):
                datum = {"id": row[2], "text": row[3], "signer_id": row[1]}

                if len(ids) > 0 and (datum["id"] not in ids):
                    continue

                if self.builder_config.include_pose is not None:
                    if self.builder_config.include_pose == "holistic":
                        # get data from parquet file
                        parquet_path = path.join(archive_path, row[0])
                        pose_df = pq.read_table(parquet_path).to_pandas()
                        num_frames = len(pose_df["frame"].drop_duplicates())
                        dimensions = ["x", "y", "z"]

                        # reorder keypoints
                        FACE = np.arange(0, 468).tolist()
                        LHAND = np.arange(468, 489).tolist()
                        POSE = np.arange(489, 522).tolist()
                        RHAND = np.arange(522, 543).tolist()
                        # rearrage the order of keypoints
                        points = POSE + FACE + LHAND + RHAND
                        pose_data = pose_df[dimensions].to_numpy().reshape(num_frames, -1, len(dimensions))
                        pose_data = pose_data[:, points, :]

                        header = get_pose_header()

                        # add the person dimension
                        pose_data = np.expand_dims(pose_data, 1)
                        # TODO: revert the normalization done in the original dataset
                        # pose_data = pose_data * np.array([width, height, 1.0])
                        body = NumPyPoseBody(
                            fps=30, data=pose_data, confidence=np.ones(shape=(pose_data.shape[0], 1, header.total_points()))
                        )
                        pose = Pose(header, body)
                        datum["pose"] = pose

                yield datum["id"], datum
