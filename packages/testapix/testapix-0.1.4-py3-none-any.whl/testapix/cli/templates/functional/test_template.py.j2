"""
Functional Tests for {{ api_name }}

Generated by TestAPIX on {{ timestamp }}

This test suite demonstrates comprehensive functional testing patterns for APIs.
Functional tests verify that your API behaves correctly from a user's perspective,
focusing on the "what" rather than the "how" of your implementation.

Key Testing Principles Demonstrated:

1. **Test Independence**: Each test can run in isolation without depending on
   other tests. This makes debugging easier and tests more reliable.

2. **Realistic Data**: We use TestAPIX's data generators to create realistic test
   data that mimics actual user input, helping uncover edge cases.

3. **Comprehensive Coverage**: We test not just the happy path, but also error
   cases, edge cases, and boundary conditions.

4. **Clear Assertions**: Using TestAPIX's fluent assertions, tests read like
   specifications, making it clear what behavior we expect.

5. **Automatic Cleanup**: Test data is cleaned up automatically, preventing
   test pollution and ensuring a clean state for each test run.

Remember: Good tests are as important as good code. They serve as living
documentation, catch regressions, and give confidence when refactoring.
"""

import pytest
from typing import Dict, Any, List, Optional, Tuple
import asyncio
from datetime import datetime
from pydantic import BaseModel

from testapix import APIClient, assert_that
from testapix.core.exceptions import RequestError, ResponseValidationError
from testapix.generators import BaseGenerator


# Example Pydantic models for schema validation
class {{ python_name|title }}Response(BaseModel):
    """Pydantic model representing the expected {{ python_name }} response structure."""
    id: str
    name: str
    email: str
    status: str
    created_at: str
    updated_at: Optional[str] = None


class {{ python_name|title }}ListResponse(BaseModel):
    """Pydantic model for paginated {{ python_name }} list responses."""
    data: List[{{ python_name|title }}Response]
    pagination: Dict[str, Any]


class ErrorResponse(BaseModel):
    """Pydantic model for error responses."""
    error: str
    message: Optional[str] = None
    details: Optional[Dict[str, Any]] = None


class Test{{ python_name|title }}BasicOperations:
    """
    Basic CRUD (Create, Read, Update, Delete) operation tests.

    These tests form the foundation of API testing, verifying that basic
    operations work correctly. Even if your API doesn't follow RESTful
    conventions exactly, the patterns here apply to most APIs.
    """

    @pytest.fixture(autouse=True)
    def setup(self, api_client: APIClient, data_generator: BaseGenerator):
        """
        Automatic setup for each test method.

        This fixture runs before each test, providing:
        - An authenticated API client configured for the test environment
        - A data generator for creating realistic test data
        - A list to track created resources for cleanup

        Using autouse=True means we don't need to explicitly request this
        fixture in each test - it runs automatically.
        """
        self.client = api_client
        self.generator = data_generator
        self.created_resources: List[Tuple[str, str]] = []  # (resource_type, id)

    async def test_create_{{ python_name }}_with_valid_data(self):
        """
        Test creating a resource with valid, realistic data.

        This test verifies:
        1. The API accepts well-formed requests
        2. Required fields are properly processed
        3. The response includes expected fields
        4. The resource can be retrieved after creation

        This is often the first test to write for a new API endpoint.
        """
        # Generate realistic test data
        # The generator creates complete, valid data with proper formats
        test_data = {
            "name": self.generator.fake.name(),
            "email": self.generator.fake.email(),
            "description": self.generator.fake.text(max_nb_chars=200),
            "phone": self.generator.fake.phone(),
            "status": "active",
            "metadata": {
                "source": "api_test",
                "test_run": datetime.now().isoformat()
            }
        }

        # Make the API request
        response = await self.client.post("/{{ python_name }}s", json=test_data)

        # Verify the response using TestAPIX's fluent assertions
        # Each assertion provides clear error messages if it fails
        assert_that(response) \
            .has_status(201) \
            .has_header("location") \
            .has_json_path("id") \
            .has_json_path("name", test_data["name"]) \
            .has_json_path("email", test_data["email"]) \
            .has_json_path("status", "active") \
            .response_time_less_than(2.0) \
            .matches_schema({{ python_name|title }}Response)

        # Extract the created resource ID for cleanup
        resource_id = response.json_path("id")
        self.created_resources.append(("{{ python_name }}", resource_id))

        # Verify the resource was actually created by retrieving it
        # This catches issues where the API returns success but doesn't
        # actually persist the data
        get_response = await self.client.get(f"/{{ python_name }}s/{resource_id}")

        assert_that(get_response) \
            .has_status(200) \
            .has_json_path("id", resource_id) \
            .has_json_path("email", test_data["email"])

    async def test_get_{{ python_name }}_by_id(self):
        """
        Test retrieving a specific resource by ID.

        This test follows the pattern of creating a known resource first,
        then verifying we can retrieve it. This ensures we're testing with
        real data rather than assuming certain resources exist.
        """
        # First, create a resource to retrieve
        create_data = {
            "name": "Test {{ python_name|title }}",
            "email": self.generator.fake.email(),
            "status": "active"
        }

        create_response = await self.client.post("/{{ python_name }}s", json=create_data)
        assert_that(create_response).has_status(201)

        resource_id = create_response.json_path("id")
        self.created_resources.append(("{{ python_name }}", resource_id))

        # Now test retrieval
        response = await self.client.get(f"/{{ python_name }}s/{resource_id}")

        # Verify the response contains all expected fields
        assert_that(response) \
            .has_status(200) \
            .has_json_path("id", resource_id) \
            .has_json_path("name", create_data["name"]) \
            .has_json_path("email", create_data["email"]) \
            .has_json_path("status", "active")

        # Verify response includes standard metadata fields
        # These fields help with debugging and audit trails
        assert_that(response) \
            .has_json_path("created_at") \
            .has_json_path("updated_at")

    async def test_update_{{ python_name }}_partial(self):
        """
        Test partial update (PATCH) of a resource.

        Many APIs support partial updates where you only send the fields
        you want to change. This test verifies that behavior works correctly
        and that unchanged fields remain intact.
        """
        # Create a resource to update
        original_data = {
            "name": "Original Name",
            "email": self.generator.fake.email(),
            "phone": self.generator.fake.phone(),
            "description": "Original description",
            "status": "active"
        }

        create_response = await self.client.post("/{{ python_name }}s", json=original_data)
        resource_id = create_response.json_path("id")
        self.created_resources.append(("{{ python_name }}", resource_id))

        # Update only specific fields
        update_data = {
            "name": "Updated Name",
            "description": "This description has been updated"
        }

        response = await self.client.patch(
            f"/{{ python_name }}s/{resource_id}",
            json=update_data
        )

        # Verify the update was successful
        assert_that(response) \
            .has_status(200) \
            .has_json_path("id", resource_id) \
            .has_json_path("name", update_data["name"]) \
            .has_json_path("description", update_data["description"])

        # Verify unchanged fields remain intact
        # This is important - partial updates shouldn't affect other fields
        assert_that(response) \
            .has_json_path("email", original_data["email"]) \
            .has_json_path("phone", original_data["phone"]) \
            .has_json_path("status", original_data["status"])

    async def test_delete_{{ python_name }}_success(self):
        """
        Test successful resource deletion.

        This test verifies both that deletion succeeds and that the
        resource is actually removed (not just marked as deleted).
        """
        # Create a resource to delete
        create_response = await self.client.post("/{{ python_name }}s", json={
            "name": "To Be Deleted",
            "email": self.generator.fake.email(),
            "status": "active"
        })

        resource_id = create_response.json_path("id")
        # Note: We don't add to created_resources since we're deleting it

        # Delete the resource
        delete_response = await self.client.delete(f"/{{ python_name }}s/{resource_id}")

        # Most APIs return either 204 (No Content) or 200 (OK) for successful deletion
        assert_that(delete_response).has_status_in([200, 204])

        # Verify the resource is actually deleted by trying to retrieve it
        get_response = await self.client.get(f"/{{ python_name }}s/{resource_id}")
        assert_that(get_response).has_status(404)

    async def test_list_{{ python_name }}s_with_pagination(self):
        """
        Test listing resources with pagination support.

        Pagination is crucial for APIs that return collections. This test
        verifies that pagination parameters work correctly and that the
        response includes necessary pagination metadata.
        """
        # Create multiple resources to ensure we have data to paginate
        for i in range(5):
            create_response = await self.client.post("/{{ python_name }}s", json={
                "name": f"Test {{ python_name|title }} {i}",
                "email": self.generator.fake.email(),
                "status": "active"
            })
            if create_response.status_code == 201:
                self.created_resources.append(
                    ("{{ python_name }}", create_response.json_path("id"))
                )

        # Test first page
        response = await self.client.get("/{{ python_name }}s", params={
            "page": 1,
            "per_page": 2
        })

        assert_that(response) \
            .has_status(200) \
            .has_json_path("data") \
            .has_json_path("pagination") \
            .matches_schema({{ python_name|title }}ListResponse)

        # Verify pagination metadata
        assert_that(response) \
            .has_json_path("pagination.page", 1) \
            .has_json_path("pagination.per_page", 2) \
            .has_json_path("pagination.total")

        # Verify we got the requested number of items (or less if total < per_page)
        data = response.json_path("data")
        assert isinstance(data, list), "Data should be a list"
        assert len(data) <= 2, "Should return at most per_page items"

    @pytest.fixture(autouse=True)
    async def cleanup_resources(self):
        """Clean up created resources after each test."""
        yield

        # Clean up in reverse order (most recently created first)
        for resource_type, resource_id in reversed(self.created_resources):
            try:
                await self.client.delete(f"/{resource_type}s/{resource_id}")
            except Exception:
                # Ignore cleanup errors - resource might not exist or be deletable
                pass

        # Clear the list for next test
        self.created_resources.clear()


class Test{{ python_name|title }}ErrorHandling:
    """
    Test error handling and validation.

    These tests are crucial for ensuring your API handles invalid input
    gracefully and returns appropriate error messages. Good error handling
    makes APIs easier to use and debug.
    """

    @pytest.fixture(autouse=True)
    def setup(self, api_client: APIClient, data_generator: BaseGenerator):
        """Setup for each test method."""
        self.client = api_client
        self.generator = data_generator

    async def test_create_with_missing_required_fields(self):
        """
        Test that missing required fields are properly validated.

        APIs should return clear error messages indicating which fields
        are missing, not generic "bad request" errors.
        """
        # Send request with missing required fields
        incomplete_data = {
            "description": "This is missing required fields like name and email"
        }

        response = await self.client.post("/{{ python_name }}s", json=incomplete_data)

        # Should return 400 Bad Request
        assert_that(response) \
            .has_status(400) \
            .has_error_message_containing("required")

        # Good APIs return structured error information
        if response.has_json_path("errors"):
            errors = response.json_path("errors")
            # Verify specific fields are mentioned in errors
            error_fields = [error.get("field") for error in errors
                          if isinstance(errors, list) and isinstance(error, dict)]
            assert "name" in error_fields or "email" in error_fields, \
                "Error should indicate which required fields are missing"

    async def test_create_with_invalid_email_format(self):
        """
        Test email validation.

        This is a common validation that many APIs implement. The test
        verifies that the API properly validates email format and returns
        helpful error messages.
        """
        invalid_data = {
            "name": self.generator.fake.name(),
            "email": "not-a-valid-email",  # Invalid email format
            "status": "active"
        }

        response = await self.client.post("/{{ python_name }}s", json=invalid_data)

        assert_that(response) \
            .has_status(400) \
            .has_error_message_containing("email")

        # The error message should be helpful
        # Good: "Email must be a valid email address"
        # Bad: "Validation failed"

    @pytest.mark.parametrize("field,invalid_value,expected_error", [
        ("email", "", "required"),
        ("email", "user@", "invalid"),
        ("email", "@example.com", "invalid"),
        ("name", "", "required"),
        ("name", "a" * 256, "too long"),  # Assuming 255 char limit
        ("status", "invalid_status", "invalid"),
        ("phone", "not-a-phone", "invalid"),
    ])
    async def test_field_validation(self, field: str, invalid_value: Any, expected_error: str):
        """
        Parameterized test for various field validations.

        Using pytest.mark.parametrize allows us to test multiple validation
        scenarios with the same test logic. This keeps tests DRY (Don't
        Repeat Yourself) while maintaining clarity.

        Each parameter set is run as a separate test, making it easy to
        identify which specific validation is failing.
        """
        # Start with valid data
        test_data = {
            "name": self.generator.fake.name(),
            "email": self.generator.fake.email(),
            "phone": self.generator.fake.phone(),
            "status": "active"
        }

        # Override the field being tested
        test_data[field] = invalid_value

        response = await self.client.post("/{{ python_name }}s", json=test_data)

        assert_that(response) \
            .has_status(400) \
            .has_error_message_containing(field) \
            .has_error_message_containing(expected_error)

    async def test_get_nonexistent_resource(self):
        """
        Test retrieving a resource that doesn't exist.

        APIs should return 404 Not Found with a clear message, not 500
        errors or empty responses.
        """
        # Use a UUID that's valid but doesn't exist
        nonexistent_id = self.generator.fake.uuid()

        response = await self.client.get(f"/{{ python_name }}s/{nonexistent_id}")

        assert_that(response) \
            .has_status(404) \
            .has_error_message_containing("not found")

    async def test_update_nonexistent_resource(self):
        """
        Test updating a resource that doesn't exist.

        This should return 404, not create a new resource (unless your
        API specifically supports "upsert" operations).
        """
        nonexistent_id = self.generator.fake.uuid()
        update_data = {"name": "Updated Name"}

        response = await self.client.patch(
            f"/{{ python_name }}s/{nonexistent_id}",
            json=update_data
        )

        assert_that(response).has_status(404)

    async def test_invalid_content_type(self):
        """
        Test sending request with wrong Content-Type header.

        APIs should validate content types and return appropriate errors
        when clients send data in unexpected formats.
        """
        response = await self.client.post(
            "/{{ python_name }}s",
            data="not json data",
            headers={"Content-Type": "text/plain"}
        )

        # Should return 415 Unsupported Media Type or 400 Bad Request
        assert_that(response).has_status_in([400, 415])


{% if has_auth %}
class Test{{ python_name|title }}Authentication:
    """
    Test authentication and authorization.

    Security is crucial for APIs. These tests verify that:
    1. Unauthenticated requests are properly rejected
    2. Invalid credentials are handled appropriately
    3. Authenticated requests work correctly
    """

    async def test_unauthenticated_request_rejected(self, unauthenticated_client: APIClient):
        """
        Test that protected endpoints require authentication.

        Using the unauthenticated_client fixture, we verify that the API
        properly rejects requests without credentials.
        """
        response = await unauthenticated_client.get("/{{ python_name }}s")

        # Should return 401 Unauthorized
        assert_that(response) \
            .has_status(401) \
            .has_header("www-authenticate")  # Standard auth challenge header

    async def test_invalid_token_rejected(self, api_client: APIClient):
        """
        Test that invalid authentication tokens are rejected.

        This helps ensure the API properly validates tokens and doesn't
        have authentication bypass vulnerabilities.
        """
        # Override auth with invalid token
        {% if auth_type == 'bearer' %}
        api_client.set_default_header("Authorization", "Bearer invalid-token-12345")
        {% elif auth_type == 'api_key' %}
        api_client.set_default_header("X-API-Key", "invalid-api-key-12345")
        {% endif %}

        response = await api_client.get("/{{ python_name }}s")

        assert_that(response) \
            .has_status(401) \
            .has_error_message_containing("invalid")

    async def test_authenticated_request_succeeds(self, api_client: APIClient):
        """
        Test that properly authenticated requests work.

        This is a sanity check ensuring our test authentication is
        configured correctly. If this fails, other tests may fail for
        authentication reasons rather than actual bugs.
        """
        response = await api_client.get("/{{ python_name }}s")

        # Should succeed with valid authentication
        assert_that(response).has_success_status()
{% endif %}


class Test{{ python_name|title }}AdvancedScenarios:
    """
    Advanced testing scenarios.

    These tests go beyond basic CRUD operations to test more complex
    scenarios that often reveal bugs in production systems.
    """

    @pytest.fixture(autouse=True)
    def setup(self, api_client: APIClient, data_generator: BaseGenerator):
        """Setup for each test method."""
        self.client = api_client
        self.generator = data_generator
        self.created_resources: List[Tuple[str, str]] = []  # (resource_type, id)

    async def test_concurrent_create_operations(self):
        """
        Test handling of concurrent requests.

        Real-world APIs must handle multiple simultaneous requests. This
        test verifies the API doesn't have race conditions or concurrency
        bugs that could cause data corruption or errors.
        """
        # Create multiple requests concurrently
        async def create_resource(index: int):
            data = {
                "name": f"Concurrent Test {index}",
                "email": self.generator.fake.email(),
                "status": "active"
            }
            return await self.client.post("/{{ python_name }}s", json=data)

        # Launch concurrent requests
        tasks = [create_resource(i) for i in range(5)]
        responses = await asyncio.gather(*tasks, return_exceptions=True)

        # Verify most requests succeeded
        successful = [r for r in responses
                     if not isinstance(r, Exception) and r.status_code == 201]

        assert len(successful) >= 3, \
            f"Expected at least 3 successful requests, got {len(successful)}"

        # Track for cleanup
        for response in successful:
            self.created_resources.append(
                ("{{ python_name }}", response.json_path("id"))
            )

    async def test_search_functionality(self):
        """
        Test search/filtering capabilities.

        Most APIs provide ways to search or filter resources. This test
        verifies those capabilities work correctly.
        """
        # Create resources with known attributes
        search_term = f"unique-{self.generator.fake.uuid()[:8]}"

        # Create some resources with our search term
        for i in range(3):
            await self.client.post("/{{ python_name }}s", json={
                "name": f"{search_term} Test {i}",
                "email": self.generator.fake.email(),
                "status": "active"
            })

        # Create some without the search term
        for i in range(2):
            await self.client.post("/{{ python_name }}s", json={
                "name": f"Other Test {i}",
                "email": self.generator.fake.email(),
                "status": "active"
            })

        # Search for our term
        response = await self.client.get("/{{ python_name }}s", params={
            "search": search_term
        })

        assert_that(response).has_status(200)

        # Verify search results
        results = response.json_path("data")
        assert isinstance(results, list), "Search should return a list"

        # All results should contain our search term
        for result in results:
            assert search_term.lower() in result.get("name", "").lower(), \
                f"Result {result} doesn't match search term {search_term}"

    async def test_bulk_operations(self):
        """
        Test bulk create/update operations if supported.

        Many APIs support bulk operations for efficiency. This test
        verifies those operations work correctly and handle errors
        appropriately.
        """
        # Try bulk create
        bulk_data = [
            {
                "name": f"Bulk Test {i}",
                "email": self.generator.fake.email(),
                "status": "active"
            }
            for i in range(3)
        ]

        # Note: Adjust endpoint based on your API's bulk operation support
        response = await self.client.post("/{{ python_name }}s/bulk", json=bulk_data)

        if response.status_code == 404:
            pytest.skip("Bulk operations not supported by this API")

        assert_that(response).has_success_status()

        # Verify all items were created
        if response.has_json_path("created"):
            created_items = response.json_path("created")
            assert len(created_items) == len(bulk_data), \
                "All items should be created"

    async def test_special_characters_handling(self):
        """
        Test handling of special characters and Unicode.

        APIs should properly handle international characters, emojis, and
        other special characters. This test verifies proper encoding and
        storage of such data.
        """
        special_data = {
            "name": "Test™ with émojis 🚀 and 中文",
            "email": self.generator.fake.email(),
            "description": "Multi-line\ntext with\ttabs and special chars: <>\"'&",
            "status": "active"
        }

        # Create with special characters
        create_response = await self.client.post("/{{ python_name }}s", json=special_data)

        if create_response.status_code == 201:
            resource_id = create_response.json_path("id")
            self.created_resources.append(("{{ python_name }}", resource_id))

            # Retrieve and verify special characters are preserved
            get_response = await self.client.get(f"/{{ python_name }}s/{resource_id}")

            assert_that(get_response) \
                .has_json_path("name", special_data["name"]) \
                .has_json_path("description", special_data["description"])

    @pytest.fixture(autouse=True)
    async def cleanup_resources(self):
        """Clean up any created resources."""
        yield

        # Cleanup resources if they exist
        if hasattr(self, 'created_resources'):
            for resource_type, resource_id in reversed(self.created_resources):
                try:
                    await self.client.delete(f"/{resource_type}s/{resource_id}")
                except Exception:
                    pass  # Ignore cleanup errors

        # Clear the list for next test
        if hasattr(self, 'created_resources'):
            self.created_resources.clear()


class Test{{ python_name|title }}SchemaValidation:
    """
    Schema validation testing using TestAPIX's comprehensive validation features.

    These tests demonstrate TestAPIX's powerful schema validation capabilities,
    including JSON Schema, Pydantic models, and OpenAPI specifications.
    """

    @pytest.fixture(autouse=True)
    def setup(self, api_client: APIClient, data_generator: BaseGenerator):
        """Setup for each test method."""
        self.client = api_client
        self.generator = data_generator
        self.created_resources: List[Tuple[str, str]] = []

    async def test_pydantic_schema_validation_success(self):
        """
        Test successful Pydantic model validation.

        Demonstrates how TestAPIX automatically validates responses against
        Pydantic models, providing clear error messages when validation fails.
        """
        # Create a resource with valid data
        test_data = {
            "name": self.generator.fake.name(),
            "email": self.generator.fake.email(),
            "status": "active"
        }

        response = await self.client.post("/{{ python_name }}s", json=test_data)

        # Pydantic validation will ensure response structure matches model
        assert_that(response) \
            .has_status(201) \
            .matches_schema({{ python_name|title }}Response)

        self.created_resources.append(("{{ python_name }}", response.json_path("id")))

    async def test_json_schema_validation(self):
        """
        Test JSON Schema validation for precise schema control.

        JSON schemas provide detailed validation rules for complex data structures.
        """
        # Define a JSON schema for {{ python_name }} response
        json_schema = {
            "type": "object",
            "properties": {
                "id": {"type": "string", "pattern": "^[a-zA-Z0-9-]+$"},
                "name": {"type": "string", "minLength": 1, "maxLength": 100},
                "email": {"type": "string", "format": "email"},
                "status": {"type": "string", "enum": ["active", "inactive", "pending"]},
                "created_at": {"type": "string", "format": "date-time"}
            },
            "required": ["id", "name", "email", "status", "created_at"],
            "additionalProperties": True
        }

        response = await self.client.get("/{{ python_name }}s", params={"per_page": 1})

        # Validate that each item in the response matches our schema
        if response.has_json_path("data") and response.json_path("data"):
            first_item = response.json_path("data[0]")
            # Create a mock response for individual item validation
            from unittest.mock import Mock
            import httpx

            mock_httpx_response = Mock(spec=httpx.Response)
            mock_httpx_response.status_code = 200
            mock_httpx_response.headers = {"content-type": "application/json"}
            mock_httpx_response.text = self.client._json_encoder.encode(first_item)
            mock_httpx_response.json.return_value = first_item
            mock_httpx_response.request.method = "GET"
            mock_httpx_response.request.url = response.url

            from testapix.core.client import EnhancedResponse
            item_response = EnhancedResponse(mock_httpx_response, response.response_time)

            assert_that(item_response).matches_schema(json_schema, schema_type="jsonschema")

    async def test_openapi_schema_validation(self):
        """
        Test OpenAPI schema validation for API specification compliance.

        OpenAPI schemas ensure your API responses match your specification.
        """
        # Sample OpenAPI specification for {{ python_name }} endpoint
        openapi_spec = {
            "openapi": "3.0.3",
            "info": {"title": "{{ api_name }}", "version": "1.0.0"},
            "paths": {
                "/{{ python_name }}s": {
                    "get": {
                        "responses": {
                            "200": {
                                "description": "List of {{ python_name }}s",
                                "content": {
                                    "application/json": {
                                        "schema": {
                                            "type": "object",
                                            "properties": {
                                                "data": {
                                                    "type": "array",
                                                    "items": {
                                                        "type": "object",
                                                        "properties": {
                                                            "id": {"type": "string"},
                                                            "name": {"type": "string"},
                                                            "email": {"type": "string", "format": "email"},
                                                            "status": {"type": "string"}
                                                        }
                                                    }
                                                },
                                                "pagination": {"type": "object"}
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }

        response = await self.client.get("/{{ python_name }}s", params={"per_page": 5})

        # Validate response against OpenAPI specification
        assert_that(response).matches_openapi_schema(
            openapi_spec,
            "/{{ python_name }}s",
            "get",
            200
        )

    async def test_schema_validation_error_handling(self):
        """
        Test schema validation error handling and reporting.

        Demonstrates how TestAPIX provides detailed error messages when
        schema validation fails, helping developers debug issues quickly.
        """
        # Define a strict schema that might fail validation
        strict_schema = {
            "type": "object",
            "properties": {
                "id": {"type": "integer"},  # Expecting integer but API returns string
                "name": {"type": "string", "minLength": 5},
                "email": {"type": "string", "format": "email"},
                "required_field": {"type": "string"}  # Field that might not exist
            },
            "required": ["id", "name", "email", "required_field"],
            "additionalProperties": False
        }

        response = await self.client.get("/{{ python_name }}s", params={"per_page": 1})

        if response.has_json_path("data") and response.json_path("data"):
            first_item = response.json_path("data[0]")

            # Create mock response for validation
            from unittest.mock import Mock
            import httpx

            mock_httpx_response = Mock(spec=httpx.Response)
            mock_httpx_response.status_code = 200
            mock_httpx_response.headers = {"content-type": "application/json"}
            mock_httpx_response.text = self.client._json_encoder.encode(first_item)
            mock_httpx_response.json.return_value = first_item
            mock_httpx_response.request.method = "GET"
            mock_httpx_response.request.url = response.url

            from testapix.core.client import EnhancedResponse
            item_response = EnhancedResponse(mock_httpx_response, response.response_time)

            try:
                assert_that(item_response).matches_schema(strict_schema)
                # If validation succeeds unexpectedly, that's worth noting
                pytest.skip("Schema validation passed unexpectedly - API structure may have changed")
            except ResponseValidationError as e:
                # Verify the error message is informative
                error_message = str(e)
                assert "validation error" in error_message.lower(), \
                    "Error message should indicate validation failure"

                # TestAPIX should provide details about what failed
                assert any(keyword in error_message.lower() for keyword in
                          ["type", "required", "missing", "expected"]), \
                    f"Error should provide specific validation details: {error_message}"

    async def test_schema_evolution_compatibility(self):
        """
        Test API compatibility when schemas evolve.

        Demonstrates how to handle API changes gracefully by testing
        backward compatibility and schema evolution scenarios.
        """
        # Test with a relaxed schema that allows for API evolution
        flexible_schema = {
            "type": "object",
            "properties": {
                "id": {"type": "string"},
                "name": {"type": "string"},
                "email": {"type": "string", "format": "email"}
                # Note: Only essential fields are required, allowing for new fields
            },
            "required": ["id", "name", "email"],
            "additionalProperties": True  # Allow new fields to be added
        }

        response = await self.client.get("/{{ python_name }}s", params={"per_page": 1})

        if response.has_json_path("data") and response.json_path("data"):
            first_item = response.json_path("data[0]")

            # Create mock response for validation
            from unittest.mock import Mock
            import httpx

            mock_httpx_response = Mock(spec=httpx.Response)
            mock_httpx_response.status_code = 200
            mock_httpx_response.headers = {"content-type": "application/json"}
            mock_httpx_response.text = self.client._json_encoder.encode(first_item)
            mock_httpx_response.json.return_value = first_item
            mock_httpx_response.request.method = "GET"
            mock_httpx_response.request.url = response.url

            from testapix.core.client import EnhancedResponse
            item_response = EnhancedResponse(mock_httpx_response, response.response_time)

            # This should pass even if the API adds new fields
            assert_that(item_response).matches_schema(flexible_schema)

    @pytest.fixture(autouse=True)
    async def cleanup_resources(self):
        """Clean up created resources after each test."""
        yield

        for resource_type, resource_id in reversed(self.created_resources):
            try:
                await self.client.delete(f"/{resource_type}s/{resource_id}")
            except Exception:
                pass  # Ignore cleanup errors

        self.created_resources.clear()


class Test{{ python_name|title }}Performance:
    """
    Basic performance testing.

    While TestAPIX will have more sophisticated performance testing in Phase 3,
    these basic tests ensure your API meets minimum performance requirements.
    """

    @pytest.fixture(autouse=True)
    def setup(self, api_client: APIClient, data_generator: BaseGenerator):
        """Setup for each test method."""
        self.client = api_client
        self.generator = data_generator

    async def test_response_time_requirements(self):
        """
        Test that API responses meet performance SLAs.

        Adjust the thresholds based on your specific requirements. These
        tests help catch performance regressions early.
        """
        # Test list endpoint performance
        response = await self.client.get("/{{ python_name }}s", params={"per_page": 10})

        assert_that(response) \
            .has_status(200) \
            .response_time_less_than(2.0)  # Adjust based on your SLA

    async def test_large_payload_handling(self):
        """
        Test API handling of large payloads.

        This verifies the API can handle reasonable payload sizes without
        timing out or failing. Adjust sizes based on your expected usage.
        """
        # Create a large but reasonable payload
        large_description = self.generator.fake.text(max_nb_chars=5000)

        large_data = {
            "name": self.generator.fake.name(),
            "email": self.generator.fake.email(),
            "description": large_description,
            "status": "active"
        }

        response = await self.client.post("/{{ python_name }}s", json=large_data)

        # Should either succeed or return appropriate error (413 Payload Too Large)
        assert response.status_code in [201, 400, 413], \
            f"Unexpected response for large payload: {response.status_code}"

        # Clean up if created
        if response.status_code == 201:
            await self.client.delete(f"/{{ python_name }}s/{response.json_path('id')}")


# Additional test classes can be added here for specific features of your API
# For example:
# - TestWebhooks for webhook functionality
# - TestRateLimiting for rate limit verification
# - TestCaching for cache behavior
# - TestVersioning for API version compatibility
