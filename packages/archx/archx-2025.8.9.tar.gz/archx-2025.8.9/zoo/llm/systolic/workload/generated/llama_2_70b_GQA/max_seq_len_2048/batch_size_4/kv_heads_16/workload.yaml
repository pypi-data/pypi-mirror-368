workload:
  llama_2_70b_GQA:
    configuration:
      accumulation_cycles: 1
      activation_bitwidth: 16
      approximate_division_cycles: 1
      architecture: systolic
      batch_size: 4
      dim: 8192
      division_mult_cycles: 14
      exp_mult_cycles: 30
      heads: 64
      hidden_dim: 28672
      kv_heads: 16
      layers: 80
      max_seq_len: 2048
      noc_stationary: os
      node_stationary: ws
      prefill_seq_len: 64
      pwl_cycles: 4
      vocab_size: 32000
      weight_bitwidth: 4
