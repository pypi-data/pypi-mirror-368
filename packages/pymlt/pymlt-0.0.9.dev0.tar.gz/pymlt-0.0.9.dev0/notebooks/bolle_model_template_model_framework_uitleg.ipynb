{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Approximately 70% of problems in Data Science are classification problems.\" (Bron: DataCamp)\n",
    "\n",
    "Classification models zijn modellen die worden ingezet om datapunten te classificeren. Voorbeelden van cases waarbij classification wordt ingezet zijn:\n",
    "- de gezondheidszorg: gegeven de levensstijl en symptonen, wordt iemand ziek of niet?\n",
    "- de verzekeringswereld: gegeven deze claims en onderbouwing, fraudeert iemand of niet?\n",
    "- de bank: kan een klant zijn lening terugbetalen of niet? *zie illustratie*\n",
    "- marketing: zien we een klant nog terug? (churn)\n",
    "\n",
    "Churn is wellicht het bekendste voorbeeld in ons vakgebied, maar er zijn nog genoeg andere voorbeelden te bedenken waarin je als marketing analist informatie wil krijgen over wie een bepaalde actie gaat uitvoeren/zal ondergaan én waarom.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*QUP2zFOd-pzmM3cW.\">\n",
    "\n",
    "Dit notebook is bedoeld om een een classification model te bouwen volgens de Eneco MI structuur. Deze structuur is grotendeels gebaseerd op de industrie standaard van het data science proces (<a href=\"https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining\">CRISP DM</a>). Belangrijk om te beseffen is dat het maken van een (classificatie) model een iteratief proces is waarbij je niet in een rechte lijn van a naar z gaat, maar vaak terugkeert naar vorige stappen om aanpassingen te maken op basis van nieuwe inzichten.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*2NajmK58hJf8lJQm25iXWw.png\" width=400>\n",
    "\n",
    "Deze Eneco MI structuur bestaat uit 6 stappen die later in het notebook nader zullen worden toegelicht.\n",
    "- Stap 1: data import\n",
    "- Stap 2: data exploration\n",
    "- Stap 3: data preparation\n",
    "    - 3a: cleaning\n",
    "    - 3b: feature engineering\n",
    "    - 3c: feature scaling\n",
    "- Stap 4: feature selection by modeling and / or business sense \n",
    "- Stap 5: modeling & evaluation\n",
    "- Step 6: uitscoren\n",
    "\n",
    "Na het doorlopen van dit notebook heb jij een classification model gebouwd! Aangezien niet iedereen even ervaren is in het bouwen van modellen hebben we per stap suggesties gegeven om je kennis weer op te frissen of voor verdere verdieping.\n",
    "Mocht je bepaalde functies niet begrijpen of wil je meer weten over een package raadpleeg dan eerst de online documentatie of roep de help functie aan in Python:\n",
    "\n",
    "```python\n",
    "print(help(*naam van de functie*))\n",
    "```\n",
    "\n",
    "Heb je naar aanleiding van dit script nog vragen over werken met Python? Schroom dan niet om contact op te nemen met DDM (sander.knol@eneco.com) of het Data Science Customers & Operations team (datascience@eneco.com). \n",
    "\n",
    "Run allereerst onderstaande twee cellen om dit notebook werkzaam te krijgen met de meeste recente functies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.prep_data import copy_df, describe_df\n",
    "from .. import return_data_dwh\n",
    "from .. import ..\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dit is niet nodig tenzij ze zelf nieuwe functies gaan schrijven, mogelijk goed om uit te leggen?\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 1: data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deze eerste stap wordt een dataset geïmporteerd waarop het model wordt gebouwd. DDM heeft een SQL script klaargezet dat dient als uitgangspunt voor de trainingsset die gebruikt wordt voor modelbouw. In het script staat uitleg over de aanpassingen die je moet maken om de set te laten aansluiten bij jouw vraagstuk. Het script vind je [hier](https://eneco.sharepoint.com/sites/econts02x/85/Team/Forms/AllItems.aspx?RootFolder=%2Fsites%2Feconts02x%2F85%2FTeam%2FENECO%5FP%2FPROJECTEN%2FVoorspelmodellen%20bouwen&FolderCTID=0x012000476CE4D54C7C7246A549975C9A513BA4&View=%7B7534D727%2D5B74%2D40D7%2DA354%2D9F0206F8BDDE%7D).\n",
    "\n",
    "Als je de resultaten van het script wegschrijft in een DWH tabel, dan kun je deze tabel vervolgens als Pandas dataframe importeren naar dit notebook. Dit kun je doen met behulp van de functie hieronder. Pas hiervoor in de regel waarin je de functie aanroept je personeelsnummer aan, de query (select * from tabelnaam), de locatie waar je SQL-wachtwoord staat en de omgeving waarop je wil inloggen (OLAB = 1, LAB = 2, zonder aanduiding = 3). Let op, met passloc wordt bedoeld een bestandje dat je lokaal op je pc hebt opgeslagen waarin je wachtwoord vermeld staat. Door daarnaar te verwijzen hoef je je wachtwoord nooit hardcoded in Python code te schrijven wat voorkomt dat je je wachtwoord per ongeluk deelt met collega's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = return_data_dwh(pnumber, query, passloc, usernumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Belangrijk**:\n",
    "Zodra de data goed is ingeladen is het belangrijk om een deel van de dataset opzij te zetten om mee te testen hoe goed je finale model presteert. We noemen dit de test set (of holdout set) en deze stap vindt dus nog vóór de data exploratie fase plaats. In onderstaand voorbeeld zetten we random 20% van de totale set opzij. Dit percentage kan naar gelieve worden aangepast, maar moet hoog genoeg zijn om een significante test te bewerkstelligen. Door een waarde in te vullen bij random_state, wordt bewaard welke variabelen random zijn gekozen (in feite is er dan ook sprake van pseudo-randomization). Het is belangrijk dat je deze holdout set verder tijdens het modelbouw proces ongemoeid laat. Wanneer je bijna alle stappen doorlopen hebt en een goed werkend model hebt getraind dan geeft de evaluatie op deze holdout set het beste beeld van de te verwachten performance op compleet nieuwe data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df, test_size=0.20, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 2: data exploratie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De data exploratie fase is een eerste kennismaking met de dataset. Het doel is om vertrouwd te raken met de dataset en om ideeën uit te werken voor de volgende fases. Belangrijke vragen in deze fase:\n",
    "- is de data goed ingeladen?\n",
    "- zijn er missing values?\n",
    "- welke variabelen hebben outliers en hoe zijn die ontstaan?\n",
    "- hoe zit het met de correlatie tussen de variabelen?\n",
    "- zijn er variabelen die weinig tot niets toevoegen (near-zero variance)?\n",
    "- ..\n",
    "\n",
    "Om inzage te krijgen in deze belangrijke vragen hebben we reeds functies ontwikkeld die je daarbij kunnen helpen: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td>\n",
    "    <img src=\"https://www.researchgate.net/profile/Sil_Aarts/publication/323072965/figure/fig1/AS:758041565724674@1557742601380/Figuur-1-Histogram-voor-de-verdeling-van-leeftijd-van-626-deelnemers-Gemiddelde-66-94_Q320.jpg\" style=\"width: 400px;\"> \n",
    "    </td>\n",
    "    <td>   \n",
    "<img src=\"https://machinelearningmind.files.wordpress.com/2019/10/screenshot-2019-10-19-at-2.00.35-pm.png?w=431\" style=\"width: 400px;\">\n",
    "    </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onderstaande functie geeft een algemene beschrijvende statistiek van de dataset, zoals #rijen, #kolommen, verdeling van type variabelen, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_df(df, dependent_variable='Y_VAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als twee of meer variabelen in hoge mate hetzelfde zeggen, dan kan hetgeen wat ze zeggen te zwaar worden meegenomen in je model. Er is dan sprake van multicollineariteit. Onderstaande functie geeft weer welke combinaties van variabele grotendeels hetzelfde gedrag vertonen en geeft een suggestie welke van deze twee variabelen dan het beste weggelaten kan worden. Het cutoff punt kun je zelf bepalen, maar staat default op 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multicollinearity(df, cut_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "** Provides the 5 most prevalent values per variable and the absolute difference between the top 2 values.\n",
    "\n",
    "    This insight can be used to determine which variables carry very little information or are interesting for\n",
    "    further feature engineering.\n",
    "abs : absolute difference between the two most prevalent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_insight(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met de functie hier beneden worden de categorische variabelen uit de trainingsset in een grafiek gevisualiseerd met op de x-as de categorie en op de y-as het gemiddelde van de dependent variable voor alle klanten die in deze categorie vallen. Op die manier krijg je een eerste inzicht in of de variabele mogelijk effect heeft op de dependent variable. \n",
    "\n",
    "Als je de trainingsset zoals eerder in dit notebook geadviseerd hebt gebruikt, zijn de categorische variabelen allemaal afkomst van infobase. In die set wordt zowel een 5-punts als een 7-punts schaal gebruikt. In de functie zijn alle mogelijke categorieën op de x-as gezet. Deze functie is dus niet geschikt om te gebruiken voor een trainingsset met zeer veel categorische variabelen die allemaal andere categorieën bevatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_visu(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ook onderstaande functie maakt inzichtelijk wat de gemiddelde waarde van de y-variabele is bij elke waarde van de x-variabele, maar doet dit voor binaire variabelen. Wederom geeft dit inzicht in de mate waarin een independent variable informatie toevoegt ten aanzien van de dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "odds_ind(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onderstaande functie laat de verdeling van numerieke variabelen zien. Op die manier kan worden afgeleid of er sprake is van een normaalverdeling, scewness, een gebrek aan variantie of bijvoorbeeld een duidelijke tweedeling in de data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_visu(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis is een analysemethode die wordt ingezet om het aantal variabelen wat je gebruikt om je gebruikt in je model te reduceren. Simpel gezegd is het een methode die de bestaande variabelen in de set vervangt door een nieuwe, kleinere, set aan variabelen. Deze kleinere set aan variabelen is eigenlijk een set van dimenties die zijn opgesteld door een combinatie en transformatie van de originele data uit de set. Hoe principal component analysis (PCA) precies werkt wordt goed uitlegd in [dit artikel](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/).\n",
    "\n",
    "Het resultaat van onderstaande functie geeft weer hoeveel variantie van de y-variable kan worden bepaald met welke hoeveelheid dimenties/prinicpal axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_visu(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 3: data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu je meer inzage hebt in je dataset gaan we de meest tijdrovende fase in: data preparation. In deze fase wordt grotendeels de performance van het model bepaald dus onderschat dit niet. Data preparation betaat uit 3 onderdelen:\n",
    "- cleaning\n",
    "- feature engineering\n",
    "- feature selection\n",
    "\n",
    "Ieder onderdeel zal afzonderlijk worden toegelicht, hou er rekening mee dat je in de fase vaak terug zal keren naar een vorig onderdeel, dit is normaal en hoort bij het iteratieve karakter van het data science proces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 3a: cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"60 to 80 percent of the total time is spent on cleaning the data before you can make any meaningful sense of it.\" (Bron: Towards Data Science)\n",
    "\n",
    "Om waardevollen inzichten uit je data te kunnen extraheren, moet je data allereerst geschoond worden om ruis weg te nemen en effecten bloot te kunnen leggen. We spreken van schone data als de data hoogt scoort op, validiteit, accuraatheid, compleetheid, consistency en uniformiteit. Onderstaand overzicht laat zien waar je aan moet denken bij data cleaning.\n",
    "\n",
    "<img src=\"https://3jd8gl2iires146kaw2hgqy9-wpengine.netdna-ssl.com/wp-content/uploads/2017/01/Data-cleaning-checklist-v2-01.png\" width=600>\n",
    "\n",
    "Een onderdeel van data cleaning is dus het omgaan met missing values. Hoe je daar het beste mee kan omgaan, hangt af van de volgende aantal factoren:\n",
    "- Wat is het type missing? MCAR/MAR/MNAR? Lees hier meer over op https://towardsdatascience.com/practical-strategies-to-handle-missing-values-626f9c43870b?gi=dcd45101e7ac\n",
    "- Hoeveel procent van de cellen in een kolom is leeg?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Door onderstaande cell te runnen worden de variabelen met een groot aantal missende waarden verwijderd. Hoe groot dit aantal is kan worden bepaald door een threshold te zetten. Alle variabelen waarvan het aandeel missings groter is dan de threshold zal worden verwijderd uit de trainingsset. Welke variabelen dit zijn is terug te zien in het resultaat van de vorige stap, waar perc_missings per column wordt weergegeven. Gebruik deze functie voor kolommen met veel missings en/of MCAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_missings(df, threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In plaats van het verwijderen van kolommen met lege cellen kan ook worden gekozen om deze lege cellen op te vullen. Met het runnen van onderstaande functie worden deze lege cellen opgevuld met de mediaan van de kolom. Gebruik deze functie voor kolommen met weinig missings en/of MCAR/MAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_impute(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als een count variabele missende waarden bevat, is de kans de groot dat dit betekent dat de telling van de waarde 0 is. In dan geval kun je bij de lege cellen een 0 invullen met onderstaande functie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na_zero(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als een missing niet volledig random is en er dus een reden is voor de missing, kan deze reden meer zeggen over de kolom dan de variabele zelf. In dat geval kan het slim zijn die informatie mee te nemen in een nieuwe variabele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_high_correlated(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Op het moment dat een variabele weinig tot geen variantie bevat (dat wil zeggen dat de waarden nagenoeg allemaal gelijk zijn), dan geeft die variabele geen informatie over de dependent variable en is deze overbodig. Daarom kun je een variabele met near zero vero uit de dataset verwijderen met onderstaande functie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nzv(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overige aanpassingen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenslotte zijn er ook nog een aantal overige aanpassingen die wellicht gewenst zijn voor het bouwen van het model dat je wil. Zo kan het een logische veronderstelling zijn dat klanten van verschillende labels zich anders gedragen en dat je daarom een model wil maken voor een specifiek label. Daarnaast zitten er misschien variabelen in de set waarvan je op basis van common sense vooraf al kan zeggen dat zij niet van invloed kunnen zijn op je Y-variabelen of wellicht heb je een andere reden om ze uit te sluiten. Tenslotte is het raadzaam de titels van je variabelen alleemaal lowercase (of uppercase) te maken zodat je niet iedere keer terug hoeft te kijken hoe de variabele wordt geschreven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_owner(df, owner)\n",
    "standaard_drops(df, list_of_variables)\n",
    "col_to_lower(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 3b: feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alter data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Pandas werkt met een aantal datatypes, welke veelal zijn afgeleid van NumPy. De belangrijkste en meest voorkomende types zijn opgenomen in onderstaand overzicht.\n",
    "\n",
    "<img src=\"https://pbpython.com/images/pandas_dtypes.png\">\n",
    "\n",
    "Hieronder worden een aantal suggesties gegeven om het dataypes van de variabelen in je set aan te passen. Run eerst onderstaand blok om te ontdekken welk variabele in je dataset welke datatype heeft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veel modellen kunnen niet omgaan met categorische variabelen. Daarom is het verstandig deze te hercoderen naar dummy-variabelen. Dat doet onderstaande functie. Let op: je bent misschien gewend dat het aantal dummies gelijk is aan het aantal categorieën - 1. Bij deze functie is dat niet het geval, hier krijg je evenveel indicator variabelen als het aantal categorieën waar de originele variabele uit bestaat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dummies(df, list_of_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy encoding \n",
    "\n",
    "Machine learning algoritmes kunnen over het algemeen *alleen* omgaan met numerieke waarden. Voor het TCS model wouden we echter gebruik maken van de energielabel, en die variabele is categorisch (object, zie datatypes). Een manier om hiermee om te gaan is het maken van dummy variabelen, en deze functie zit standaard in de Pandas package.\n",
    "\n",
    "Het idee is om van ieder label een numerieke indicator te maken, zodat alle informatie beschikbaar blijft maar dan numeriek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label\n",
      "0     A\n",
      "1     B\n",
      "2     C\n",
      "3     D\n",
      "4     E\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'label': ['A', 'B', 'C', 'D', 'E']})\n",
    "print(df)\n",
    "print(df['label'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label_A  label_B  label_C  label_D  label_E\n",
      "0        1        0        0        0        0\n",
      "1        0        1        0        0        0\n",
      "2        0        0        1        0        0\n",
      "3        0        0        0        1        0\n",
      "4        0        0        0        0        1\n",
      "label_A    uint8\n",
      "label_B    uint8\n",
      "label_C    uint8\n",
      "label_D    uint8\n",
      "label_E    uint8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(pd.get_dummies(df))\n",
    "print(pd.get_dummies(df).dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misschien staat er waarden in je dataset die wel numeriek zijn, maar die Pandas niet als zodanig herkent. Dat kan problemen veroorzaken: Python snapt niet 1 + 2 = 3 niet als 1 wordt gezien als een stukje tekst in plaats een cijfer. Met onderstaande functie kun je een datatype veranderen naar numeriek. Doe dit door je dataframe aan te roepen en een lijst van de variabelen die je numeriek wil maken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_numeric(df, list_of_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een ander datatype issue gaat over numerieke ordinale variabelen en ratio variabelen. Beide zijn integers of floats, maar bij een binaire indicator geeft een 1 een categorie aan en geen aantal. Daarom wil je dat indicatoren ook als dusdanig worden behandeld en niet worden gezien als een frequency variabelen. Onderstaand blok lost dat voor je op. Let er wel op dat je deze functie altijd gebruikt nadat je de to_dummies functie hebt gebruikt, omdat je met die functie indicator variabelen aanmaakt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_to_ind(df, list_of_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veel statistische technieken vereisen dat er aan bepaalde aannames wordt voldaan, zoals normaalverdeling, multicollinearity of homoscedasticity. [Dit artikel](https://towardsdatascience.com/all-the-annoying-assumptions-31b55df246c3) legt uit hoe het ook alweer zat met die aannames. \n",
    "\n",
    "Wanneer je data niet voldoet aan de aannames van het model, zijn er een aanal datatransformaties mogelijk.\n",
    "\n",
    "Eén daarvan is het vervangen van de waarden door de wortel van deze waaarden. Deze transformatie is bijzonder geschikt voor: \n",
    "1. correctie voor de schending van de aanname van homogeniteit van variantie\n",
    "2. het corrigeren van data met een skew naar rechts naar (meer) normaal verdeelde data\n",
    "3. het algemeen verbeteren van lineaire fit\n",
    "\n",
    "Onderstaande functie voert die transformatie uit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_transformer(df, list_of_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bij een log-transformatie  wordt het natuurlijk logaritme van de originele data genomen. Deze transformatie methode wordt afgeraden voor datasets/variabelen met negatieve waardes. Deze transformatie is handig voor: \n",
    "1. het corrigeren voor de schending van de aanname van homogeniteit van variantie\n",
    "2. het corrigeren van een sterke skew\n",
    "3. het algemeen verbeteren van lineaire fit.\n",
    "\n",
    "Je kunt een log-transformatie aanroepen voor een lijst van variabelen met de functie hier beneden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transform(df, list_of_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Coming up with features is difficult, time-consuming, requires expert knowledge. \"Applied machine learning\" is basically feature engineering.\"<br>\n",
    "-- Andrew NG, *Machine Learning and AI via Brain simulations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier boven zijn een aantal voorbeelden gegeven van standaard datatransformaties die je kunt toepassen. Feature engineering is echter vooral een stuk waarin je zelf veel creativiteit en business kennis kwijt kunt! Hoe transformeer je je gegevens dusdanig dat het model wat je later gaat bouwen zo goed mogelijk in staat is de juiste informatie uit je data te halen? In feite is het antwoord op deze vraag volledig afhankelijk van je vraagstuk dat je wil oplossen met je model. Misschien hebben eerdere analyses al aangetoond dat een interactie tussen twee bepaalden variabelen een effect heeft op je y-variabele, weet je dat uitschieters veroorzaakt worden door fouten in de data of ben je benieuwd naar het effect van een bepaalde factor die niet direct is terug te herleiden naar één variabele uit je set.\n",
    "\n",
    "Om je een beetje een richting te geven in te transformaties waar je aan zou kunnen denken, geven we hieronder alvast wat voorbeelden. Maar let op: deze voorbeelden zijn niet voor iedere toepassing bruikbaar en bovenal zijn er nog veel meer transformatie denkbaar! Voor hulp of advies kun je altijd bij DDM terecht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning betekent dat we een machine gaan trainen om iets te leren maar besef goed dat deze machine an sich niet intelligent is. Om de machine zo goed mogelijk te laten leren moet je het de machine zo makkelijk mogelijk maken om te datgene te leren waar jij interesse in hebt. Om dit concept duidelijk te maken volgen enkele voorbeelden uit het Toon cross-sell (TCS) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expert kennis: duurzaamheid\n",
    "\n",
    "Zoals beschreven in de quote gaat feature engineering over het inzetten van expert kennis om het maximale uit het ML algoritme te halen. Een voorbeeld uit het Toon cross-sell model is een gecombineerde 'duurzaamheid' variabele. Deze variabele bevat de som van de volgende 4 indicator variables (1/0):\n",
    "- wind energie uit Nederland\n",
    "- wind energie uit Europa\n",
    "- zon\n",
    "- ecogas\n",
    "\n",
    "Het idee om deze variabelen te combineren ontstond doordat:\n",
    "- we het idee hadden dat Toon voor sommige klanten mogelijk een duurzame keuze is, die zou kunnen correleren met andere bewuste keuzes\n",
    "- iedere losse variabele weinig informatie bevat (veel 0-en, 'sparse') maar gecombineerd meer informatie bevat\n",
    "\n",
    "De winst is dat we de informatie van 4 variabelen kunnen vatten in 1 en iedere losse variabele geen associatie met de dependent variable vertoonde maar de som van de variabelen wel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/toon_duurzaam.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expert kennis: woonwaarde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om het Toon cross-sell model te verbeteren hebben we ook gekeken naar externe variabelen, zoals de waarde van de woning. Een van de variabelen die we onderzochten was woning_waarde_cat. Dit is een (numerieke) categorische variabele met 7 levels:\n",
    "- 1: geen koopwoning\n",
    "- 2: < 75.000 euro\n",
    "- ..\n",
    "- 6: 350.000 - 500.000 euro\n",
    "- 7: > 500.000 euro\n",
    "\n",
    "Ook voor deze variabele onderzochten we de associatie met Toon cross-sell per level:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/woning_waarde.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levels 3, 4, en 5 laten een (licht) verhoogde associatie zien met Toon cross-sell. We zien hier dus wel een effect van woningwaarde op Toon cross-sell, maar dit verband is niet lineair en er is geen éénduidig cutoff punt vast te stellen. Belangrijk om te beseffen is dat lang niet alle ML algoritmes deze associatie goed kunnen modelleren. Een simpel voorbeeld is een decision tree, bij welke level maak je de cutoff om Toon cross-sell zo goed mogelijk te voorspellen? Om het ML algoritme te helpen hebben we een extra indicator variable toegevoegd:\n",
    "```python\n",
    "df[\"nf_woning_waarde_ind\"] = np.where(df[\"woning_waarde_cat\"].isin([3, 4, 5]), 1, 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 3c: feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het 'schalen' van data is een belangrijk onderdeel in het trainen van een ML model, in sommige gevallen maakt het zelfs een verschil tussen een slecht of goed model. Maar wat is het precies en waarom maakt het uit?\n",
    "\n",
    "Het schalen van de data betekent dat je distributie van de ruwe data verandert naar een schaal met een gemiddelde van 0 en een standaard deviatie van 1. Het klinkt handig dat features dezelfde schaal hebben maar waarom is het belangrijk?:\n",
    "- sommige ML algoritmes maken gebruik van afstand\n",
    "- sommige pre-processing technieken worden gestuurd door de absolute variantie\n",
    "- sommige ML algoritmes zoeken iteratief naar de beste oplossing\n",
    "\n",
    "K-means en K-Nearest-Neighbours (KNN) zijn 2 bekende ML algoritmes die werken op basis van de afstand tussen\n",
    "verschillende datapunten. Stel je de volgende situatie voor:\n",
    "- x1: aantal kamers [1-6]\n",
    "- x2: originele vraagprijs [150.000 - 1.000.000]\n",
    "\n",
    "Wanneer je met deze data zou gaan werken zou *alleen* de originele vraagprijs invloed hebben aangezien de ruwe afstanden met betrekking tot het aantal kamers compleet worden ondergesneeuwd.\n",
    "\n",
    "Principal components analysis (PCA) is een pre-processing techniek om dimensies te reduceren en zoekt naar features met de hoogste absolute variantie. Zonder te schalen zal wederom alleen de orginele vraagprijs mee worden genomen aangezien die verschillen vele malen groter zijn. Hieronder een voorbeeld, het is niet erg als je de code nog niet helemaal begrijpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aantal_kamers  vraagprijs\n",
      "0              4      626329\n",
      "1              3      333703\n",
      "2              5      741123\n",
      "3              5      180405\n",
      "4              2      552493\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "test_data = pd.DataFrame({'aantal_kamers': np.random.randint(low=1, high=6, size=1000),\n",
    "                          'vraagprijs': np.random.randint(low=150000, high=1000000, size=1000)})\n",
    "\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aantal_kamers         1.419326\n",
       "vraagprijs       246427.718265\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std() # standaard deviatie, sqrt(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aantal_kamers  vraagprijs\n",
      "0       0.660483    0.199449\n",
      "1      -0.053553   -0.997164\n",
      "2       1.374518    0.668867\n",
      "3       1.374518   -1.624033\n",
      "4      -0.767588   -0.102483\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "test_data_scaled = scaler.fit_transform(test_data)\n",
    "test_data_scaled = pd.DataFrame(test_data_scaled, columns=test_data.columns)\n",
    "print(test_data_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aantal_kamers    1.0005\n",
       "vraagprijs       1.0005\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_scaled.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tot slot is er een belangrijk en veelgebruikt optimalisatie algoritme genaamd Gradient Descent. Dit algoritme gaat iteratief op zoek naar de beste combinatie van parameters die de error minimaliseren (het verschil tussen de voorspelling en de werkelijkheid). Wanneer je de data niet schaalt kan het *heel* erg lang duren voordat het algoritme een optimale oplossing heeft gevonden aangezien het algoritme moet 'omlopen':\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*vXpodxSx-nslMSpOELhovg.png\">\n",
    "\n",
    "Let op dat er meerdere scaling / normalisatie technieken zijn en afhankelijk van jouw dataset wil je mogelijk meerdere technieken proberen. Meer informatie is te vinden op <a href=\"https://becominghuman.ai/demystifying-feature-scaling-baff53e9b3fd\">demystifying-feature-scaling</a>. De StandardScaler() zit op het moment van schrijven standaard in de train_basic_models() functie en wordt op ieder model toegepast. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 4: feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de modelbackbone uit DWH zit een zeer groot aantal variabelen en je kunt ervan uitgaan ze niet allemaal evenveel van belang zijn voor hetgeen je wil modelleren. Om te bepalen welke variabelen wel veel invloed hebben en je sowieso mee wil nemen in je uiteindelijke model, draai je in deze stap een aantal basismodellen om de feature importance te bepalen. Op die manier kun je na deze stap de variabelen die niets doen voor je afhankelijke variabele verwijderen. Dat bespaart een hoop computation costs tijdens het runnen van het model en zorg er bovendien voor dat je uitkomsten overzichtelijk blijven en makkelijker te presenteren.\n",
    "\n",
    "Bovendien bestaan er een groot aantal classification algoritmes, met elk eigen voordelen en nadelen. Bij verschillende datasets kunnen zij verschillende prestaties vertonen. Om die reden kan het slim zijn enkele verschillende modellen te testen om te weten te komen welk model geschikt is voor jouw dataset. Met dat inzicht kun je het best presterende model gaan optimaliseren. Hierbij kan het verstandig zijn een afweging te maken tussen enerzijds de accuracy score van het model en anderzijds te running time van een model. Als een model dat veel langer runt maar minimaal beter presteert, kies je wellicht liever voor het snellere model. Ook kun je in je keuze laten meewegen hoe moeilijk of makkelijk de resultaten van het model te presenteren zijn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tevens moet het model weten wat je x-variabelen zijn en wat je y-variabele is. Gebruik daarvoor onderstaand blok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_set['y_var']\n",
    "X = train_set.drop('y_var', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu is het tijd om een aantal verschillende basale classification modellen te trainen om de feature importance te bepalen en een idee te krijgen van de potentie van een algoritme. Om dit te kunnen doen definieer je eerst de y- en de x-variabelen(n). Dat kan simpel met onderstaande code wanneer je de y-variabele y_var hebt genoemd.\n",
    "\n",
    "Vervolgens gaan we de train set verder opsplitsen, maar let op: de holdout set blijft onaangeraakt. De train set wordt opgesplitst in een train en een validation set. De train set zal door de modellen gebruikt worden om de algoritmes te fitten, terwijl de validation set zal worden gebruikt om te bepalen hoe goed de fit is. Ook nu is het mogelijk om de verdeling van de sets aan te passen en vast te houden hoe de random verdeling plaatsvindt.\n",
    "\n",
    "Daarna worden vier classification models getraind om de feature importance te bepalen. Het doel van van het draaien van de modellen in deze fase is nog niet dat de modellen optimaal werken, wel dat ze snel een overzicht geeft van de feature importance en de potentie van het modellen. We behandelen de volgende modellen:\n",
    "- Tree based classifiers\n",
    "    - Random forest\n",
    "    - XGBClassifier\n",
    "    - ExtraTreesClassificer\n",
    "    - SGDClassifier\n",
    "- Linear classifiers\n",
    "    - Logistic regression\n",
    "    - LightGBMClassifier\n",
    "\n",
    "Hier beneden worden deze technieken één voor één uitgelegd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "\n",
    "Random forest bundelt de resultaten van meerdere decision trees die simultaan worden gerund met verschillende random subsets (met terugleggen) van features en verschillende random subsamples van de trainingset. \n",
    "<img src=\"https://www.amo-nl.com/wp-content/uploads/2019/08/Random-Forest.png\">\n",
    "Via onderstaande links vind je meer documentatie over dit algorithme:\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
    "- https://www.amo-nl.com/het-principe-van-de-werking-van-random-forest/\n",
    "\n",
    "Onderstaand blok code resulteert in een aanbeveling aangaande de features om mee te nemen in het finale model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_features_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoostClassifier \n",
    "\n",
    "Extreme Gradient Boosting maakt net als random forest gebruik van meerdere decision trees, maar in tegenstelling tot random forest wordt iedere tree op de volledige training set gefit in plaats van op subsamples.Bovendien gebeurt het runnen van de trees achter sequentieel in plaats van parallel. Hierbij heeft iedere nieuwe tree als doel heeft de voorgaande tree te verbeteren met een extra stukje informatie. Via onderstaande links vind je meer documentatie over dit algoritme:\n",
    "- https://xgboost.readthedocs.io/en/latest/python/python_intro.html\n",
    "- https://xgboost.readthedocs.io/en/latest/get_started.html\n",
    "- https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "- https://statquest.org/2019/12/16/xgboost-part-1-xgboost-trees-for-regression/\n",
    "- https://statquest.org/2020/01/18/xgboost-part-2-xgboost-trees-for-classification/\n",
    "\n",
    "Het runnen van de code hieronder resulteert in een lijst van features die geselecteerd worden om mee te nemen in het model volgens de XGBoostClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreesClassificer\n",
    "\n",
    "ExtraTrees maakt wederom gebruikt van individuele decision trees, maar dit algoritme gebruikt subsamples zonder terugleggen en gebruikt bij iedere vertakking een random cutoff point voor de feature in plaats van een geoptimaliseerd cutoff point. Via onderstaande links vind je meer documentatie over dit algoritme:\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "- https://towardsdatascience.com/an-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b\n",
    "- https://www.youtube.com/watch?v=Q1qpG7gwix4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Logistic regression is een variant die lijkt op een lineare regressie, met als belangrijkste verschil dat logistic regression een binaire uitkomst schat in plaats van een continue uitkomst. Lees hier meer over logistische regressie:\n",
    "\n",
    "- https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python\n",
    "- https://statquest.org/2018/03/05/statquest-logistic-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier\n",
    "SGDClassifier is een algoritme dat een linear regressie toepast met minimalisering van de loss functie. Dat doet het algoritme door voor random samples de loss bij toepassing van het regressie te bepalen en deze kennis mee te nemen bij het toepassen van een regressie op een nieuwe sample.\n",
    "Stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).\n",
    "\n",
    "What is SGD Classifier?\n",
    "SGD Classifier implements regularised linear models with Stochastic Gradient Descent.\n",
    "So, what is stochastic gradient descent?\n",
    "Stochastic gradient descent considers only 1 random point while changing weights unlike gradient descent which considers the whole training data. As such stochastic gradient descent is much faster than gradient descent when dealing with large data sets. Here is a nice answer on Quora which explains in detail the difference between gradient descent and stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBMClassifier\n",
    "\n",
    "LightGBM is tevens een gradient boosting classifier dat gebruikt maken van tree based learning algorithms, maar werkt leaf-wise in plaats van level-wise en is daarmee efficiënter in snelheid en geheugen dan andere algoritmes.\n",
    "<img src=\"https://miro.medium.com/max/1211/1*AZsSoXb8lc5N6mnhqX5JCg.png\">\n",
    "<img src=\"https://miro.medium.com/max/983/1*whSa8rY4sgFQj1rEcWr8Ag.png\">\n",
    "Meer uitleg over LightGBM vind je hier:\n",
    "- https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n",
    "- https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hier nu duidelijk handvaten geven over de beslissingen die nu moeten worden genomen (welk model en welke features) en gabseerd waarop!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Als je bovenstaande stappen hebt doorlopen, heb je nu vier lists van features die volgens een model worden gedefinieerd als informatie voor het inschatten van de Y-variabele. Het kan zijn dat die vier lists veelal dezelfde features bevatten, maar flinke afwijkingen in het aantal features of de inhoud van de features kan ook voorkomen. Het is verstanding om bij de volgende stap business kennis mee te nemen bij het maken van de beslissing welke features verder mee worden genomen in het model. Er zou kunnen worden gekozen om alle features die hierboven als informatief zijn bestempeld door minimaal één van de modellen mee te nemen. Dat kun je doen met behulp van onderstaand stukje code, waarbij de lists worden samengevoegd en de duplicaten worden verwijderd.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Afhankelijk van de wensen van het model, zijn andere methoden ook mogelijk. Bijvoorbeeld: wanneer je een classification model bouwt om meer inzicht te krijgen in drivers, kan het wenselijk zijn een kleiner aantal features te pakken. Dan kun je ervoor kiezen om alleen de features mee te nemen die bij alle vier de modellen naar boven kwamen.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 6: run models, determine best model, selected model met hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als het goed is, heb je nu beter begrip van een aantal veelgebruikte voorspelmodellen. Nu is het tijd om de resultaten van deze modellen te gaan vergelijken. Op die manier kun je makkelijker beoordelen welk model het meest geschikt is voor jouw vraagstuk. Maar let op: bij deze afweging komt meer kijken dan alleen modelresultaten. Hier is het vooral van belang dat je ook praktische overwegingen en business kennis meeneemt!\n",
    "\n",
    "Met onderstaand blok fit je de hierboven genoemde modellen voor jouw dataset en print je hoe ze presteren op het gebied van: \n",
    "- de tijd die het kost om het model te fitten\n",
    "- **param_classifier?**\n",
    "- **sum_rank?**\n",
    "- accuracy\n",
    "- precision\n",
    "- **mean_test?**\n",
    "- ROC AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 5a: train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic_models(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 5b: predict model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 5c: determine model diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recall_precision(model, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undefit_overfit(model, x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation metrics: https://neptune.ai/blog/evaluation-metrics-binary-classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}