llm:
  # OpenAI配置
  openai:
    chat:
      api_base: "https://api.siliconflow.cn"
      api_key: "${OPENAI_API_KEY}"
      model_name: "Qwen/Qwen2.5-7B-Instruct"
      # model_name: "Pro/deepseek-ai/DeepSeek-R1"
      temperature: 0.7
      max_tokens: 8192
      connect_timeout: 30
      read_timeout: 60
    embeddings:
      # api_base: "https://api.siliconflow.cn"
      api_base: "http://10.122.4.199:8002/v1"
      api_key: "${OPENAI_API_KEY}"
      model_name: "bge-m3"
      # dimensions: 1024
      timeout: 30

  # VLLM配置
  vllm:
    chat:
      api_base: "http://10.122.4.199:8000/v1"
      api_key: "${OPENAI_API_KEY}"
      model_name: "/models/Qwen3-32B-AWQ"
      temperature: 0.6
      top_p: 0.8
      max_tokens: 8192
      connect_timeout: 30
      read_timeout: 60
      extra_body:
        return_reasoning: false
    embeddings:
      api_base: "http://10.122.4.199:8002/v1"
      api_key: "${OPENAI_API_KEY}"
      model_name: "bge-m3"
      dimensions: 1024
      timeout: 30

  reasoning:
    chat:
      api_base: "http://10.122.5.141:8000/v1"
      api_key: "${OPENAI_API_KEY}"
      model_name: "peft"
      temperature: 0.6
      top_p: 0.8
      max_tokens: 8192
      connect_timeout: 30
      read_timeout: 60
      extra_body:
        return_reasoning: true

  vlm:
    chat:
      api_base: "http://10.122.5.24:23333/v1"
      api_key: "${OPENAI_API_KEY}"
      model_name: "internlm-xcomposer2"
      temperature: 0.6
      top_p: 0.8
      max_tokens: 8192
      connect_timeout: 30
      read_timeout: 60

  # Infinity配置
  infinity:
    embeddings:
      api_base: "http://10.122.4.199:7997/v1"
      model_name: "models/bge-m3"

  # 默认提供者配置
  default:
    chat_provider: "vllm" # 可选值: openai, vllm
    embedding_provider: "vllm" # 可选值: openai, infinity
